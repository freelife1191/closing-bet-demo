#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë°ì´í„° ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ (Data Initialization Script)
- ì‹¤ì‹œê°„ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ (yfinance)
- í•„ìš”í•œ ë°ì´í„° íŒŒì¼ ìƒì„±
- ì—ëŸ¬ ì²˜ë¦¬ ë° ì§„í–‰ë¥  í‘œì‹œ ê°œì„ 
"""

import os
import sys
import pandas as pd
import numpy as np
import json
import socket
import yfinance as yf
import time
import random
import logging
from datetime import datetime, timedelta

# [FIX] Filter out pykrx's broken logging calls
class PykrxFilter(logging.Filter):
    def filter(self, record):
        # pykrx.website.comm.util calls logging.info(args, kwargs) which causes TypeError
        if 'pykrx' in record.pathname and 'util.py' in record.pathname:
            return False
        return True

# Apply filter to root logger (Logger.filter runs before handlers/formatting)
logging.getLogger().addFilter(PykrxFilter())
# If no handlers yet (basicConfig not called), we might need to add it later or rely on basicConfig


# ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ ì„¤ì • (30ì´ˆ) - ë¬´í•œ ëŒ€ê¸° ë°©ì§€
socket.setdefaulttimeout(30)

# Import shared state for stop logic
try:
    import engine.shared as shared_state
except ImportError:
    import sys, os
    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if root_dir not in sys.path:
        sys.path.append(root_dir)
    try:
        import engine.shared as shared_state
    except ImportError:
        class MockShared:
            STOP_REQUESTED = False
        shared_state = MockShared()

# Custom JSON encoder for numpy types
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.bool_):
            return bool(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super().default(obj)

# yfinance for real market data
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
except ImportError:
    YFINANCE_AVAILABLE = False

# ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, BASE_DIR)
import asyncio

from engine.config import config, app_config
from engine.collectors import EnhancedNewsCollector
from engine.llm_analyzer import LLMAnalyzer
from engine.market_gate import MarketGate

# =====================================================
# ì£¼ë§/íœ´ì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =====================================================

def get_last_trading_date(reference_date=None):
    """
    ë§ˆì§€ë§‰ ê°œì¥ì¼ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    - ì£¼ë§(í† /ì¼)ì¸ ê²½ìš° ê¸ˆìš”ì¼ë¡œ ì´ë™
    - ê¸ˆìš”ì¼ì´ íœ´ì¼ì¸ ê²½ìš° pykrxë¥¼ í†µí•´ ì‹¤ì œ ë§ˆì§€ë§‰ ê°œì¥ì¼ í™•ì¸
    
    Args:
        reference_date: ê¸°ì¤€ ë‚ ì§œ (datetime ê°ì²´). Noneì´ë©´ ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©.
    
    Returns:
        tuple: (last_trading_date_str, last_trading_date_obj)
               - last_trading_date_str: 'YYYYMMDD' í˜•ì‹ì˜ ë¬¸ìì—´
               - last_trading_date_obj: datetime ê°ì²´
    """
    if reference_date is None:
        reference_date = datetime.now()
    
    target_date = reference_date
    
    # 1ì°¨: ì£¼ë§ ì²˜ë¦¬ (í† /ì¼ â†’ ê¸ˆìš”ì¼ë¡œ ì´ë™)
    if target_date.weekday() == 5:  # í† ìš”ì¼
        target_date -= timedelta(days=1)
    elif target_date.weekday() == 6:  # ì¼ìš”ì¼
        target_date -= timedelta(days=2)
    
    # 2ì°¨: pykrxë¥¼ í†µí•´ ì‹¤ì œ ê°œì¥ì¼ í™•ì¸
    try:
        from pykrx import stock
        
        # ìµœê·¼ 10ì¼ê°„ ê±°ë˜ì¼ ì¡°íšŒ (íœ´ì¼ ì—°ì† ëŒ€ë¹„)
        start_check = (target_date - timedelta(days=10)).strftime('%Y%m%d')
        end_check = target_date.strftime('%Y%m%d')
        
        # KOSPI ì§€ìˆ˜ì˜ OHLCVë¡œ ê°œì¥ì¼ í™•ì¸
        kospi_data = stock.get_index_ohlcv_by_date(start_check, end_check, "1001")
        
        if not kospi_data.empty:
            # ë§ˆì§€ë§‰ ê±°ë˜ì¼ì„ ê°€ì ¸ì˜´
            last_trading_date = kospi_data.index[-1]
            last_trading_date_str = last_trading_date.strftime('%Y%m%d')
            log(f"ë§ˆì§€ë§‰ ê°œì¥ì¼ í™•ì¸: {last_trading_date_str}", "DEBUG")
            return last_trading_date_str, last_trading_date
        else:
            # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê³„ì‚°ëœ ë‚ ì§œ ì‚¬ìš©
            log(f"pykrx ë°ì´í„° ì—†ìŒ, ê³„ì‚°ëœ ë‚ ì§œ ì‚¬ìš©: {target_date.strftime('%Y%m%d')}", "DEBUG")
            
    except ImportError:
        log("pykrx ë¯¸ì„¤ì¹˜ - ì£¼ë§ ì²˜ë¦¬ë§Œ ì ìš©", "WARNING")
    except Exception as e:
        # ì§€ìˆ˜ëª… KeyError ë“± pykrx ë‚´ë¶€ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¡°ìš©íˆ ë„˜ì–´ê°€ê³  ì£¼ë§ ì²˜ë¦¬ë§Œ ì ìš©
        log(f"ê°œì¥ì¼ í™•ì¸ ì‹¤íŒ¨ (pykrx): {e}. ê¸°ë³¸ ì£¼ë§ ì²˜ë¦¬ë§Œ ì ìš©í•©ë‹ˆë‹¤.", "DEBUG")
    
    # í´ë°±: ì£¼ë§ ì²˜ë¦¬ë§Œ ëœ ë‚ ì§œ ë°˜í™˜
    return target_date.strftime('%Y%m%d'), target_date


# =====================================================
# ì‹¤ì‹œê°„ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
# =====================================================

def fetch_market_indices():
    """KOSPI/KOSDAQ ì‹¤ì‹œê°„ ì§€ìˆ˜ ìˆ˜ì§‘"""
    indices = {
        'kospi': {'value': 2650.0, 'change_pct': 0.0, 'prev_close': 2650.0},
        'kosdaq': {'value': 850.0, 'change_pct': 0.0, 'prev_close': 850.0}
    }
    
    if not YFINANCE_AVAILABLE:
        log("yfinance ë¯¸ì„¤ì¹˜ - ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©", "WARNING")
        return indices
    
    try:
        # yfinance ì¼ê´„ ë‹¤ìš´ë¡œë“œ (threads=False í•„ìˆ˜)
        ticker_map = {
            'kospi': '^KS11', 'kosdaq': '^KQ11',
            'gold': '411060.KS', 'silver': '144600.KS',
            'us_gold': 'GC=F', 'us_silver': 'SI=F',
            'sp500': '^GSPC', 'nasdaq': '^IXIC',
            'btc': 'BTC-USD', 'eth': 'ETH-USD', 'xrp': 'XRP-USD'
        }
        
        symbols = list(ticker_map.values())
        
        # ì•ˆì „í•œ ë‹¤ìš´ë¡œë“œ (ìŠ¤ë ˆë“œ ë¹„í™œì„±í™”)
        data = yf.download(symbols, period="5d", progress=False, threads=False)
        
        # ë°ì´í„° ì¶”ì¶œ Helper
        def get_val_change_prev(ticker):
             try:
                # MultiIndex ì²˜ë¦¬
                if isinstance(data.columns, pd.MultiIndex):
                    if ticker in data['Close'].columns:
                        series = data['Close'][ticker].dropna()
                    else:
                        return 0, 0, 0
                else: # ë‹¨ì¼ í‹°ì»¤ í˜¹ì€ Flattened
                    if ticker in data.columns:
                        series = data[ticker].dropna()
                    elif 'Close' in data.columns:
                        series = data['Close'].dropna()
                    else:
                        return 0, 0, 0
                
                if series.empty: return 0, 0, 0
                
                latest = float(series.iloc[-1])
                prev = float(series.iloc[-2]) if len(series) >= 2 else latest
                change = ((latest - prev) / prev) * 100 if prev != 0 else 0
                return latest, change, prev
             except:
                return 0, 0, 0

        # ê²°ê³¼ ë§¤í•‘
        ks_val, ks_chg, ks_prev = get_val_change_prev(ticker_map['kospi'])
        indices['kospi'] = {'value': round(ks_val, 2), 'change_pct': round(ks_chg, 2), 'prev_close': round(ks_prev, 2)}
        
        kq_val, kq_chg, kq_prev = get_val_change_prev(ticker_map['kosdaq'])
        indices['kosdaq'] = {'value': round(kq_val, 2), 'change_pct': round(kq_chg, 2), 'prev_close': round(kq_prev, 2)}
        
        g_val, g_chg, g_prev = get_val_change_prev(ticker_map['gold'])
        indices['kr_gold'] = {'value': round(g_val, 0), 'change_pct': round(g_chg, 2), 'prev_close': round(g_prev, 0)}
        
        s_val, s_chg, s_prev = get_val_change_prev(ticker_map['silver'])
        indices['kr_silver'] = {'value': round(s_val, 0), 'change_pct': round(s_chg, 2), 'prev_close': round(s_prev, 0)}
        
        ug_val, ug_chg, ug_prev = get_val_change_prev(ticker_map['us_gold'])
        indices['us_gold'] = {'value': round(ug_val, 2), 'change_pct': round(ug_chg, 2), 'prev_close': round(ug_prev, 2)}
        
        us_val, us_chg, us_prev = get_val_change_prev(ticker_map['us_silver'])
        indices['us_silver'] = {'value': round(us_val, 2), 'change_pct': round(us_chg, 2), 'prev_close': round(us_prev, 2)}
        
        sp_val, sp_chg, sp_prev = get_val_change_prev(ticker_map['sp500'])
        indices['sp500'] = {'value': round(sp_val, 2), 'change_pct': round(sp_chg, 2), 'prev_close': round(sp_prev, 2)}
        
        nd_val, nd_chg, nd_prev = get_val_change_prev(ticker_map['nasdaq'])
        indices['nasdaq'] = {'value': round(nd_val, 2), 'change_pct': round(nd_chg, 2), 'prev_close': round(nd_prev, 2)}
        
        b_val, b_chg, b_prev = get_val_change_prev(ticker_map['btc'])
        indices['btc'] = {'value': round(b_val, 2), 'change_pct': round(b_chg, 2), 'prev_close': round(b_prev, 2)}
        
        e_val, e_chg, e_prev = get_val_change_prev(ticker_map['eth'])
        indices['eth'] = {'value': round(e_val, 2), 'change_pct': round(e_chg, 2), 'prev_close': round(e_prev, 2)}
        
        x_val, x_chg, x_prev = get_val_change_prev(ticker_map['xrp'])
        indices['xrp'] = {'value': round(x_val, 4), 'change_pct': round(x_chg, 2), 'prev_close': round(x_prev, 4)}
        
        log(f"ì‹œì¥ ì§€ìˆ˜ ìˆ˜ì§‘ ì™„ë£Œ: KOSPI {ks_val}, Gold {g_val}", "SUCCESS")
            
    except Exception as e:
        log(f"ì‹œì¥ ì§€ìˆ˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {e} - ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©", "WARNING")
    
    return indices


def fetch_sector_indices():
    """pykrxë¥¼ ì‚¬ìš©í•˜ì—¬ KOSPI ì„¹í„° ì§€ìˆ˜ ìˆ˜ì§‘"""
    # ì„¹í„° ì½”ë“œ ë§¤í•‘ (KOSPI ì—…ì¢… ì§€ìˆ˜ - KRX ê³µì‹ ì½”ë“œ)
    sector_codes = {
        '1012': 'ì² ê°•',       # ì² ê°•Â·ê¸ˆì†
        '1027': '2ì°¨ì „ì§€',   # ì „ê¸°Â·ì „ì (2ì°¨ì „ì§€, ë°˜ë„ì²´ í¬í•¨)
        '1024': 'ë°˜ë„ì²´',     # ë°˜ë„ì²´
        '1016': 'ìë™ì°¨',     # ìš´ìˆ˜ì¥ë¹„
        '1020': 'ì¦ê¶Œ',       # ê¸ˆìœµì—…
        '1018': 'ITì„œë¹„ìŠ¤',   # ì„œë¹„ìŠ¤ì—… (IT)
        '1001': 'KOSPI200',   # KOSPI 200
        '1026': 'ì€í–‰',       # ì€í–‰
    }
    
    sectors = []
    
    try:
        from pykrx import stock
        # from datetime import datetime, timedelta
        
        today = datetime.now().strftime('%Y%m%d')
        yesterday = (datetime.now() - timedelta(days=3)).strftime('%Y%m%d')
        
        for code, name in sector_codes.items():
            try:
                df = stock.get_index_ohlcv_by_date(yesterday, today, code)
                if not df.empty and len(df) >= 2:
                    current = df['ì¢…ê°€'].iloc[-1]
                    prev = df['ì¢…ê°€'].iloc[-2]
                    change_pct = ((current - prev) / prev) * 100 if prev > 0 else 0
                    
                    # ê°•ì„¸/ì•½ì„¸ íŒë‹¨
                    if change_pct > 1.0:
                        signal = 'bullish'
                    elif change_pct < -1.0:
                        signal = 'bearish'
                    else:
                        signal = 'neutral'
                    
                    # ì ìˆ˜ ê³„ì‚° (ë“±ë½ë¥  ê¸°ë°˜)
                    score = min(max(50 + int(change_pct * 10), 0), 100)
                    
                    sectors.append({
                        'name': name,
                        'signal': signal,
                        'change_pct': round(change_pct, 2),
                        'score': score
                    })
            except Exception as e:
                pass
        
        if sectors:
            log(f"ì„¹í„° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(sectors)}ê°œ ì„¹í„°", "SUCCESS")
        
    except ImportError:
        log("pykrx ë¯¸ì„¤ì¹˜ - ìƒ˜í”Œ ì„¹í„° ë°ì´í„° ì‚¬ìš©", "WARNING")
    except Exception as e:
        log(f"pykrx ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e} - ìƒ˜í”Œ ë°ì´í„° ìƒì„±", "WARNING")
        
        
        return False
    
    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œ ë°˜í™˜
    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ìƒ˜í”Œ ê¸ˆì§€)
    if not sectors:
        return []
    
    return sectors


def fetch_stock_price(ticker):
    """ê°œë³„ ì¢…ëª© ì‹¤ì‹œê°„ ê°€ê²© ìˆ˜ì§‘ (Use Shared Logic)"""
    try:
        from engine.data_sources import fetch_stock_price as shared_fetch
        return shared_fetch(ticker)
    except ImportError:
        # Fallback if engine package not found (e.g. running script directly)
        import sys
        if BASE_DIR not in sys.path:
            sys.path.append(BASE_DIR)
        try:
            from engine.data_sources import fetch_stock_price as shared_fetch
            return shared_fetch(ticker)
        except:
            return None


# ì „ì—­ ìºì‹œ (ì—¬ëŸ¬ í•¨ìˆ˜ì—ì„œ ê³µìœ )
_market_indices_cache = None
_sector_indices_cache = None

def get_market_indices():
    """ìºì‹œëœ ì‹œì¥ ì§€ìˆ˜ ë°˜í™˜"""
    global _market_indices_cache
    if _market_indices_cache is None:
        _market_indices_cache = fetch_market_indices()
    return _market_indices_cache

def get_sector_indices():
    """ìºì‹œëœ ì„¹í„° ì§€ìˆ˜ ë°˜í™˜"""
    global _sector_indices_cache
    if _sector_indices_cache is None:
        _sector_indices_cache = fetch_sector_indices()
    return _sector_indices_cache

def reset_cache():
    """ìºì‹œ ì´ˆê¸°í™” (Refresh ì‹œ í˜¸ì¶œ)"""
    global _market_indices_cache, _sector_indices_cache
    _market_indices_cache = None
    _sector_indices_cache = None
    log("ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ", "SUCCESS")



# ì „ì—­ ë¡œê±° ì„¤ì • (ì¤‘ë³µ ì¶œë ¥ ë°©ì§€)
logger = logging.getLogger("init_data")
logger.propagate = False
logger.setLevel(logging.INFO)

try:
    _log_dir = os.path.join(BASE_DIR, 'logs')
    if not os.path.exists(_log_dir):
        os.makedirs(_log_dir)
    _fh = logging.FileHandler(os.path.join(_log_dir, 'init_data.log'), encoding='utf-8')
    _fh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    if not logger.handlers:
        logger.addHandler(_fh)
except Exception as e:
    print(f"Logger setup failed: {e}")

# ìƒ‰ìƒ ì½”ë“œ (í„°ë¯¸ë„)
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def log(message, level="INFO"):
    # File logging (propagate=Falseë¡œ ì½˜ì†” ì¤‘ë³µ ë°©ì§€)
    if level == "ERROR":
        logger.error(message)
    elif level == "WARNING":
        logger.warning(message)
    elif level == "SUCCESS":
        logger.info(f"âœ… {message}")
    elif level == "DEBUG":
        logger.debug(message)
    else:
        logger.info(message)

    # Console logging
    if level == "SUCCESS":
        print(f"{Colors.OKGREEN}âœ… {message}{Colors.ENDC}", flush=True)
    elif level == "ERROR":
        print(f"{Colors.FAIL}âŒ {message}{Colors.ENDC}", flush=True)
    elif level == "WARNING":
        print(f"{Colors.WARNING}âš ï¸  {message}{Colors.ENDC}", flush=True)
    elif level == "HEADER":
        print(f"\n{Colors.HEADER}{Colors.BOLD}{'='*60}{Colors.ENDC}", flush=True)
        print(f"{Colors.HEADER}{message}{Colors.ENDC}", flush=True)
        print(f"{Colors.HEADER}{'='*60}{Colors.ENDC}", flush=True)
    elif level == "DEBUG":
        pass  # Skip console output for debug
    else:
        # INFO logs also skipped in console if not important, but keep default behavior for now or strict?
        # User wants "only errors when there is an error", but some info might be useful.
        # Let's keep INFO printing but move verbose logs to DEBUG.
        print(f"ğŸ“Œ {message}", flush=True)


def ensure_directory(dir_path):
    """ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤."""
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
        log(f"ë””ë ‰í† ë¦¬ ìƒì„±ë¨: {dir_path}", "SUCCESS")
    else:
        log(f"ë””ë ‰í† ë¦¬ í™•ì¸ë¨: {dir_path}")

def create_korean_stocks_list():
    """í•œêµ­ ì£¼ì‹ ëª©ë¡ ìƒì„± - pykrxë¡œ ì‹œê°€ì´ì•¡ ìƒìœ„ ì¢…ëª© ì¡°íšŒ"""
    log("í•œêµ­ ì£¼ì‹ ëª©ë¡ ìƒì„± ì¤‘ (pykrx ì‹œê°€ì´ì•¡ ìƒìœ„)...")
    try:
        from pykrx import stock
        # from datetime import datetime
        
        today = datetime.now().strftime('%Y%m%d')
        
        all_data = []
        
        def get_market_cap_safe(target_date, market):
            try:
                df = stock.get_market_cap(target_date, market=market)
                if not df.empty:
                    return df
            except:
                pass
            return pd.DataFrame()

        # KOSPI
        kospi_cap = get_market_cap_safe(today, "KOSPI")
        if kospi_cap.empty: # ì˜¤ëŠ˜ ë°ì´í„° ì—†ìœ¼ë©´ í•˜ë£¨ ì „ ì‹œë„
             from datetime import timedelta
             prev_date = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
             log(f"ì˜¤ëŠ˜({today}) KOSPI ë°ì´í„° ì—†ìŒ. ì „ì¼({prev_date}) ë°ì´í„° ì‹œë„...", "WARNING")
             kospi_cap = get_market_cap_safe(prev_date, "KOSPI")

        if not kospi_cap.empty:
            # ì‹œê°€ì´ì•¡ ìˆœ ì •ë ¬ í›„ ìƒìœ„ 1000ê°œ (VCP ë°œêµ´ í™•ë¥  í™•ëŒ€ë¥¼ ìœ„í•´ ëŒ€í­ ì¦ê°€)
            kospi_cap = kospi_cap.sort_values('ì‹œê°€ì´ì•¡', ascending=False)
            kospi_top_cap = kospi_cap.head(1000)
            
            # [ì¶”ê°€] ê±°ë˜ëŒ€ê¸ˆ ìƒìœ„ 500ê°œë„ ì¶”ê°€ (ì‹œì´ì€ ì‘ì§€ë§Œ ê±°ë˜ê°€ í„°ì§„ ê¸‰ë“±ì£¼ í¬ì°©)
            kospi_top_vol = kospi_cap.sort_values('ê±°ë˜ëŒ€ê¸ˆ', ascending=False).head(500)
            
            # ë³‘í•© (ì¤‘ë³µ ì œê±°)
            kospi_metrics = pd.concat([kospi_top_cap, kospi_top_vol])
            kospi_metrics = kospi_metrics[~kospi_metrics.index.duplicated(keep='first')]
            
            for ticker in kospi_metrics.index:
                try:
                    name = stock.get_market_ticker_name(ticker)
                    all_data.append({'ticker': ticker, 'name': name, 'market': 'KOSPI', 'sector': ''})
                except: pass
            log(f"KOSPI ì¢…ëª© ìˆ˜ì§‘: ì‹œì´ìƒìœ„ 1000 + ê±°ë˜ëŒ€ê¸ˆìƒìœ„ 500 -> í†µí•© {len(kospi_metrics)}ê°œ", "SUCCESS")
        else:
            log("KOSPI ì‹œê°€ì´ì•¡ ì¡°íšŒ ì‹¤íŒ¨", "WARNING")

        # KOSDAQ
        kosdaq_cap = get_market_cap_safe(today, "KOSDAQ")
        if kosdaq_cap.empty: 
             from datetime import timedelta
             prev_date = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
             log(f"ì˜¤ëŠ˜({today}) KOSDAQ ë°ì´í„° ì—†ìŒ. ì „ì¼({prev_date}) ë°ì´í„° ì‹œë„...", "WARNING")
             kosdaq_cap = get_market_cap_safe(prev_date, "KOSDAQ")

        if not kosdaq_cap.empty:
            # ì‹œê°€ì´ì•¡ ìˆœ ì •ë ¬ í›„ ìƒìœ„ 1000ê°œ (ì½”ìŠ¤ë‹¥ í¬í•¨ ìš”ì²­ ë°˜ì˜)
            kosdaq_cap = kosdaq_cap.sort_values('ì‹œê°€ì´ì•¡', ascending=False)
            kosdaq_top_cap = kosdaq_cap.head(1000)
            
            # [ì¶”ê°€] ê±°ë˜ëŒ€ê¸ˆ ìƒìœ„ 500ê°œë„ ì¶”ê°€
            kosdaq_top_vol = kosdaq_cap.sort_values('ê±°ë˜ëŒ€ê¸ˆ', ascending=False).head(500)

            # ë³‘í•©
            kosdaq_metrics = pd.concat([kosdaq_top_cap, kosdaq_top_vol])
            kosdaq_metrics = kosdaq_metrics[~kosdaq_metrics.index.duplicated(keep='first')]

            for ticker in kosdaq_metrics.index:
                try:
                    name = stock.get_market_ticker_name(ticker)
                    all_data.append({'ticker': ticker, 'name': name, 'market': 'KOSDAQ', 'sector': ''})
                except: pass
            log(f"KOSDAQ ì¢…ëª© ìˆ˜ì§‘: ì‹œì´ìƒìœ„ 1000 + ê±°ë˜ëŒ€ê¸ˆìƒìœ„ 500 -> í†µí•© {len(kosdaq_metrics)}ê°œ", "SUCCESS")
        else:
            log("KOSDAQ ì‹œê°€ì´ì•¡ ì¡°íšŒ ì‹¤íŒ¨", "WARNING")
        
        if all_data:
            df = pd.DataFrame(all_data)
            file_path = os.path.join(BASE_DIR, 'data', 'korean_stocks_list.csv')
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            log(f"ì¢…ëª© ëª©ë¡ ìƒì„± ì™„ë£Œ: {file_path} ({len(df)} ì¢…ëª©)", "SUCCESS")
            return True
        else:
            raise Exception("ì‹œê°€ì´ì•¡ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        
    except Exception as e:
        log(f"pykrx ì¢…ëª© ì¡°íšŒ ì‹¤íŒ¨: {e} - ê¸°ë³¸ ì¢…ëª© ì‚¬ìš©", "WARNING")
        # í´ë°±: ì‹œê°€ì´ì•¡ ìƒìœ„ ì£¼ìš” ì¢…ëª© (KOSPI + KOSDAQ)
        data = {
            'ticker': [
                # KOSPI ìƒìœ„ 20ê°œ
                '005930', '000660', '005380', '373220', '207940', '000270', '035420', '068270', '105560', '055550',
                '035720', '003550', '015760', '028260', '017670', '032830', '009150', '251270', '012330', '034730',
                # KOSDAQ ìƒìœ„ 10ê°œ + ì¸ê¸°/ê¸‰ë“±ì£¼ (ì•Œí…Œì˜¤ì  , ë¦¬ë…¸ê³µì—… ë“±)
                '247540', '086520', '196170', '263750', '145020', '403870', '328130', '091990', '336370', '058470',
                '293490', '214150', '035900', '041510', '036930', '039030', '035760', '022100', '042700', '064350'
            ],
            'name': [
                # KOSPI
                'ì‚¼ì„±ì „ì', 'SKí•˜ì´ë‹‰ìŠ¤', 'í˜„ëŒ€ì°¨', 'LGì—ë„ˆì§€ì†”ë£¨ì…˜', 'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤', 'ê¸°ì•„', 'NAVER', 'ì…€íŠ¸ë¦¬ì˜¨', 'KBê¸ˆìœµ', 'ì‹ í•œì§€ì£¼',
                'ì¹´ì¹´ì˜¤', 'LG', 'í•œêµ­ì „ë ¥', 'ì‚¼ì„±ë¬¼ì‚°', 'SKí…”ë ˆì½¤', 'ì‚¼ì„±ìƒëª…', 'ì‚¼ì„±ì „ê¸°', 'ë„·ë§ˆë¸”', 'í˜„ëŒ€ëª¨ë¹„ìŠ¤', 'SK',
                # KOSDAQ
                'ì—ì½”í”„ë¡œë¹„ì— ', 'ì—ì½”í”„ë¡œ', 'ì•Œí…Œì˜¤ì  ', 'í„ì–´ë¹„ìŠ¤', 'íœ´ì ¤', 'í”¼ì—ì´ì¹˜ì—ì´', 'ë£¨ë‹›', 'ì…€íŠ¸ë¦¬ì˜¨ì œì•½', 'ì†”ë¸Œë ˆì¸', 'ë¦¬ë…¸ê³µì—…',
                'ì¹´ì¹´ì˜¤ê²Œì„ì¦ˆ', 'í´ë˜ì‹œìŠ¤', 'JYP Ent.', 'ì—ìŠ¤ì— ', 'ì£¼ì„±ì—”ì§€ë‹ˆì–´ë§', 'ì´ì˜¤í…Œí¬ë‹‰ìŠ¤', 'CJ ENM', 'í¬ìŠ¤ì½”DX', 'í•œë¯¸ë°˜ë„ì²´', 'í˜„ëŒ€ë¡œí…œ'
            ],
            'market': [
                'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI',
                'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI',
                'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ',
                'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ'
            ],
            'sector': [
                'ë°˜ë„ì²´', 'ë°˜ë„ì²´', 'ìë™ì°¨', '2ì°¨ì „ì§€', 'í—¬ìŠ¤ì¼€ì–´', 'ìë™ì°¨', 'ì¸í„°ë„·', 'í—¬ìŠ¤ì¼€ì–´', 'ê¸ˆìœµ', 'ê¸ˆìœµ',
                'ì¸í„°ë„·', 'ì§€ì£¼', 'ì—ë„ˆì§€', 'ê±´ì„¤', 'í†µì‹ ', 'ê¸ˆìœµ', 'ì „ê¸°ì „ì', 'ê²Œì„', 'ìë™ì°¨ë¶€í’ˆ', 'ì§€ì£¼',
                '2ì°¨ì „ì§€', '2ì°¨ì „ì§€', 'í—¬ìŠ¤ì¼€ì–´', 'ê²Œì„', 'í—¬ìŠ¤ì¼€ì–´', 'ìë™ì°¨ë¶€í’ˆ', 'AI/ì˜ë£Œ', 'í—¬ìŠ¤ì¼€ì–´', 'ë°˜ë„ì²´ì†Œì¬', 'ë°˜ë„ì²´ì¥ë¹„',
                'ê²Œì„', 'ë¯¸ìš©ê¸°ê¸°', 'ì—”í„°', 'ì—”í„°', 'ë°˜ë„ì²´ì¥ë¹„', 'ë°˜ë„ì²´ì¥ë¹„', 'ë¯¸ë””ì–´', 'ITì„œë¹„ìŠ¤', 'ë°˜ë„ì²´ì¥ë¹„', 'ë°©ì‚°'
            ],
        }
        df = pd.DataFrame(data)
        file_path = os.path.join(BASE_DIR, 'data', 'korean_stocks_list.csv')
        df.to_csv(file_path, index=False, encoding='utf-8-sig')
        log(f"ê¸°ë³¸ ì¢…ëª© ëª©ë¡ ìƒì„± ì™„ë£Œ: {file_path} ({len(df)} ì¢…ëª© - KOSPI 15ê°œ + KOSDAQ 10ê°œ)", "SUCCESS")
        return True



def fetch_prices_yfinance(start_date, end_date, existing_df, file_path):
    """yfinanceë¥¼ ì´ìš©í•œ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ í´ë°±"""
    try:
        if start_date.date() > end_date.date():
            log(f"yfinance ìˆ˜ì§‘: ì‹œì‘ì¼({start_date.strftime('%Y-%m-%d')})ì´ ì¢…ë£Œì¼({end_date.strftime('%Y-%m-%d')})ë³´ë‹¤ ë¯¸ë˜ì…ë‹ˆë‹¤. (ìµœì‹  ìƒíƒœ)", "SUCCESS")
            return True

        import yfinance as yf
        log("yfinance ë°±ì—… ìˆ˜ì§‘ ëª¨ë“œ ê°€ë™...", "DEBUG")
        
        # ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
        stocks_file = os.path.join(BASE_DIR, 'data', 'korean_stocks_list.csv')
        if not os.path.exists(stocks_file):
            log("ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ì–´ yfinance ìˆ˜ì§‘ ë¶ˆê°€", "ERROR")
            return False
            
        stocks_df = pd.read_csv(stocks_file, dtype={'ticker': str})
        tickers = stocks_df['ticker'].tolist()
        
        new_data_list = []
        
        total = len(tickers)
        for idx, ticker in enumerate(tickers):
            if shared_state.STOP_REQUESTED:
                log("â›”ï¸ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ yfinance ìˆ˜ì§‘ ì¤‘ë‹¨", "WARNING")
                return False
            
            try:
                # ë§ˆì¼“ í™•ì¸
                market_info = stocks_df[stocks_df['ticker'] == ticker]['market'].values
                suffix = ".KS" if len(market_info) > 0 and market_info[0] == 'KOSPI' else ".KQ"
                yf_ticker = f"{ticker}{suffix}"
                
                # yfinance ì—ëŸ¬ ë¡œê·¸ ì–µì œ
                import logging as _logging
                yf_logger = _logging.getLogger('yfinance')
                original_level = yf_logger.level
                yf_logger.setLevel(_logging.CRITICAL)
                
                try:
                    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì§„í–‰ë¥  í‘œì‹œ ì—†ì´, ìŠ¤ë ˆë“œ ë¹„í™œì„±í™”)
                    df = yf.download(yf_ticker, start=start_date.strftime('%Y-%m-%d'), end=(end_date + timedelta(days=1)).strftime('%Y-%m-%d'), progress=False, threads=False)
                finally:
                    yf_logger.setLevel(original_level)
                
                if not df.empty:
                    # MultiIndex ì»¬ëŸ¼ ì²˜ë¦¬
                    if isinstance(df.columns, pd.MultiIndex):
                         # yfinance 0.2.x+ returns MultiIndex if configured or sometimes by default
                         # It usually is (Price, Ticker) or just Price.
                         # Dropping level if it exists
                        try:
                            df.columns = df.columns.droplevel(1)
                        except:
                            pass
                        
                    df = df.reset_index()
                    # ì»¬ëŸ¼ ì´ë¦„ì´ Date, Open, High ...
                    
                    # Rename columns to standard lowercase
                    df = df.rename(columns={
                        'Date': 'date', 'Open': 'open', 'High': 'high', 
                        'Low': 'low', 'Close': 'close', 'Volume': 'volume'
                    })
                    
                    # Ensure columns exist
                    required_cols = ['date', 'open', 'high', 'low', 'close', 'volume']
                    if not all(col in df.columns for col in required_cols):
                         continue

                    df['date'] = df['date'].dt.strftime('%Y-%m-%d')
                    df['ticker'] = ticker
                    
                    # Type conversion
                    df['open'] = df['open'].astype(int)
                    df['high'] = df['high'].astype(int)
                    df['low'] = df['low'].astype(int)
                    df['close'] = df['close'].astype(int)
                    df['volume'] = df['volume'].astype(int)
                    
                    subset = df[['date', 'ticker', 'open', 'high', 'low', 'close', 'volume']]
                    new_data_list.append(subset)
                    
            except Exception as e:
                continue
                
            if idx % 50 == 0:
                print(f"  -> yfinance ì§„í–‰: {idx}/{total}")

        if new_data_list:
            new_df = pd.concat(new_data_list)
            
            if not existing_df.empty:
                if 'date' in existing_df.columns and not pd.api.types.is_string_dtype(existing_df['date']):
                     existing_df['date'] = existing_df['date'].dt.strftime('%Y-%m-%d')
                     
                final_df = pd.concat([existing_df, new_df])
                final_df = final_df.drop_duplicates(subset=['ticker', 'date'], keep='last')
            else:
                final_df = new_df
                
            final_df.to_csv(file_path, index=False, encoding='utf-8-sig')
            log(f"yfinance ë°±ì—… ìˆ˜ì§‘ ì™„ë£Œ ({len(final_df)}í–‰)", "SUCCESS")
            return True
        else:
            log("yfinance ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ", "WARNING")
            return True
            
    except Exception as e:
        log(f"yfinance í´ë°± ì‹¤íŒ¨: {e}", "ERROR")
        return False


def create_daily_prices(target_date=None, force=False, lookback_days=5):
    """
    ì¼ë³„ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ - pykrx ë‚ ì§œë³„ ì¼ê´„ ì¡°íšŒ (ì†ë„ ìµœì í™”)
    Args:
        target_date: ê¸°ì¤€ ë‚ ì§œ (ê¸°ë³¸: ì˜¤ëŠ˜)
        force: ê°•ì œ ì—…ë°ì´íŠ¸ ì—¬ë¶€
        lookback_days: ê°•ì œ ì—…ë°ì´íŠ¸ ì‹œ ì¬ìˆ˜ì§‘í•  ê¸°ê°„ (ê¸°ë³¸: 5ì¼)
    """
    log("Daily Prices ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œì‘ (Date-based Fast Mode)...", "INFO")
    try:
        from pykrx import stock
        import time
        from datetime import datetime, timedelta

        # ë‚ ì§œ ì„¤ì •
        if target_date:
            if isinstance(target_date, str):
                end_date_obj = datetime.strptime(target_date, '%Y-%m-%d')
            else:
                end_date_obj = target_date
        else:
            end_date_obj = datetime.now()

        # ë§ˆì§€ë§‰ ê°œì¥ì¼ í™•ì¸
        end_date_str, end_date_obj = get_last_trading_date(reference_date=end_date_obj)

        # [Safety] ë¯¸ë˜ ë‚ ì§œ ìš”ì²­ ë°©ì§€
        if end_date_obj > datetime.now():
            log(f"ìš”ì²­ ë‚ ì§œ({end_date_str})ê°€ ë¯¸ë˜ì´ë¯€ë¡œ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì¡°ì •í•©ë‹ˆë‹¤.", "WARNING")
            end_date_obj = datetime.now()
            end_date_str = end_date_obj.strftime('%Y%m%d')
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ë° ì‹œì‘ì¼ ê²°ì •
        file_path = os.path.join(BASE_DIR, 'data', 'daily_prices.csv')
        existing_df = pd.DataFrame()
        start_date_obj = end_date_obj - timedelta(days=90) # ê¸°ë³¸ 90ì¼

        if os.path.exists(file_path):
            try:
                log(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘... ({file_path})", "INFO")
                existing_df = pd.read_csv(file_path, dtype={'ticker': str})
                if not existing_df.empty and 'date' in existing_df.columns:
                    max_date_str = existing_df['date'].max()
                    
                    # (ì¤‘ìš”) ì¢…ëª© ìˆ˜ ì²´í¬ - ìƒˆë¡œ ì¶”ê°€ëœ ì¢…ëª©ì´ ìˆì„ ìˆ˜ ìˆìŒ
                    # í˜„ì¬ ë“±ë¡ëœ ì¢…ëª© ìˆ˜(600ê°œ)ì™€ ë§ˆì§€ë§‰ ë‚ ì§œì˜ ë°ì´í„° ê°œìˆ˜ ë¹„ê²Œ
                    stocks_file = os.path.join(BASE_DIR, 'data', 'korean_stocks_list.csv')
                    total_stocks_count = 600
                    if os.path.exists(stocks_file):
                        try:
                            stocks_df = pd.read_csv(stocks_file)
                            total_stocks_count = len(stocks_df)
                        except:
                            pass
                    
                    last_date_count = len(existing_df[existing_df['date'] == max_date_str])
                    
                    if start_date_obj.date() > end_date_obj.date():
                        # Force check
                        if not force and last_date_count >= total_stocks_count * 0.9:
                            log("ì´ë¯¸ ìµœì‹  ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©° ì¶©ë¶„í•©ë‹ˆë‹¤.", "SUCCESS")
                            return True
                        elif force:
                             log(f"ìµœì‹  ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ë§Œ ê°•ì œ ì—…ë°ì´íŠ¸(force=True)ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. (ìµœê·¼ {lookback_days}ì¼)", "DEBUG")
                             start_date_obj = end_date_obj - timedelta(days=lookback_days)
                        else:
                            log(f"ë°ì´í„° ë‚ ì§œëŠ” ìµœì‹ ì´ë‚˜ ì¢…ëª© ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤({last_date_count}/{total_stocks_count}). ì¬ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.", "WARNING")
                            start_date_obj = end_date_obj - timedelta(days=lookback_days) # ë¶€ì¡±í•œ ê²½ìš°ì—ë„ lookback_days ì‚¬ìš©
                    else:
                        max_date_dt = datetime.strptime(max_date_str, '%Y-%m-%d')
                        # ë§ˆì§€ë§‰ ì €ì¥ì¼ ë‹¤ìŒë‚ ë¶€í„° ìˆ˜ì§‘
                        start_date_obj = max_date_dt + timedelta(days=1)
                        if force:
                             log(f"ê°•ì œ ì—…ë°ì´íŠ¸: ê¸°ì¡´ ë°ì´í„° ë¬´ì‹œí•˜ê³  ìµœê·¼ {lookback_days}ì¼ ì¬ìˆ˜ì§‘", "DEBUG")
                             start_date_obj = end_date_obj - timedelta(days=lookback_days)
                        else:
                             log(f"ê¸°ì¡´ ë°ì´í„° í™•ì¸: {max_date_str}ê¹Œì§€ ì¡´ì¬. ì´í›„ë¶€í„° ìˆ˜ì§‘.", "INFO")
                else:
                    log("ê¸°ì¡´ ë°ì´í„° ë¹„ì–´ìˆìŒ.", "INFO")
            except Exception as e:
                log(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}", "WARNING")
             
        req_start_date_str = start_date_obj.strftime('%Y%m%d')
        log(f"ìˆ˜ì§‘ êµ¬ê°„: {req_start_date_str} ~ {end_date_str}", "INFO")

        # ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        date_range = pd.date_range(start=start_date_obj, end=end_date_obj)
        total_days = len(date_range)
        
        # [Optimization] Create a set of existing dates for O(1) lookup
        existing_dates_set = set()
        if not existing_df.empty and 'date' in existing_df.columns:
            existing_dates_set = set(existing_df['date'].unique())
            log(f"ê¸°ì¡´ ë°ì´í„° ë‚ ì§œ ì¸ë±ì‹± ì™„ë£Œ ({len(existing_dates_set)}ì¼)", "INFO")

        new_data_list = []
        processed_days = 0
        skipped_count = 0
        
        for dt in date_range:
            if shared_state.STOP_REQUESTED:
                log("â›”ï¸ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì¤‘ë‹¨", "WARNING")
                break
                
            cur_date_str = dt.strftime('%Y%m%d')
            cur_date_fmt = dt.strftime('%Y-%m-%d')
            
            # ì£¼ë§ ì²´í¬ (í† /ì¼) - pykrxê°€ ì•Œì•„ì„œ ë¹ˆê°’ ì¤„ ìˆ˜ ìˆìœ¼ë‚˜ ë¯¸ë¦¬ ê±´ë„ˆë›°ë©´ ë¹ ë¦„
            if dt.weekday() >= 5: 
                processed_days += 1
                skipped_count += 1
                continue

            # [Optimization] ì´ë¯¸ ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” ê±´ë„ˆë›°ê¸° (ê³¼ê±° ë°ì´í„°ì¸ ê²½ìš°ë§Œ)
            # ì˜¤ëŠ˜ ë‚ ì§œëŠ” ì¥ì¤‘ ë³€ë™ ê°€ëŠ¥í•˜ë¯€ë¡œ í•­ìƒ ìˆ˜ì§‘
            if not existing_df.empty and 'date' in existing_df.columns:
                if cur_date_fmt in existing_dates_set:
                     # ì˜¤ëŠ˜ì´ ì•„ë‹ˆë©´ Skip
                    if dt.date() < datetime.now().date():
                         processed_days += 1
                         skipped_count += 1
                         # ë””ë²„ê¹…ì„ ìœ„í•´ ì¼ë‹¨ ë§¤ë²ˆ ì¶œë ¥ (ë˜ëŠ” 5íšŒë§ˆë‹¤)
                         if total_days < 20 or processed_days % 5 == 0:
                             progress = (processed_days / total_days) * 100
                             log(f"  -> {cur_date_fmt} ë°ì´í„° ì¡´ì¬ (Skip) - ì§„í–‰ë¥ : {progress:.1f}%", "INFO")
                         continue
                
            try:
                # í•´ë‹¹ ë‚ ì§œì˜ ì „ ì¢…ëª© ì‹œì„¸ ì¡°íšŒ (1íšŒ ìš”ì²­)
                df = stock.get_market_ohlcv(cur_date_str, market="ALL")
                
                if df is None or df.empty:
                    # íœ´ì¥ì¼ ê°€ëŠ¥ì„±
                    processed_days += 1
                    continue
                    
                # DataFrame ì •ë¦¬
                # indexëŠ” ticker, columns: ì‹œê°€, ê³ ê°€, ì €ê°€, ì¢…ê°€, ê±°ë˜ëŸ‰, ê±°ë˜ëŒ€ê¸ˆ, ë“±ë½ë¥ 
                df = df.reset_index() # tickerê°€ ì»¬ëŸ¼ìœ¼ë¡œ ë‚˜ì˜´ ('í‹°ì»¤')
                
                # ì»¬ëŸ¼ ë§¤í•‘
                # pykrx ë²„ì „ì— ë”°ë¼ ì»¬ëŸ¼ëª…ì´ 'í‹°ì»¤'ì¼ìˆ˜ë„, indexì¼ìˆ˜ë„ ìˆìŒ. 
                # get_market_ohlcv("YYYYMMDD") returns index=í‹°ì»¤.
                if 'í‹°ì»¤' in df.columns:
                    df = df.rename(columns={'í‹°ì»¤': 'ticker'})
                else: 
                    # reset_index() í–ˆì„ ë•Œ ê¸°ì¡´ index ì´ë¦„ì´ 'í‹°ì»¤'ì˜€ë‹¤ë©´ ê·¸ê²Œ ì»¬ëŸ¼ëª…ì´ ë¨
                    # ë§Œì•½ ì´ë¦„ì´ ì—†ì—ˆë‹¤ë©´ 'index'
                    if 'index' in df.columns:
                        df = df.rename(columns={'index': 'ticker'})
                
                # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸ (í•œê¸€/ì˜ë¬¸ ëŒ€ì‘)
                rename_map = {
                    'ì‹œê°€': 'open', 'ê³ ê°€': 'high', 'ì €ê°€': 'low', 
                    'ì¢…ê°€': 'close', 'ê±°ë˜ëŸ‰': 'volume', 'ê±°ë˜ëŒ€ê¸ˆ': 'trading_value',
                    'Open': 'open', 'High': 'high', 'Low': 'low', 
                    'Close': 'close', 'Volume': 'volume', 'Amount': 'trading_value'
                }
                
                # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ rename
                available_map = {k: v for k, v in rename_map.items() if k in df.columns}
                df = df.rename(columns=available_map)
                
                df['ticker'] = df['ticker'].astype(str).str.zfill(6)
                df['date'] = cur_date_fmt
                
                # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
                cols = ['date', 'ticker', 'open', 'high', 'low', 'close', 'volume', 'trading_value']
                # ê±°ë˜ëŒ€ê¸ˆ ì—†ì„ ê²½ìš° ì²˜ë¦¬
                if 'trading_value' not in df.columns:
                    df['trading_value'] = df['volume'] * df['close']
                    
                df_final = df[cols].copy()
                
                # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ë©”ëª¨ë¦¬ ê³ ë ¤: ë°”ë¡œë°”ë¡œ ëª¨ìŒ)
                # DataFrame to dict list is slow? append DF to list then concat.
                new_data_list.append(df_final)
                
                processed_days += 1
                progress = (processed_days / total_days) * 100
                log(f"[Daily Prices] {cur_date_fmt} ìˆ˜ì§‘ ì™„ë£Œ ({len(df_final)}ì¢…ëª©) - {progress:.1f}%", "INFO")
                
                # Rate Limit ë°©ì§€
                time.sleep(random.uniform(0.05, 0.1))
                
            except Exception as e:
                log(f"ë‚ ì§œë³„ ìˆ˜ì§‘ ì‹¤íŒ¨ ({cur_date_str}): {e}", "WARNING")
                processed_days += 1
                
        
        # ë³‘í•© ë° ì €ì¥
        if new_data_list:
            log("ë°ì´í„° ë³‘í•© ì¤‘...", "DEBUG")
            new_chunk_df = pd.concat(new_data_list, ignore_index=True)
            
            if not existing_df.empty:
                final_df = pd.concat([existing_df, new_chunk_df])
                final_df = final_df.drop_duplicates(subset=['date', 'ticker'], keep='last')
            else:
                final_df = new_chunk_df
                
            final_df = final_df.sort_values(['ticker', 'date'])
            
            # [Added] Toss APIë¥¼ í†µí•œ ì˜¤ëŠ˜ì˜ ì¢…ê°€ ë³´ì • (ì •í™•ë„ í™•ë³´)
            if end_date_obj.date() == datetime.now().date():
                try:
                    log("Toss Batch APIë¥¼ ì´ìš©í•œ ì˜¤ëŠ˜ì˜ ì¢…ê°€ ë³´ì • ì¤‘...", "INFO")
                    from engine.toss_collector import TossCollector
                    from engine.data_sources import fetch_stock_price
                    from concurrent.futures import ThreadPoolExecutor, as_completed

                    collector = TossCollector()
                    end_date_fmt = end_date_obj.strftime('%Y-%m-%d')
                    target_tickers = final_df[final_df['date'] == end_date_fmt]['ticker'].unique().tolist()
                    
                    # 1. Attempt Toss Batch (Fastest)
                    toss_prices = collector.get_prices_batch(target_tickers)
                    
                    # 2. Identify missing tickers
                    processed_tickers = set(toss_prices.keys())
                    all_tickers_set = set(target_tickers)
                    missing_tickers = list(all_tickers_set - processed_tickers)
                    
                    # Update with Toss data first
                    if toss_prices:
                        mask = (final_df['date'] == end_date_fmt)
                        def get_toss_price(row):
                            t_data = toss_prices.get(row['ticker'])
                            return int(t_data['current']) if t_data and t_data.get('current') and t_data['current'] > 0 else row['close']
                        final_df.loc[mask, 'close'] = final_df[mask].apply(get_toss_price, axis=1)
                    
                    # 3. Fallback for missing tickers (Naver -> YF via fetch_stock_price)
                    if missing_tickers:
                        log(f"Toss Batch ë¯¸ìˆ˜ì§‘ {len(missing_tickers)}ì¢…ëª©. Naver/YF í´ë°± ë³‘ë ¬ ìˆ˜í–‰...", "WARNING")
                        
                        fallback_results = {}
                        with ThreadPoolExecutor(max_workers=20) as executor:
                            future_to_ticker = {executor.submit(fetch_stock_price, t): t for t in missing_tickers}
                            for future in as_completed(future_to_ticker):
                                t = future_to_ticker[future]
                                try:
                                    data = future.result()
                                    if data:
                                        fallback_results[t] = data
                                except Exception as exc:
                                    pass
                        
                        # Apply Fallback Data
                        if fallback_results:
                            mask = (final_df['date'] == end_date_fmt) & (final_df['ticker'].isin(fallback_results.keys()))
                            # Pandas apply is slow, loop might be better for partial updates or map
                            # Using map for speed
                            for t, data in fallback_results.items():
                                # Locating specific row
                                idx = (final_df['date'] == end_date_fmt) & (final_df['ticker'] == t)
                                if any(idx):
                                    final_df.loc[idx, 'close'] = int(data['price'])
                            
                            log(f"í´ë°± ë°ì´í„° ë³´ì • ì™„ë£Œ: {len(fallback_results)}ê°œ ì¢…ëª© ì¶”ê°€ ì—…ë°ì´íŠ¸", "SUCCESS")
                        else:
                             log("í´ë°± ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ (ëª¨ë“  ì†ŒìŠ¤ ì‘ë‹µ ì—†ìŒ)", "WARNING")

                    log(f"ìµœì¢… ë°ì´í„° ë³´ì • ì™„ë£Œ: ì´ {len(toss_prices) + len(fallback_results) if 'fallback_results' in locals() else len(toss_prices)}ê°œ", "SUCCESS")

                except Exception as te:
                    log(f"ë°ì´í„° ë³´ì • í”„ë¡œì„¸ìŠ¤ ì¤‘ ì˜¤ë¥˜: {te}", "WARNING")
            else:
                log(f"Toss Batch ë³´ì • ìƒëµ (ìš”ì²­ì¼ {end_date_obj.date()} != ì˜¤ëŠ˜ {datetime.now().date()})", "INFO")

            log(f"ë°ì´í„° ì €ì¥ ì‹œì‘... ({file_path})", "INFO")
            final_df.to_csv(file_path, index=False, encoding='utf-8-sig')
            log(f"ì¼ë³„ ê°€ê²© ì €ì¥ ì™„ë£Œ: ì´ {len(final_df)}í–‰ (ì‹ ê·œ {len(new_chunk_df)}í–‰)", "INFO")
            print("DEBUG: create_daily_prices finished successfully.", flush=True)
        else:
             if start_date_obj.date() > end_date_obj.date():
                 log("pykrx ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ (ì´ë¯¸ ìµœì‹ ).", "SUCCESS")
                 return True

             log("pykrx ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ. yfinance í´ë°± ì‹œë„...", "DEBUG")
             
             if skipped_count == total_days and total_days > 0:
                 log(f"ëª¨ë“  ë‚ ì§œ({total_days}ì¼) ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. yfinance í´ë°± ìƒëµ.", "SUCCESS")
                 return True

             return fetch_prices_yfinance(start_date_obj, end_date_obj, existing_df, file_path)
                 
        return True

    except Exception as e:
        log(f"pykrx ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e} -> yfinance í´ë°± ì‹œë„", "WARNING")
        return fetch_prices_yfinance(start_date_obj, end_date_obj, existing_df, file_path)


def create_institutional_trend(target_date=None, force=False, lookback_days=7):
    """
    ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ - pykrx ê¸°ê´€/ì™¸êµ­ì¸ ìˆœë§¤ë§¤ (Optimized)
    Args:
        target_date: ê¸°ì¤€ ë‚ ì§œ
        force: ê°•ì œ ì—…ë°ì´íŠ¸ ì—¬ë¶€
        lookback_days: ê°•ì œ ì—…ë°ì´íŠ¸ ì‹œ ì¬ìˆ˜ì§‘í•  ê¸°ê°„ (ê¸°ë³¸: 7ì¼)
    """
    log("ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ (pykrx ì‹¤ì œ ë°ì´í„°)...", "DEBUG")
    try:
        from pykrx import stock
        
        # ì¢…ëª© ëª©ë¡ ë¡œë“œ
        stocks_file = os.path.join(BASE_DIR, 'data', 'korean_stocks_list.csv')
        tickers_set = set() # ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•´ set ì‚¬ìš©
        if os.path.exists(stocks_file):
            stocks_df = pd.read_csv(stocks_file)
            tickers_set = set(stocks_df['ticker'].astype(str).str.zfill(6).tolist())
            if '069500' not in tickers_set:
                tickers_set.add('069500')
        else:
            tickers_set = {'069500', '005930', '000660', '000270', '051910', '006400'}
        
        if target_date:
            if isinstance(target_date, str):
                target_date_obj = datetime.strptime(target_date, '%Y-%m-%d')
            else:
                target_date_obj = target_date
        else:
            target_date_obj = datetime.now()

        # ë§ˆì§€ë§‰ ê°œì¥ì¼ í™•ì¸ (ì£¼ë§/íœ´ì¼ ìë™ ì²˜ë¦¬)
        end_date, end_date_obj = get_last_trading_date(reference_date=target_date_obj)
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        file_path = os.path.join(BASE_DIR, 'data', 'all_institutional_trend_data.csv')
        existing_df = pd.DataFrame()
        start_date_obj = end_date_obj - timedelta(days=30) # ê¸°ë³¸ 30ì¼ ì „
        
        if os.path.exists(file_path):
            try:
                existing_df = pd.read_csv(file_path, dtype={'ticker': str, 'date': str})
                if not existing_df.empty and 'date' in existing_df.columns:
                    # ê°€ì¥ ìµœê·¼ ë°ì´í„° ë‚ ì§œ í™•ì¸
                    max_date_str = existing_df['date'].max()
                    
                    # (ì¤‘ìš”) ë‹¨ìˆœ ë‚ ì§œ ì²´í¬ë§Œ í•˜ì§€ ì•Šê³ , ì¢…ëª© ìˆ˜ê°€ ë¶€ì¡±í•œì§€ í™•ì¸
                    last_date_tickers = len(existing_df[existing_df['date'] == max_date_str])
                    
                    # ì‹ ê·œ ì¶”ê°€ëœ ì¢…ëª©ì´ ìˆëŠ”ì§€ í™•ì¸ (Backfill í•„ìš” ì—¬ë¶€)
                    existing_tickers = set(existing_df['ticker'].unique())
                    missing_tickers = tickers_set - existing_tickers
                    
                    if start_date_obj.date() > end_date_obj.date() and not missing_tickers:
                        if not force and last_date_tickers >= len(tickers_set) * 0.9: # 90% ì´ìƒ ì°¨ìˆìœ¼ë©´ ìµœì‹ ìœ¼ë¡œ ê°„ì£¼
                            log("ìˆ˜ê¸‰ ë°ì´í„°: ì´ë¯¸ ìµœì‹  ìƒíƒœì´ë©° ë°ì´í„°ê°€ ì¶©ë¶„í•©ë‹ˆë‹¤.", "SUCCESS")
                            return True
                        elif force:
                             log(f"ìˆ˜ê¸‰ ë°ì´í„°: ê°•ì œ ì—…ë°ì´íŠ¸ ì§„í–‰ (ìµœê·¼ {lookback_days}ì¼ ì¬ìˆ˜ì§‘)", "WARNING")
                             start_date_obj = end_date_obj - timedelta(days=lookback_days)

                    if missing_tickers and not force: # Forceì¼ë•ŒëŠ” ìœ„ì—ì„œ ì²˜ë¦¬ë¨
                        log(f"ìˆ˜ê¸‰ ë°ì´í„°: ì‹ ê·œ ì¢…ëª© {len(missing_tickers)}ê°œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. (ìµœì í™”: ìµœê·¼ {lookback_days}ì¼ë§Œ ì¬ìˆ˜ì§‘)", "WARNING")
                        # ì‹ ê·œ ì¢…ëª©ì´ ìˆì–´ë„ ê³¼ë„í•œ ì¬ìˆ˜ì§‘ ë°©ì§€ (30ì¼ -> lookback_days)
                        start_date_obj = end_date_obj - timedelta(days=lookback_days)
                    elif last_date_tickers < len(tickers_set) * 0.8:
                        log(f"ìˆ˜ê¸‰ ë°ì´í„°: ìµœì‹  ë‚ ì§œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤({last_date_tickers}/{len(tickers_set)}). ì¬ìˆ˜ì§‘í•©ë‹ˆë‹¤.", "WARNING")
                        start_date_obj = end_date_obj - timedelta(days=lookback_days)     
                    elif not force:
                        # ì •ìƒì ì¸ ê²½ìš° max_date ë‹¤ìŒë‚ ë¶€í„° (Forceê°€ ì•„ë‹ ë•Œë§Œ)
                        try:
                            max_date_dt = datetime.strptime(max_date_str, '%Y-%m-%d')
                            start_date_obj = max_date_dt + timedelta(days=1)
                        except: pass
            except Exception as e:
                log(f"ê¸°ì¡´ ìˆ˜ê¸‰ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ (ìƒˆë¡œ ì‹œì‘): {e}", "WARNING")

        start_date = start_date_obj.strftime('%Y%m%d')
        
        # ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ë¯¸ë˜ì¸ ê²½ìš° (ê·¸ë¦¬ê³  ë¯¸ì‹± í‹°ì»¤ ì—†ëŠ” ê²½ìš°) ì²˜ë¦¬
        if start_date > end_date:
             log("ìˆ˜ê¸‰ ë°ì´í„°: ì´ë¯¸ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.", "SUCCESS")
             return True

        log(f"ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ êµ¬ê°„(ê°œì„ ë¨): {start_date} ~ {end_date} (Date-based Bulk Fetch)", "DEBUG")
        
        # ë‚ ì§œ ë£¨í”„ ì‹œì‘
        date_range = pd.date_range(start=start_date_obj, end=end_date_obj)
        total_days = len(date_range)
        processed_days = 0
        
        new_data_list = []
        
        for dt in date_range:
            if shared_state.STOP_REQUESTED:
                log("â›”ï¸ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ë‹¨", "WARNING")
                break
                
            cur_date_str = dt.strftime('%Y%m%d')
            cur_date_fmt = dt.strftime('%Y-%m-%d')
            
            # ì£¼ë§ ì²´í¬
            if dt.weekday() >= 5:
                processed_days += 1
                continue
            
            # [Optimization] ì´ë¯¸ ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” ê±´ë„ˆë›°ê¸° (ê³¼ê±° ë°ì´í„°ì¸ ê²½ìš°ë§Œ)
            if not existing_df.empty and 'date' in existing_df.columns:
                if cur_date_fmt in existing_df['date'].values:
                    # ì˜¤ëŠ˜ì´ ì•„ë‹ˆë©´ Skip
                    if dt.date() < datetime.now().date():
                         log(f"  -> {cur_date_fmt} ìˆ˜ê¸‰ ë°ì´í„° ì¡´ì¬ (Skip)", "DEBUG")
                         processed_days += 1
                         continue
            
            try:
                # 1. ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ (ì „ ì¢…ëª©)
                df_foreign = stock.get_market_net_purchases_of_equities_by_ticker(cur_date_str, cur_date_str, "ALL", "ì™¸êµ­ì¸")
                time.sleep(0.2) # Rate limit
                
                # 2. ê¸°ê´€ ìˆœë§¤ìˆ˜ (ì „ ì¢…ëª©)
                df_inst = stock.get_market_net_purchases_of_equities_by_ticker(cur_date_str, cur_date_str, "ALL", "ê¸°ê´€í•©ê³„")
                time.sleep(0.2)
                
                # ë°ì´í„° ë³‘í•©
                # ì¸ë±ìŠ¤: í‹°ì»¤
                combined_rows = []
                
                # ì™¸êµ­ì¸ ë°ì´í„° ê¸°ì¤€ ë£¨í”„ (ë˜ëŠ” set of tickers)
                # target_tickersì— ìˆëŠ” ê²ƒë§Œ í•„í„°ë§
                
                # ì¸ë±ìŠ¤(í‹°ì»¤)ë¥¼ setìœ¼ë¡œ í™•ë³´
                available_tickers = set(df_foreign.index) | set(df_inst.index)
                target_intersect = available_tickers & tickers_set
                
                for ticker in target_intersect:
                    f_val = 0
                    i_val = 0
                    
                    if ticker in df_foreign.index:
                        # ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ ì»¬ëŸ¼ í™•ì¸
                        if 'ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ' in df_foreign.columns:
                            f_val = df_foreign.loc[ticker, 'ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ']
                    
                    if ticker in df_inst.index:
                        if 'ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ' in df_inst.columns:
                            i_val = df_inst.loc[ticker, 'ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ']
                            
                    combined_rows.append({
                        'date': cur_date_fmt,
                        'ticker': ticker,
                        'foreign_buy': int(f_val),
                        'inst_buy': int(i_val)
                    })
                
                if combined_rows:
                    new_data_list.extend(combined_rows)
                    log(f"[Supply Trend] {cur_date_fmt} ìˆ˜ì§‘ ì™„ë£Œ ({len(combined_rows)}ì¢…ëª©)", "DEBUG")
                else:
                    log(f"[Supply Trend] {cur_date_fmt} ë°ì´í„° ì—†ìŒ (íœ´ì¥ì¼?)", "DEBUG")
                
            except Exception as e:
                log(f"ìˆ˜ê¸‰ ë°ì´í„° ë‚ ì§œë³„ ìˆ˜ì§‘ ì‹¤íŒ¨ ({cur_date_str}): {e}", "WARNING")
            
            processed_days += 1
        
        # ê²°ê³¼ ì €ì¥
        if new_data_list:
            log("ìˆ˜ê¸‰ ë°ì´í„° ë³‘í•© ë° ì €ì¥ ì¤‘...", "DEBUG")
            new_df = pd.DataFrame(new_data_list)
            
            # [Added] ì£¼ìš” ì¢…ëª© Toss API ì •ë°€ ìˆ˜ê¸‰ ë³´ì • (ì •í™•ë„ í™•ë³´)
            if end_date_obj.date() == datetime.now().date():
                try:
                    from engine.toss_collector import TossCollector
                    collector = TossCollector()
                    
                    # 1. ì˜¤ëŠ˜ ë‚ ì§œ ë°ì´í„°ë§Œ ì¶”ì¶œ
                    today_mask = (new_df['date'] == cur_date_fmt)
                    today_df = new_df[today_mask].copy()
                    
                    if not today_df.empty:
                        # 2. ë³´ì • ëŒ€ìƒ ì¢…ëª© ì„ ì • (ì˜ˆ: ê±°ë˜ëŒ€ê¸ˆì´ í° ì¢…ëª© ìœ„ì£¼ë¡œ 300ê°œ)
                        # ê°€ê²© ë°ì´í„°ì™€ ë³‘í•©í•˜ì—¬ ê±°ë˜ëŒ€ê¸ˆ í™•ì¸
                        price_file = os.path.join(BASE_DIR, 'data', 'daily_prices.csv')
                        p_df = pd.read_csv(price_file, dtype={'ticker': str})
                        p_today = p_df[p_df['date'] == cur_date_fmt]
                        
                        merged = pd.merge(today_df, p_today[['ticker', 'trading_value']], on='ticker', how='left')
                        top_candidates = merged.sort_values('trading_value', ascending=False).head(300)['ticker'].tolist()
                        
                        log(f"Toss APIë¥¼ ì´ìš©í•œ ìˆ˜ê¸‰ ì •ë°€ ë³´ì • ì¤‘ (ëŒ€ìƒ: {len(top_candidates)}ì¢…ëª©)...", "DEBUG")
                        
                        success_count = 0
                        for t in top_candidates:
                            try:
                                # 1. Toss ì‹œë„
                                t_trend = collector.get_investor_trend(t, days=1)
                                if t_trend and (t_trend.get('foreign', 0) != 0 or t_trend.get('institution', 0) != 0):
                                    idx_mask = (new_df['date'] == cur_date_fmt) & (new_df['ticker'] == t)
                                    if any(idx_mask):
                                        new_df.loc[idx_mask, 'foreign_buy'] = int(t_trend.get('foreign', 0))
                                        new_df.loc[idx_mask, 'inst_buy'] = int(t_trend.get('institution', 0))
                                        success_count += 1
                                        continue
                                
                                # 2. Toss ì‹¤íŒ¨/ë°ì´í„° 0ì¸ ê²½ìš° Naver ì‹¤ì‹œê°„ ì‹œë„ (Fallback)
                                try:
                                    # data_sourcesì—ì„œ í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸° (Lazy import)
                                    from engine.data_sources import fetch_investor_trend_naver
                                    n_trend = fetch_investor_trend_naver(t)
                                    
                                    if n_trend and (n_trend.get('foreign', 0) != 0 or n_trend.get('institution', 0) != 0):
                                        idx_mask = (new_df['date'] == cur_date_fmt) & (new_df['ticker'] == t)
                                        if any(idx_mask):
                                            new_df.loc[idx_mask, 'foreign_buy'] = int(n_trend.get('foreign', 0))
                                            new_df.loc[idx_mask, 'inst_buy'] = int(n_trend.get('institution', 0))
                                            success_count += 1
                                            # log(f"  -> {t} Naver í´ë°± ì„±ê³µ", "DEBUG")
                                except:
                                    pass
                            except:
                                pass
                        
                        log(f"Toss ìˆ˜ê¸‰ ë³´ì • ì™„ë£Œ: {success_count}ê°œ ì¢…ëª© ì •ë°€ ì—…ë°ì´íŠ¸", "SUCCESS")
                except Exception as te:
                    log(f"Toss ìˆ˜ê¸‰ ë³´ì • ì‹¤íŒ¨: {te}", "WARNING")

            if not existing_df.empty:
                final_df = pd.concat([existing_df, new_df])
                final_df = final_df.drop_duplicates(subset=['date', 'ticker'], keep='last')
            else:
                final_df = new_df
            
            # ì •ë ¬
            final_df = final_df.sort_values(['ticker', 'date'])
            final_df.to_csv(file_path, index=False, encoding='utf-8-sig')
            log(f"ìˆ˜ê¸‰ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì´ {len(final_df)}í–‰ (ì‹ ê·œ {len(new_data_list)}í–‰)", "DEBUG")
            return True
        else:
            log("ìˆ˜ê¸‰ ë°ì´í„°: ì‹ ê·œ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", "SUCCESS")
            return True

    except Exception as e:
        log(f"ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", "ERROR")
        import traceback
        traceback.print_exc()
        return False


def create_signals_log(target_date=None, run_ai=True):
    """VCP ì‹œê·¸ë„ ë¡œê·¸ ìƒì„± - Using SmartMoneyScreener (engine.screener)"""
    log("VCP ì‹œê·¸ë„ ë¶„ì„ ì¤‘ (SmartMoneyScreener)...")
    try:
        from engine.screener import SmartMoneyScreener
        
        # ìŠ¤í¬ë¦¬ë„ˆ ì‹¤í–‰ (KOSPI+KOSDAQ ì „ì²´ ë¶„ì„)
        screener = SmartMoneyScreener(target_date=target_date)
        df_result = screener.run_screening(max_stocks=600)
        
        signals = []
        if not df_result.empty:
            for _, row in df_result.iterrows():
                signals.append({
                    'ticker': row['ticker'],
                    'name': row['name'],
                    'signal_date': target_date if target_date else datetime.now().strftime('%Y-%m-%d'),
                    'market': row['market'],
                    'status': 'OPEN',
                    'score': round(row['score'], 1),
                    'contraction_ratio': row.get('contraction_ratio', 0),
                    'entry_price': int(row['entry_price']),
                    'foreign_5d': int(row['foreign_net_5d']),
                    'inst_5d': int(row['inst_net_5d']),
                    'vcp_score': row.get('vcp_score', 0), # [FIX] Use calculated score from screener
                    'current_price': int(row.get('entry_price', 0)) # Approximation or need fetch
                })
        
        log(f"ì´ {len(signals)}ê°œ ì‹œê·¸ë„ ê°ì§€")
        
        # ì ìˆ˜ ë†’ì€ ìˆœ ì •ë ¬ (Top 20 ì œí•œ)
        signals = sorted(signals, key=lambda x: x['score'], reverse=True)[:20]
        
        # AI ë¶„ì„ ì‹¤í–‰ (ì˜µì…˜)
        if run_ai and signals:
            try:
                log(f"[AI Analysis] ê°ì§€ëœ {len(signals)}ê°œ ì‹œê·¸ë„ì— ëŒ€í•´ AI ì •ë°€ ë¶„ì„ ìˆ˜í–‰...", "INFO")
                from engine.vcp_ai_analyzer import get_vcp_analyzer
                analyzer = get_vcp_analyzer()
                
                # ë¹„ë™ê¸° ì‹¤í–‰ì„ ìœ„í•œ ë£¨í”„ ê°€ì ¸ì˜¤ê¸°
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                # ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰
                ai_results = loop.run_until_complete(analyzer.analyze_batch(signals))
                
                # ê²°ê³¼ ì €ì¥
                if ai_results:
                    date_str = signals[0]['signal_date'].replace('-', '')
                    
                    # 1. ai_analysis_results.jsonì— ì €ì¥ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                    ai_filename = f'ai_analysis_results_{date_str}.json'
                    ai_filepath = os.path.join(BASE_DIR, 'data', ai_filename)
                    
                    save_data = {
                        'generated_at': datetime.now().isoformat(),
                        'signal_date': signals[0]['signal_date'],
                        'signals': list(ai_results.values())
                    }
                    
                    with open(ai_filepath, 'w', encoding='utf-8') as f:
                        json.dump(save_data, f, ensure_ascii=False, indent=2)
                        
                    latest_path = os.path.join(BASE_DIR, 'data', 'ai_analysis_results.json')
                    with open(latest_path, 'w', encoding='utf-8') as f:
                         json.dump(save_data, f, ensure_ascii=False, indent=2)
                    
                    # 2. kr_ai_analysis.jsonì—ë„ ì €ì¥ (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ í˜•ì‹)
                    # VCP ì‹œê·¸ë„ ì •ë³´ + AI ë¶„ì„ ê²°ê³¼ + ë‰´ìŠ¤ í†µí•©
                    kr_ai_signals = []
                    
                    # ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
                    news_collector = None
                    try:
                        from engine.collectors import EnhancedNewsCollector
                        from engine.config import app_config
                        news_collector = EnhancedNewsCollector(app_config)
                        log("[AI Analysis] ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ", "INFO")
                    except Exception as news_init_err:
                        log(f"[AI Analysis] ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {news_init_err}", "WARNING")
                    
                    for signal in signals:
                        ticker = signal.get('ticker', '')
                        name = signal.get('name', '')
                        ai_data = ai_results.get(ticker, {})
                        
                        # ë‰´ìŠ¤ ìˆ˜ì§‘ (ìµœëŒ€ 5ê°œ)
                        news_items = []
                        if news_collector:
                            try:
                                news_list = asyncio.get_event_loop().run_until_complete(
                                    news_collector.get_stock_news(ticker, limit=5, name=name)
                                )
                                for news in news_list:
                                    news_items.append({
                                        'title': getattr(news, 'title', str(news)),
                                        'url': getattr(news, 'url', ''),
                                        'source': getattr(news, 'source', 'Naver'),
                                        'date': getattr(news, 'date', '')
                                    })
                            except Exception as news_err:
                                log(f"[AI Analysis] {name} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {news_err}", "WARNING")
                        
                        # í˜„ì¬ê°€ ë° ìˆ˜ìµë¥  ëª…ì‹œì  ê³„ì‚°
                        curr_p = int(signal.get('current_price', signal.get('entry_price', 0)))
                        entry_p = int(signal.get('entry_price', curr_p))
                        ret_p = round(((curr_p - entry_p) / entry_p * 100), 2) if entry_p > 0 else 0

                        kr_signal = {
                            'ticker': ticker,
                            'name': name,
                            'market': signal.get('market', 'KOSPI'),
                            'score': signal.get('score', 0),
                            'contraction_ratio': signal.get('contraction_ratio', 0),
                            'foreign_5d': signal.get('foreign_5d', 0),
                            'inst_5d': signal.get('inst_5d', 0),
                            'entry_price': entry_p,
                            'current_price': curr_p,
                            'return_pct': ret_p,
                            'vcp_score': signal.get('vcp_score', 0),
                            # AI ë¶„ì„ ê²°ê³¼ í†µí•©
                            'gemini_recommendation': ai_data.get('gemini_recommendation'),
                            'gpt_recommendation': ai_data.get('gpt_recommendation'),
                            'perplexity_recommendation': ai_data.get('perplexity_recommendation'),
                            # ë‰´ìŠ¤ ë°ì´í„° ì¶”ê°€
                            'news': news_items,
                        }
                        kr_ai_signals.append(kr_signal)
                    
                    # ì‹œì¥ ì§€ìˆ˜ ë°ì´í„° ìˆ˜ì§‘
                    market_indices = {}
                    try:
                        from pykrx import stock
                        today_str = datetime.now().strftime('%Y%m%d')
                        kospi = stock.get_index_ohlcv(today_str, today_str, "1001")  # KOSPI
                        kosdaq = stock.get_index_ohlcv(today_str, today_str, "2001")  # KOSDAQ
                        
                        if not kospi.empty:
                            market_indices['kospi'] = {
                                'value': float(kospi['ì¢…ê°€'].iloc[-1]) if len(kospi) > 0 else 0,
                                'change_pct': float(kospi['ë“±ë½ë¥ '].iloc[-1]) if len(kospi) > 0 and 'ë“±ë½ë¥ ' in kospi.columns else 0
                            }
                        if not kosdaq.empty:
                            market_indices['kosdaq'] = {
                                'value': float(kosdaq['ì¢…ê°€'].iloc[-1]) if len(kosdaq) > 0 else 0,
                                'change_pct': float(kosdaq['ë“±ë½ë¥ '].iloc[-1]) if len(kosdaq) > 0 and 'ë“±ë½ë¥ ' in kosdaq.columns else 0
                            }
                    except Exception as idx_e:
                        log(f"[AI Analysis] ì‹œì¥ ì§€ìˆ˜ ìˆ˜ì§‘ ì‹¤íŒ¨ (ë¬´ì‹œ): {idx_e}", "WARNING")
                    
                    kr_ai_data = {
                        'market_indices': market_indices,
                        'signals': kr_ai_signals,
                        'generated_at': datetime.now().isoformat(),
                        'signal_date': signals[0]['signal_date']
                    }
                    
                    kr_ai_path = os.path.join(BASE_DIR, 'data', 'kr_ai_analysis.json')
                    with open(kr_ai_path, 'w', encoding='utf-8') as f:
                        json.dump(kr_ai_data, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
                    
                    # ë‚ ì§œë³„ íˆìŠ¤í† ë¦¬ë„ ì €ì¥
                    kr_ai_history_path = os.path.join(BASE_DIR, 'data', f'kr_ai_analysis_{date_str}.json')
                    with open(kr_ai_history_path, 'w', encoding='utf-8') as f:
                        json.dump(kr_ai_data, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
                         
                    log(f"[AI Analysis] ë¶„ì„ ì™„ë£Œ ë° ì €ì¥: {ai_filename}, kr_ai_analysis.json", "SUCCESS")
                
            except Exception as e:
                log(f"[AI Analysis] ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", "ERROR")
                import traceback
                traceback.print_exc()
        


        # [FIX] AI ë¶„ì„ ê²°ê³¼ë¥¼ signals ë¦¬ìŠ¤íŠ¸ì— ë³‘í•© (CSV ì €ì¥ì„ ìœ„í•´)
        if run_ai and signals and 'ai_results' in locals() and ai_results:
            try:
                for signal in signals:
                    ticker = signal['ticker']
                    if ticker in ai_results:
                        ai_data = ai_results[ticker]
                        
                        # Gemini ê²°ê³¼ ìš°ì„ 
                        gemini = ai_data.get('gemini_recommendation')
                        if gemini:
                            signal['ai_action'] = gemini.get('action', 'HOLD')
                            signal['ai_confidence'] = gemini.get('confidence', 0)
                            signal['ai_reason'] = gemini.get('reason', '')
                        else:
                            # ë‹¤ë¥¸ AI ê²°ê³¼ í´ë°±? (ì¼ë‹¨ Gemini ê¸°ì¤€)
                            signal['ai_action'] = 'N/A'
                            signal['ai_confidence'] = 0
                            signal['ai_reason'] = 'ë¶„ì„ ì‹¤íŒ¨'
            except Exception as e:
                log(f"AI ê²°ê³¼ ë³‘í•© ì¤‘ ì˜¤ë¥˜: {e}", "WARNING")

        # [Fix] jongga_v2_results_{date}.json ì €ì¥ (Messenger/Dashboard ì—°ë™ìš©)
        try:
            from engine.market_gate import MarketGate
            market_gate = MarketGate()
            market_status = market_gate.analyze()
            
            # Grade ë¶„í¬ ê³„ì‚°
            grade_counts = {'S': 0, 'A': 0, 'B': 0, 'C': 0, 'D': 0}
            # signalsì—ëŠ” grade ì •ë³´ê°€ ì—†ì„ ìˆ˜ ìˆìŒ (Screener ê²°ê³¼ì—” scoreë§Œ ìˆìŒ)
            # Score ê¸°ë°˜ ì¶”ì • (Scorer ë¡œì§ ì°¸ê³ : ~80:S, ~70:A, ~60:B)
            for s in signals:
                score = float(s.get('score', 0))
                if score >= 80: grade_counts['S'] += 1
                elif score >= 70: grade_counts['A'] += 1
                elif score >= 60: grade_counts['B'] += 1
                else: grade_counts['C'] += 1
                
            jongga_data = {
                "date": target_date if target_date else datetime.now().strftime('%Y-%m-%d'),
                "total_candidates": len(signals), # ì „ì²´ í›„ë³´ (ì—¬ê¸°ì„  Top 20ë§Œ ë‚¨ì•˜ì„ ìˆ˜ ìˆìŒ, ì›ë³¸ df_result ì‚¬ìš© ê¶Œì¥í•˜ì§€ë§Œ ì—¬ê¸°ì„  signals ì‚¬ìš©)
                "filtered_count": len(df_result) - len(signals) if 'df_result' in locals() else 0,
                "signals": signals,
                "by_grade": grade_counts,
                "by_market": {}, # ìƒëµ
                "processing_time_ms": 0,
                "market_status": market_status,
                "market_summary": "", # AI ìš”ì•½ì´ ìˆë‹¤ë©´ ì¶”ê°€ ê°€ëŠ¥
                "trending_themes": [],
                "updated_at": datetime.now().isoformat()
            }
            
            # ë‚ ì§œ í¬ë§· (YYYYMMDD)
            j_date_str = str(jongga_data['date']).replace('-', '')
            j_path = os.path.join(BASE_DIR, 'data', f'jongga_v2_results_{j_date_str}.json')
            
            with open(j_path, 'w', encoding='utf-8') as f:
                json.dump(jongga_data, f, indent=2, ensure_ascii=False)
            log(f"ë©”ì‹ ì € ì—°ë™ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {j_path}", "SUCCESS")
            
            # Latest íŒŒì¼ ì—…ë°ì´íŠ¸
            j_latest_path = os.path.join(BASE_DIR, 'data', 'jongga_v2_latest.json')
            with open(j_latest_path, 'w', encoding='utf-8') as f:
                json.dump(jongga_data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            log(f"jongga_v2_results ì €ì¥ ì‹¤íŒ¨: {e}", "WARNING")

        if signals:
            df_new = pd.DataFrame(signals)
            file_path = os.path.join(BASE_DIR, 'data', 'signals_log.csv')
            
            # ê¸°ì¡´ ë¡œê·¸ê°€ ìˆìœ¼ë©´ ë¡œë“œí•˜ì—¬ ë³‘í•© (Append & Deduplicate)
            if os.path.exists(file_path):
                try:
                    # íƒ€ì… ëª…ì‹œí•˜ì—¬ ë¡œë“œ (ì¤‘ë³µ ë°©ì§€ í•µì‹¬)
                    df_old = pd.read_csv(file_path, dtype={'ticker': str, 'signal_date': str})
                    df_old['ticker'] = df_old['ticker'].str.zfill(6)
                    
                    # ìƒˆ ë°ì´í„° í¬ë§· í†µì¼
                    df_new['ticker'] = df_new['ticker'].astype(str).str.zfill(6)
                    df_new['signal_date'] = df_new['signal_date'].astype(str)

                    # [ìˆ˜ì •] í•´ë‹¹ ë‚ ì§œì˜ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì¬ì‹¤í–‰ ì‹œ ì¤‘ë³µ ë°©ì§€)
                    current_date = str(df_new['signal_date'].iloc[0])
                    df_old = df_old[df_old['signal_date'] != current_date]

                    # ë³‘í•©
                    if df_old.empty and df_new.empty:
                         df_combined = pd.DataFrame()
                    elif df_old.empty:
                         df_combined = df_new
                    elif df_new.empty:
                         df_combined = df_old
                    else:
                         # concat ì‹œ ignore_index=True ê¶Œì¥
                         df_combined = pd.concat([df_old, df_new], ignore_index=True)
                         
                    # ì¤‘ë³µ ì œê±° (ì•ˆì „ì¥ì¹˜)
                    if not df_combined.empty:
                        df_combined = df_combined.drop_duplicates(subset=['signal_date', 'ticker'], keep='last')
                        # ì •ë ¬ (ìµœì‹  ë‚ ì§œ ìš°ì„ , ì ìˆ˜ ë†’ì€ ìˆœ)
                        df_combined = df_combined.sort_values(by=['signal_date', 'score'], ascending=[False, False])
                    
                    df_combined.to_csv(file_path, index=False, encoding='utf-8-sig')
                    # í•´ë‹¹ ë‚ ì§œ ë°ì´í„° ë°˜í™˜ (common.py ì—°ë™ìš©) -> init_data.pyì—ì„œëŠ” True ë°˜í™˜í•´ì•¼ í•¨
                    return True
                except Exception as e:
                    log(f"ê¸°ì¡´ ë¡œê·¸ ë³‘í•© ì‹¤íŒ¨: {e}, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤(ë®ì–´ì“°ê¸°).", "WARNING")
                    df_new.to_csv(file_path, index=False, encoding='utf-8-sig')
                    return True
            else:
                df_new.to_csv(file_path, index=False, encoding='utf-8-sig')
                return True
                
            log(f"VCP ì‹œê·¸ë„ ë¶„ì„ ì™„ë£Œ: {len(signals)} ì¢…ëª© ê°ì§€ (ëˆ„ì  ì €ì¥)", "SUCCESS")
            return True
        else:
            log("VCP ì¡°ê±´ ì¶©ì¡± ì¢…ëª© ì—†ìŒ", "WARNING")
            # ë¹ˆ ê²°ê³¼ íŒŒì¼ ìƒì„± (ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì•ˆí•¨)
            df = pd.DataFrame(columns=['ticker', 'name', 'signal_date', 'market', 'status', 'score', 'contraction_ratio', 'entry_price', 'foreign_5d', 'inst_5d'])
            file_path = os.path.join(BASE_DIR, 'data', 'signals_log.csv')
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            log("VCP ì¡°ê±´ ì¶©ì¡± ì¢…ëª© ì—†ìŒ - ë¹ˆ ê²°ê³¼ ì €ì¥", "INFO")
            return True
            
    except Exception as e:
        log(f"VCP ë¶„ì„ ì‹¤íŒ¨: {e}", "WARNING")
        # ë¹ˆ ê²°ê³¼ íŒŒì¼ ìƒì„± (ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì•ˆí•¨)
        df = pd.DataFrame(columns=['ticker', 'name', 'signal_date', 'market', 'status', 'score', 'contraction_ratio', 'entry_price', 'foreign_5d', 'inst_5d'])
        file_path = os.path.join(BASE_DIR, 'data', 'signals_log.csv')
        df.to_csv(file_path, index=False, encoding='utf-8-sig')
        log("VCP ë¶„ì„ ì˜¤ë¥˜ - ë¹ˆ ê²°ê³¼ ì €ì¥", "INFO")
        return True



def create_jongga_v2_latest():
    """ì¢…ê°€ë² íŒ… V2 ìµœì‹  ê²°ê³¼ ìƒì„± - Using Central SignalGenerator"""
    log("ì¢…ê°€ë² íŒ… V2 ë¶„ì„ ì¤‘ (SignalGenerator)...")
    try:
        from engine.generator import run_screener
        import asyncio

        # Run analysis (Sync wrapper for Async)
        # run_screener returns ScreenerResult object
        result = asyncio.run(run_screener())
        
        # Convert to JSON serializable structure
        if result:
            signals_json = [s.to_dict() for s in result.signals]
            
            output_data = {
                'date': result.date.strftime('%Y-%m-%d'),
                'total_candidates': result.total_candidates,
                'filtered_count': result.filtered_count,
                'scanned_count': getattr(result, 'scanned_count', 0),
                'signals': signals_json,
                'by_grade': result.by_grade,
                'by_market': result.by_market,
                'processing_time_ms': result.processing_time_ms,
                'market_status': result.market_status,
                'market_summary': result.market_summary,
                'trending_themes': result.trending_themes,
                'updated_at': datetime.now().isoformat()
            }

            # Save to JSON
            file_path = os.path.join(BASE_DIR, 'data', 'jongga_v2_latest.json')
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
                
            log(f"ì¢…ê°€ë² íŒ… V2 ë¶„ì„ ì™„ë£Œ: {len(signals_json)} ì¢…ëª© (SignalGenerator)", "SUCCESS")
            return True
        else:
            log("ì¢…ê°€ë² íŒ… ë¶„ì„ ê²°ê³¼ ì—†ìŒ (None returned)", "WARNING")
            return False

    except Exception as e:
        log(f"ì¢…ê°€ë² íŒ… ë¶„ì„ ì‹¤íŒ¨: {e}", "ERROR")
        import traceback
        traceback.print_exc()
        return False


def create_market_gate(target_date=None):
    """Market Gate ë°ì´í„° ìƒì„± (8ê°œ ì„¹í„°, KOSPI/KOSDAQ ì§€ìˆ˜ í¬í•¨) - ì‹¤ì‹œê°„ ë°ì´í„°"""
    log("Market Gate ë°ì´í„° ìƒì„± ì¤‘...")
    try:
        # ì‹¤ì‹œê°„ ì‹œì¥ ì§€ìˆ˜ ìˆ˜ì§‘
        indices = get_market_indices()
        kospi = indices['kospi']
        kosdaq = indices['kosdaq']
        
        # Market Gate ì ìˆ˜ ê³„ì‚° (KOSPI ë“±ë½ë¥  ê¸°ë°˜ ì„¸ë¶„í™”)
        change = kospi['change_pct']
        
        if change >= 2.0:
            gate_status = 'GREEN'
            gate_label = 'VERY BULLISH'
            gate_score = 90
        elif change >= 1.0:
            gate_status = 'GREEN'
            gate_label = 'BULLISH'
            gate_score = 75
        elif change >= 0.5:
            gate_status = 'YELLOW'
            gate_label = 'SLIGHTLY BULLISH'
            gate_score = 60
        elif change >= 0:
            gate_status = 'YELLOW'
            gate_label = 'NEUTRAL'
            gate_score = 50
        elif change >= -0.5:
            gate_status = 'YELLOW'
            gate_label = 'SLIGHTLY BEARISH'
            gate_score = 40
        elif change >= -1.0:
            gate_status = 'RED'
            gate_label = 'BEARISH'
            gate_score = 25
        else:
            gate_status = 'RED'
            gate_label = 'VERY BEARISH'
            gate_score = 10
        
        gate_data = {
            'status': gate_status,
            'score': gate_score,
            'label': gate_label,
            'reasons': [
                f"KOSPI {kospi['change_pct']:+.2f}% ë³€ë™",
                'ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ ì§€ì†',
                'ë°˜ë„ì²´ ì„¹í„° ê°•ì„¸ ì§€ì†'
            ],
            'sectors': get_sector_indices(),  # ì‹¤ì œ ì„¹í„° ë°ì´í„° ì‚¬ìš©
            'indices': {
                'kospi': {'value': kospi['value'], 'change_pct': kospi['change_pct']},
                'kosdaq': {'value': kosdaq['value'], 'change_pct': kosdaq['change_pct']}
            },
            'commodities': {
                'gold': indices.get('kr_gold', {'value': 0, 'change_pct': 0}),
                'silver': indices.get('kr_silver', {'value': 0, 'change_pct': 0}),
                'us_gold': indices.get('us_gold', {'value': 0, 'change_pct': 0}),
                'us_silver': indices.get('us_silver', {'value': 0, 'change_pct': 0})
            },
            'global_indices': {
                'sp500': indices.get('sp500', {'value': 0, 'change_pct': 0}),
                'nasdaq': indices.get('nasdaq', {'value': 0, 'change_pct': 0})
            },
            'crypto': {
                'btc': indices.get('btc', {'value': 0, 'change_pct': 0}),
                'eth': indices.get('eth', {'value': 0, 'change_pct': 0}),
                'xrp': indices.get('xrp', {'value': 0, 'change_pct': 0})
            },
            'metrics': {
                'kospi': kospi['value'],
                'kospi_ma20': kospi['value'] * 0.98,  # ê·¼ì‚¬ê°’
                'kospi_ma60': kospi['value'] * 0.96,  # ê·¼ì‚¬ê°’
                'kosdaq': kosdaq['value'],
                'kosdaq_ma20': kosdaq['value'] * 0.98,
                'usd_krw': 1345.5,
                'foreign_net_total': 1200000000000,
                'rsi': 62.5
            },
            'updated_at': datetime.now().isoformat()
        }

        file_path = os.path.join(BASE_DIR, 'data', 'market_gate.json')
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(gate_data, f, indent=2, ensure_ascii=False)
            
        # ë‚ ì§œë³„ ì•„ì¹´ì´ë¸Œ ì €ì¥
        if target_date:
             date_str = target_date.replace('-', '') if isinstance(target_date, str) else target_date.strftime('%Y%m%d')
        else:
             date_str = datetime.now().strftime('%Y%m%d')
        
        archive_path = os.path.join(BASE_DIR, 'data', f'market_gate_{date_str}.json')
        with open(archive_path, 'w', encoding='utf-8') as f:
             json.dump(gate_data, f, indent=2, ensure_ascii=False)
             
        log(f"Market Gate ë°ì´í„° ìƒì„± ì™„ë£Œ: {file_path}", "SUCCESS")
        return True

    except Exception as e:
        log(f"Market Gate ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}", "ERROR")
        return False

def create_kr_ai_analysis(target_date=None):
    """AI ë¶„ì„ ê²°ê³¼ ìƒì„± (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)"""
    log("AI ë¶„ì„ ì‹œì‘ (Real Mode)...")
    try:
        import sys
        # Root path ì¶”ê°€
        root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        if root_dir not in sys.path:
            sys.path.append(root_dir)
            
        from engine.kr_ai_analyzer import KrAiAnalyzer
        import pandas as pd
        import json
        
        # ë‚ ì§œ ì„¤ì •
        if not target_date:
            target_date = datetime.now().strftime('%Y-%m-%d')
            
        data_dir = os.path.join(BASE_DIR, 'data')
        signals_path = os.path.join(data_dir, 'signals_log.csv')
        
        if not os.path.exists(signals_path):
            log("VCP ì‹œê·¸ë„ íŒŒì¼ì´ ì—†ì–´ AI ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.", "WARNING")
            return
            
        # VCP ê²°ê³¼ ë¡œë“œ
        df = pd.read_csv(signals_path, dtype={'ticker': str, 'signal_date': str})
        if df.empty:
            log("VCP ì‹œê·¸ë„ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.", "WARNING")
            return

        # í•´ë‹¹ ë‚ ì§œ ë°ì´í„° í•„í„°ë§
        target_df = df[df['signal_date'] == str(target_date)].copy()
        
        if target_df.empty:
            # ë‚ ì§œ í¬ë§· ë¶ˆì¼ì¹˜ ê°€ëŠ¥ì„± ì²´í¬ (YYYY-MM-DD vs YYYYMMDD)
            alt_date = target_date.replace('-', '')
            target_df = df[df['signal_date'] == alt_date].copy()
            
        if target_df.empty:
            log(f"í•´ë‹¹ ë‚ ì§œ({target_date})ì˜ VCP ì‹œê·¸ë„ì´ ì—†ìŠµë‹ˆë‹¤.", "WARNING")
            return

        # [í•„ìˆ˜] ê¸°ì¡´ ë¶„ì„ íŒŒì¼ ì‚­ì œ (ì´ˆê¸°í™”)
        date_str_clean = str(target_date).replace('-', '')
        filename = f'ai_analysis_results_{date_str_clean}.json'
        filepath = os.path.join(data_dir, filename)
        
        if os.path.exists(filepath):
            try:
                os.remove(filepath)
                log(f"ê¸°ì¡´ AI ë¶„ì„ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {filename}", "INFO")
            except Exception as e:
                log(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}", "WARNING")

        # ë¶„ì„ ëŒ€ìƒ ì„ ì • (Score ìƒìœ„ 20ê°œ)
        if 'score' in target_df.columns:
            target_df['score'] = pd.to_numeric(target_df['score'], errors='coerce').fillna(0)
            target_df = target_df.sort_values('score', ascending=False)
            
        target_df = target_df.head(20)
        tickers = target_df['ticker'].tolist()
        
        log(f"AI ë¶„ì„ ëŒ€ìƒ: {len(tickers)} ì¢…ëª©")
        
        # ë¶„ì„ ì‹¤í–‰
        analyzer = KrAiAnalyzer()
        results = analyzer.analyze_multiple_stocks(tickers)
        
        # [Fix] CSVì˜ supply ë°ì´í„°ë¥¼ AI ê²°ê³¼ì— ë³‘í•©
        try:
            csv_data = {row['ticker']: row for _, row in target_df.iterrows()}
            for signal in results.get('signals', []):
                ticker = signal.get('ticker')
                if ticker in csv_data:
                    csv_row = csv_data[ticker]
                    
                    # ë°ì´í„° ë³‘í•© (íƒ€ì… ì•ˆì „ ì²˜ë¦¬)
                    try:
                        signal['foreign_5d'] = int(float(csv_row.get('foreign_5d', 0)))
                    except: signal['foreign_5d'] = 0
                        
                    try:
                        signal['inst_5d'] = int(float(csv_row.get('inst_5d', 0)))
                    except: signal['inst_5d'] = 0
                        
                    try:
                        signal['score'] = float(csv_row.get('score', 0))
                    except: signal['score'] = 0.0
                        
                    try:
                        signal['contraction_ratio'] = float(csv_row.get('contraction_ratio', 0))
                    except: signal['contraction_ratio'] = 0.0
                        
                    try:
                        signal['entry_price'] = int(float(csv_row.get('entry_price', 0)))
                    except: signal['entry_price'] = 0

                    try:
                        current_p = int(float(csv_row.get('current_price', 0)))
                        if current_p == 0:
                            current_p = signal['entry_price']
                        signal['current_price'] = current_p
                    except: signal['current_price'] = signal.get('entry_price', 0)
                        
                    try:
                        signal['vcp_score'] = float(csv_row.get('vcp_score', 0))
                    except: signal['vcp_score'] = 0.0
                        
                    signal['market'] = csv_row.get('market', signal.get('market', 'KOSPI'))
            
            log("AI ê²°ê³¼ì— Supply ë°ì´í„° ë³‘í•© ì™„ë£Œ", "INFO")
        except Exception as merge_e:
            log(f"ë°ì´í„° ë³‘í•© ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {merge_e}", "WARNING")
        
        # ë©”íƒ€ë°ì´í„°
        results['generated_at'] = datetime.now().isoformat()
        results['signal_date'] = target_date
        
        # ì‹œì¥ ì§€ìˆ˜ ë°ì´í„° ìˆ˜ì§‘ (frontend í˜¸í™˜ìš©)
        market_indices = {}
        try:
            from pykrx import stock
            today_str = datetime.now().strftime('%Y%m%d')
            kospi = stock.get_index_ohlcv(today_str, today_str, "1001")
            kosdaq = stock.get_index_ohlcv(today_str, today_str, "2001")
            
            if not kospi.empty:
                market_indices['kospi'] = {
                    'value': float(kospi['ì¢…ê°€'].iloc[-1]),
                    'change_pct': float(kospi['ë“±ë½ë¥ '].iloc[-1]) if 'ë“±ë½ë¥ ' in kospi.columns else 0
                }
            if not kosdaq.empty:
                market_indices['kosdaq'] = {
                    'value': float(kosdaq['ì¢…ê°€'].iloc[-1]),
                    'change_pct': float(kosdaq['ë“±ë½ë¥ '].iloc[-1]) if 'ë“±ë½ë¥ ' in kosdaq.columns else 0
                }
        except: pass
        
        results['market_indices'] = market_indices

        # ì €ì¥ (ai_analysis_results.json)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
            
        log(f"AI ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}", "SUCCESS")
        
        # ìµœì‹  íŒŒì¼ (ai_analysis_results.json)
        if target_date == datetime.now().strftime('%Y-%m-%d'):
            main_path = os.path.join(data_dir, 'ai_analysis_results.json')
            with open(main_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            # [Fix] kr_ai_analysis.json ìƒì„± (Frontendìš©)
            kr_ai_path = os.path.join(data_dir, 'kr_ai_analysis.json')
            with open(kr_ai_path, 'w', encoding='utf-8') as f:
                 json.dump(results, f, ensure_ascii=False, indent=2)
            log(f"Frontend ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ: {kr_ai_path}", "SUCCESS")
                
        return True

    except Exception as e:
        log(f"AI ë¶„ì„ ì‹¤íŒ¨: {e}", "ERROR")
        import traceback
        traceback.print_exc()
        return False

def create_kr_ai_analysis_with_key(target_dates=None, api_key=None):
    """
    [ì‚¬ìš©ì ìš”ì²­] API Keyë¥¼ ì£¼ì…í•˜ì—¬ AI ë¶„ì„ ì‹¤í–‰ (create_kr_ai_analysis ë³€í˜•)
    - ê³µìš© ë°°ì¹˜ ì‘ì—…ì´ ì•„ë‹ˆë¼, íŠ¹ì • ì‚¬ìš©ìì˜ ìš”ì²­ì— ì˜í•´ íŠ¸ë¦¬ê±°ë¨.
    - target_dates: ['YYYY-MM-DD', ...] or None
    - api_key: ì‚¬ìš©ìì˜ Google Gemini API Key (ì—†ìœ¼ë©´ ê³µìš© í‚¤ ì‚¬ìš© - ì •ì±…ì— ë”°ë¦„)
    """
    log(f"AI ì¬ë¶„ì„ ìš”ì²­ (Key Present: {bool(api_key)})", "INFO")
    
    try:
        import sys
        # Root path ì¶”ê°€
        root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        if root_dir not in sys.path:
            sys.path.append(root_dir)
            
        from kr_ai_analyzer import KrAiAnalyzer
        import pandas as pd
        import json
        
        # Analyzer ì´ˆê¸°í™”ì‹œ í‚¤ ì£¼ì…
        analyzer = KrAiAnalyzer(api_key=api_key)
        
        data_dir = os.path.join(BASE_DIR, 'data')
        signals_path = os.path.join(data_dir, 'signals_log.csv')
        
        if not os.path.exists(signals_path):
            log("VCP ì‹œê·¸ë„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", "WARNING")
            return {'count': 0}

        df = pd.read_csv(signals_path, dtype={'ticker': str, 'signal_date': str})
        if df.empty:
            return {'count': 0}

        # ë‚ ì§œ í•„í„°ë§
        if not target_dates:
            # ë‚ ì§œ ì—†ìœ¼ë©´ ìµœì‹  ë‚ ì§œ í•˜ë‚˜ë§Œ
            latest_date = df['signal_date'].max()
            target_dates = [latest_date]
            
        all_results = {}
        total_analyzed = 0
        
        for t_date in target_dates:
            log(f"Deep Analysis for date: {t_date}")
            
            # ë‚ ì§œ í¬ë§· ë§¤ì¹­
            target_df = df[df['signal_date'] == str(t_date)].copy()
            if target_df.empty:
                 alt_date = str(t_date).replace('-', '')
                 target_df = df[df['signal_date'] == alt_date].copy()
            
            if target_df.empty:
                continue
                
            # Score ìƒìœ„ ì¢…ëª© ì„ ì •
            if 'score' in target_df.columns:
                target_df['score'] = pd.to_numeric(target_df['score'], errors='coerce').fillna(0)
                target_df = target_df.sort_values('score', ascending=False)
            
            # ìµœëŒ€ 20ê°œ (Rate Limit ë° ì‹œê°„ ê³ ë ¤)
            target_df = target_df.head(20)
            tickers = target_df['ticker'].tolist()
            
            # ë¶„ì„ ì‹¤í–‰
            results = analyzer.analyze_multiple_stocks(tickers) # api_key ì‚¬ìš©ë¨
            
            if results and 'signals' in results:
                count = len(results['signals'])
                total_analyzed += count
                
                # ì €ì¥ (ë®ì–´ì“°ê¸°)
                date_str_clean = str(t_date).replace('-', '')
                filename = f'ai_analysis_results_{date_str_clean}.json'
                filepath = os.path.join(data_dir, filename)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                
                # ì˜¤ëŠ˜ ë‚ ì§œë©´ ë©”ì¸ íŒŒì¼ë„ ì—…ë°ì´íŠ¸
                if t_date == datetime.now().strftime('%Y-%m-%d'):
                    main_path = os.path.join(data_dir, 'ai_analysis_results.json')
                    with open(main_path, 'w', encoding='utf-8') as f:
                        json.dump(results, f, ensure_ascii=False, indent=2)
                        
        return {'count': total_analyzed}

    except Exception as e:
        log(f"AI ì¬ë¶„ì„ ì‹¤íŒ¨: {e}", "ERROR")
        import traceback
        traceback.print_exc()
        return {'error': str(e)}

def send_jongga_notification():
    """ì¢…ê°€ë² íŒ… V2 ê²°ê³¼ ì•Œë¦¼ ë°œì†¡"""
    try:
        import json
        import os
        from engine.messenger import Messenger
        from engine.models import ScreenerResult, Signal, ScoreDetail, ChecklistDetail, SignalStatus, Grade
        from datetime import datetime
        
        data_file = os.path.join(BASE_DIR, 'data', 'jongga_v2_latest.json')
        
        if os.path.exists(data_file):
            with open(data_file, 'r', encoding='utf-8') as f:
                file_data = json.load(f)
            
            if file_data and file_data.get('signals'):
                # ê°ì²´ ë³µì› (Messenger í˜¸í™˜ì„±)
                signals = []
                for i, s in enumerate(file_data.get('signals', [])):
                    # ScoreDetail ë³µì› (total í¬í•¨)
                    sc = s.get('score', {})
                    score_obj = ScoreDetail(**sc)
                    
                    # ChecklistDetail ë³µì›
                    cl = s.get('checklist', {})
                    checklist_obj = ChecklistDetail(**cl)
                    
                    # ë‚ ì§œ/ì‹œê°„
                    try:
                        sig_date = datetime.strptime(s.get('signal_date', datetime.now().strftime('%Y-%m-%d')), '%Y-%m-%d').date()
                    except:
                        sig_date = datetime.now().date()
                        
                    try:
                        created_at = datetime.fromisoformat(s.get('created_at', datetime.now().isoformat()))
                    except:
                        created_at = datetime.now()
                    
                    # Enum ì²˜ë¦¬
                    grade_val = s.get('grade')
                    if isinstance(grade_val, str):
                        try:
                            grade = Grade(grade_val)
                        except:
                            grade = grade_val
                    
                    status_val = s.get('status', 'waiting')
                    if isinstance(status_val, str):
                        try:
                            status = SignalStatus(status_val)
                        except:
                            status = SignalStatus.PENDING
                    
                    target_price = s.get('target_price', 0)
                    if target_price == 0:
                        target_price = s.get('target_price_1', 0)

                    signal_obj = Signal(
                        stock_code=s['stock_code'],
                        stock_name=s['stock_name'],
                        market=s.get('market', ''),
                        sector=s.get('sector', ''),
                        signal_date=sig_date,
                        signal_time=datetime.now(),
                        grade=grade,
                        score=score_obj,
                        checklist=checklist_obj,
                        news_items=s.get('news_items', []),
                        current_price=s.get('current_price', 0.0),
                        entry_price=s.get('entry_price', 0),
                        stop_price=s.get('stop_price', 0),
                        target_price=target_price,
                        r_value=s.get('r_value', 0.0),
                        position_size=s.get('position_size', 0.0),
                        quantity=s.get('quantity', 0),
                        r_multiplier=s.get('r_multiplier', 0.0),
                        trading_value=s.get('trading_value', 0),
                        change_pct=s.get('change_pct', 0.0),
                        status=status,
                        created_at=created_at,
                        volume_ratio=s.get('volume_ratio', 0.0),
                        themes=s.get('themes', []),
                        score_details=s.get('score_details', {})
                    )
                    signals.append(signal_obj)
                    
                # ScreenerResult ìƒì„±
                res_date = datetime.now().date()
                try:
                    date_val = file_data.get('date')
                    if date_val:
                        res_date = datetime.strptime(date_val, '%Y-%m-%d').date()
                except:
                    pass
                
                # Calculate statistics if missing
                by_grade = file_data.get('by_grade', {})
                if not by_grade:
                    from collections import Counter
                    grades = [str(s.grade.value if hasattr(s.grade, 'value') else s.grade) for s in signals]
                    by_grade = dict(Counter(grades))
                    
                by_market = file_data.get('by_market', {})
                if not by_market:
                    from collections import Counter
                    markets = [s.market for s in signals]
                    by_market = dict(Counter(markets))
                    
                result = ScreenerResult(
                    date=res_date,
                    total_candidates=file_data.get('total_candidates', 0),
                    filtered_count=len(signals),
                    scanned_count=file_data.get('scanned_count', 0),
                    signals=signals,
                    by_grade=by_grade,
                    by_market=by_market,
                    processing_time_ms=file_data.get('processing_time_ms', 0.0),
                    market_status=file_data.get('market_status'),
                    market_summary=file_data.get('market_summary', ""),
                    trending_themes=file_data.get('trending_themes', [])
                )
                
                messenger = Messenger()
                messenger.send_screener_result(result)
                log(f"ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ: {len(signals)}ê°œ ì‹ í˜¸", "SUCCESS")
            else:
                log("ë°œì†¡í•  ì‹ í˜¸ ì—†ìŒ (0ê°œ)", "INFO")
                
    except Exception as notify_error:
        log(f"ì•Œë¦¼ ë°œì†¡ ì¤‘ ì˜¤ë¥˜: {notify_error}", "ERROR")
        import traceback
        traceback.print_exc()

def main():
    log("ë°ì´í„° ì´ˆê¸°í™” ì‹œì‘...", "HEADER")
    data_dir = os.path.join(BASE_DIR, 'data')
    ensure_directory(data_dir)
    
    tasks = [
        create_korean_stocks_list,
        create_daily_prices,
        create_institutional_trend,
        create_signals_log,
        create_jongga_v2_latest,

        create_kr_ai_analysis  # AI ë¶„ì„ ì¶”ê°€
    ]

    
    success_count = 0
    total_tasks = len(tasks)
    
    for task in tasks:
        if task():
            success_count += 1
            
    print()
    log("ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ", "HEADER")
    print(f"ì™„ë£Œëœ ì‘ì—…: {success_count}/{total_tasks}")
    
    if success_count == total_tasks:
        log("ğŸ‰ ëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!", "SUCCESS")
        log("ë‹¤ìŒ ë‹¨ê³„: [python3 flask_app.py] ì‹¤í–‰ í›„ í”„ë¡ íŠ¸ì—”ë“œ í™•ì¸")
    else:
        log(f"âš ï¸ ì¼ë¶€ ë°ì´í„° ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ ({total_tasks - success_count}/{total_tasks}).", "WARNING")
        log("ìƒì„¸ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.", "WARNING")


def update_vcp_signals_recent_price():
    """VCP ì‹œê·¸ë„ ë¡œê·¸(signals_log.csv)ì˜ ìµœì‹  ê°€ê²© ì—…ë°ì´íŠ¸"""
    log("VCP ì‹œê·¸ë„ ìµœì‹  ê°€ê²© ì—…ë°ì´íŠ¸ ì‹œì‘...")
    try:
        file_path = os.path.join(BASE_DIR, 'data', 'signals_log.csv')
        if not os.path.exists(file_path):
            log("VCP ì‹œê·¸ë„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", "WARNING")
            return

        df = pd.read_csv(file_path, dtype={'ticker': str})
        
        # ì˜¤ëŠ˜ ë‚ ì§œ
        today_str = datetime.now().strftime('%Y%m%d')
        
        # ìµœì‹  ê°€ê²© ë°ì´í„° ë¡œë“œ (pykrx ì‚¬ìš© - ì¼ê´„ ìˆ˜ì§‘ ë°©ì‹)
        from pykrx import stock
        
        current_prices = {}
        updated_count = 0  # [Fix] Initialize counter
        
        # 1. KOSPI/KOSDAQ ì „ì²´ ì¢…ê°€ ì¼ê´„ ì¡°íšŒ (ë§¤ìš° ë¹ ë¦„)
        for market in ["KOSPI", "KOSDAQ"]:
            try:
                # get_market_ohlcv(date, market=...) ì‚¬ìš©
                df_market = stock.get_market_ohlcv(today_str, market=market)
                if not df_market.empty:
                    for ticker, row in df_market.iterrows():
                        # [Fix] Handle float strings (e.g. '2650.55')
                        try:
                            price = int(float(row['ì¢…ê°€']))
                        except:
                            price = 0
                            
                        if price > 0:
                            current_prices[str(ticker)] = price
            except Exception as e:
                log(f"{market} ì „ì²´ ê°€ê²© ì¡°íšŒ ì‹¤íŒ¨ fallback ì‹œë„: {e}", "WARNING")
        
        # 2. Toss Batch APIë¥¼ ì´ìš©í•œ ìµœì¢… ë³´ì • (ì‹ ë¢°ë„ ë†’ìŒ)
        try:
            from engine.toss_collector import TossCollector
            collector = TossCollector()
            # ì‹œê·¸ë„ì— ìˆëŠ” ì¢…ëª©ë“¤ ìš°ì„  ë³´ì •
            sig_tickers = df['ticker'].unique().tolist()
            toss_data = collector.get_prices_batch(sig_tickers)
            if toss_data:
                for t, data in toss_data.items():
                    if data['current'] > 0:
                        current_prices[str(t)] = int(float(data['current']))
                log(f"Toss Batch APIë¥¼ í†µí•œ {len(toss_data)}ê°œ ì¢…ëª© ìµœì¢… ë³´ì • ì™„ë£Œ", "SUCCESS")
        except Exception as te:
            log(f"Toss ë³´ì • ì‹œë„ ì‹¤íŒ¨: {te}", "WARNING")
        
        # 3. ë¶€ì¡±í•œ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ê°œë³„ ì¡°íšŒ (Fallback - Naver)
        tickers = df['ticker'].unique()
        missing_tickers = [t for t in tickers if t not in current_prices]
        
        if missing_tickers:
            log(f"ëˆ„ë½ëœ {len(missing_tickers)}ê°œ ì¢…ëª© ê°œë³„ ì¡°íšŒ ì‹œì‘ (Fallback)...")
            for ticker in missing_tickers:
                try:
                    data = fetch_stock_price(ticker)
                    if data and 'price' in data:
                        current_prices[ticker] = int(float(data['price']))
                except:
                    pass
        
        log(f"ì´ {len(current_prices)}ê°œ ì¢…ëª©ì˜ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ. ì—…ë°ì´íŠ¸ ì ìš© ì¤‘...")
        
        # ë°ì´í„°í”„ë ˆì„ ì—…ë°ì´íŠ¸
        for idx, row in df.iterrows():
            ticker = row['ticker']
            if ticker in current_prices:
                current_p = current_prices[ticker]
                entry_p = row['entry_price']
                
                df.at[idx, 'current_price'] = current_p
                if entry_p > 0:
                    ret = ((current_p - entry_p) / entry_p) * 100
                    df.at[idx, 'return_pct'] = round(ret, 2)
                
                updated_count += 1
        
        # ì €ì¥
        df.to_csv(file_path, index=False, encoding='utf-8-sig')
        log(f"VCP ì‹œê·¸ë„ ê°€ê²© ì—…ë°ì´íŠ¸ ì™„ë£Œ: {updated_count}ê±´ ê°±ì‹ ", "SUCCESS")
        
        # kr_ai_analysis.jsonë„ ë™ê¸°í™” (ì„ íƒ ì‚¬í•­)
        update_kr_ai_analysis_prices(current_prices)
        
    except Exception as e:
        log(f"ê°€ê²© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}", "ERROR")

def update_kr_ai_analysis_prices(price_map):
    """kr_ai_analysis.json íŒŒì¼ì˜ ê°€ê²© ì •ë³´ë„ ì—…ë°ì´íŠ¸"""
    try:
        kr_ai_path = os.path.join(BASE_DIR, 'data', 'kr_ai_analysis.json')
        if not os.path.exists(kr_ai_path):
            return
            
        with open(kr_ai_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        updated = False
        if 'signals' in data:
            for signal in data['signals']:
                ticker = signal.get('ticker')
                if ticker in price_map:
                    current_p = price_map[ticker]
                    entry_p = signal.get('entry_price', current_p)
                    
                    signal['current_price'] = current_p
                    if entry_p > 0:
                        signal['return_pct'] = round(((current_p - entry_p) / entry_p) * 100, 2)
                    updated = True
        
        if updated:
            with open(kr_ai_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
            log("kr_ai_analysis.json ê°€ê²© ë™ê¸°í™” ì™„ë£Œ", "INFO")
            
    except Exception as e:
        log(f"AI ë¶„ì„ íŒŒì¼ ê°€ê²© ë™ê¸°í™” ì‹¤íŒ¨: {e}", "WARNING")

if __name__ == '__main__':
    # ë¡œê¹… í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” (ì¤‘ë³µ ë°©ì§€ - ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ìš©)
    root = logging.getLogger()
    if root.handlers:
        for h in root.handlers[:]:
            root.removeHandler(h)
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    if len(sys.argv) > 1:
        cmd = sys.argv[1]
        if cmd == "init-prices":
            create_daily_prices()
        elif cmd == "init-inst":
            create_institutional_trend()
        elif cmd == "init-stocks":
            create_korean_stocks_list()
        elif cmd == "vcp-signal":
             # íŠ¹ì • ë‚ ì§œ ì§€ì • ê°€ëŠ¥ (YYYY-MM-DD)
            target_date = sys.argv[2] if len(sys.argv) > 2 else None
            create_signals_log(target_date)
        elif cmd == "ai-analysis":
            create_kr_ai_analysis()
        elif cmd == "update-prices":
            update_vcp_signals_recent_price()
        elif cmd == "all":
            log("ì „ì²´ ë°ì´í„° ì´ˆê¸°í™” ì‹œì‘...")
            create_korean_stocks_list()
            create_daily_prices()
            create_institutional_trend()
            create_signals_log() # VCP ë¶„ì„
            create_kr_ai_analysis() # AI ë¶„ì„
            log("ì „ì²´ ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ!", "SUCCESS")
    else:
        main()

