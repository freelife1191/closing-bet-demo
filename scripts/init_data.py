#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë°ì´í„° ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ (Data Initialization Script)
- ì‹¤ì‹œê°„ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ (yfinance)
- í•„ìš”í•œ ë°ì´í„° íŒŒì¼ ìƒì„±
- ì—ëŸ¬ ì²˜ë¦¬ ë° ì§„í–‰ë¥  í‘œì‹œ ê°œì„ 
"""

import os
import sys
import pandas as pd
import numpy as np
import json
import socket
import yfinance as yf
import time
import random
import logging
from datetime import datetime, timedelta

# [FIX] Filter out pykrx's broken logging calls
class PykrxFilter(logging.Filter):
    def filter(self, record):
        # pykrx.website.comm.util calls logging.info(args, kwargs) which causes TypeError
        if 'pykrx' in record.pathname and 'util.py' in record.pathname:
            return False
        return True

# Apply filter to root logger (Logger.filter runs before handlers/formatting)
logging.getLogger().addFilter(PykrxFilter())
# If no handlers yet (basicConfig not called), we might need to add it later or rely on basicConfig


# ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ ì„¤ì • (30ì´ˆ) - ë¬´í•œ ëŒ€ê¸° ë°©ì§€
socket.setdefaulttimeout(30)

# Import shared state for stop logic
try:
    import engine.shared as shared_state
except ImportError:
    import sys, os
    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if root_dir not in sys.path:
        sys.path.append(root_dir)
    try:
        import engine.shared as shared_state
    except ImportError:
        class MockShared:
            STOP_REQUESTED = False
        shared_state = MockShared()

# Custom JSON encoder for numpy types
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.bool_):
            return bool(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super().default(obj)

# yfinance for real market data
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
except ImportError:
    YFINANCE_AVAILABLE = False

# ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, BASE_DIR)
import asyncio

from engine.config import config, app_config
from engine.collectors import EnhancedNewsCollector
from engine.llm_analyzer import LLMAnalyzer
from engine.market_gate import MarketGate

# =====================================================
# ì£¼ë§/íœ´ì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =====================================================

def get_last_trading_date(reference_date=None):
    """
    ë§ˆì§€ë§‰ ê°œì¥ì¼ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    - ì£¼ë§(í† /ì¼)ì¸ ê²½ìš° ê¸ˆìš”ì¼ë¡œ ì´ë™
    - ê¸ˆìš”ì¼ì´ íœ´ì¼ì¸ ê²½ìš° pykrxë¥¼ í†µí•´ ì‹¤ì œ ë§ˆì§€ë§‰ ê°œì¥ì¼ í™•ì¸
    
    Args:
        reference_date: ê¸°ì¤€ ë‚ ì§œ (datetime ê°ì²´). Noneì´ë©´ ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©.
    
    Returns:
        tuple: (last_trading_date_str, last_trading_date_obj)
               - last_trading_date_str: 'YYYYMMDD' í˜•ì‹ì˜ ë¬¸ìì—´
               - last_trading_date_obj: datetime ê°ì²´
    """
    if reference_date is None:
        reference_date = datetime.now()
    
    target_date = reference_date
    
    # 1ì°¨: ì£¼ë§ ì²˜ë¦¬ (í† /ì¼ â†’ ê¸ˆìš”ì¼ë¡œ ì´ë™)
    if target_date.weekday() == 5:  # í† ìš”ì¼
        target_date -= timedelta(days=1)
    elif target_date.weekday() == 6:  # ì¼ìš”ì¼
        target_date -= timedelta(days=2)
    
    # 2ì°¨: pykrxë¥¼ í†µí•´ ì‹¤ì œ ê°œì¥ì¼ í™•ì¸
    try:
        from pykrx import stock
        
        # ìµœê·¼ 10ì¼ê°„ ê±°ë˜ì¼ ì¡°íšŒ (íœ´ì¼ ì—°ì† ëŒ€ë¹„)
        start_check = (target_date - timedelta(days=10)).strftime('%Y%m%d')
        end_check = target_date.strftime('%Y%m%d')
        
        # KOSPI ì§€ìˆ˜ì˜ OHLCVë¡œ ê°œì¥ì¼ í™•ì¸
        kospi_data = stock.get_index_ohlcv_by_date(start_check, end_check, "1001")
        
        if not kospi_data.empty:
            # ë§ˆì§€ë§‰ ê±°ë˜ì¼ì„ ê°€ì ¸ì˜´
            last_trading_date = kospi_data.index[-1]
            last_trading_date_str = last_trading_date.strftime('%Y%m%d')
            log(f"ë§ˆì§€ë§‰ ê°œì¥ì¼ í™•ì¸: {last_trading_date_str}", "DEBUG")
            return last_trading_date_str, last_trading_date
        else:
            # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê³„ì‚°ëœ ë‚ ì§œ ì‚¬ìš©
            log(f"pykrx ë°ì´í„° ì—†ìŒ, ê³„ì‚°ëœ ë‚ ì§œ ì‚¬ìš©: {target_date.strftime('%Y%m%d')}", "DEBUG")
            
    except ImportError:
        log("pykrx ë¯¸ì„¤ì¹˜ - ì£¼ë§ ì²˜ë¦¬ë§Œ ì ìš©", "WARNING")
    except Exception as e:
        # ì§€ìˆ˜ëª… KeyError ë“± pykrx ë‚´ë¶€ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¡°ìš©íˆ ë„˜ì–´ê°€ê³  ì£¼ë§ ì²˜ë¦¬ë§Œ ì ìš©
        log(f"ê°œì¥ì¼ í™•ì¸ ì‹¤íŒ¨ (pykrx): {e}. ê¸°ë³¸ ì£¼ë§ ì²˜ë¦¬ë§Œ ì ìš©í•©ë‹ˆë‹¤.", "DEBUG")
    
    # í´ë°±: ì£¼ë§ ì²˜ë¦¬ë§Œ ëœ ë‚ ì§œ ë°˜í™˜
    return target_date.strftime('%Y%m%d'), target_date


# =====================================================
# ì‹¤ì‹œê°„ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
# =====================================================

def fetch_market_indices():
    """KOSPI/KOSDAQ ì‹¤ì‹œê°„ ì§€ìˆ˜ ìˆ˜ì§‘"""
    indices = {
        'kospi': {'value': 2650.0, 'change_pct': 0.0, 'prev_close': 2650.0},
        'kosdaq': {'value': 850.0, 'change_pct': 0.0, 'prev_close': 850.0}
    }
    
    if not YFINANCE_AVAILABLE:
        log("yfinance ë¯¸ì„¤ì¹˜ - ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©", "WARNING")
        return indices
    
    try:
        # yfinance ì¼ê´„ ë‹¤ìš´ë¡œë“œ (threads=False í•„ìˆ˜)
        ticker_map = {
            'kospi': '^KS11', 'kosdaq': '^KQ11',
            'gold': '411060.KS', 'silver': '144600.KS',
            'us_gold': 'GC=F', 'us_silver': 'SI=F',
            'sp500': '^GSPC', 'nasdaq': '^IXIC',
            'btc': 'BTC-USD', 'eth': 'ETH-USD', 'xrp': 'XRP-USD'
        }
        
        symbols = list(ticker_map.values())
        
        # ì•ˆì „í•œ ë‹¤ìš´ë¡œë“œ (ìŠ¤ë ˆë“œ ë¹„í™œì„±í™”)
        data = yf.download(symbols, period="5d", progress=False, threads=False)
        
        # ë°ì´í„° ì¶”ì¶œ Helper
        def get_val_change_prev(ticker):
             try:
                # MultiIndex ì²˜ë¦¬
                if isinstance(data.columns, pd.MultiIndex):
                    if ticker in data['Close'].columns:
                        series = data['Close'][ticker].dropna()
                    else:
                        return 0, 0, 0
                else: # ë‹¨ì¼ í‹°ì»¤ í˜¹ì€ Flattened
                    if ticker in data.columns:
                        series = data[ticker].dropna()
                    elif 'Close' in data.columns:
                        series = data['Close'].dropna()
                    else:
                        return 0, 0, 0
                
                if series.empty: return 0, 0, 0
                
                latest = float(series.iloc[-1])
                prev = float(series.iloc[-2]) if len(series) >= 2 else latest
                change = ((latest - prev) / prev) * 100 if prev != 0 else 0
                return latest, change, prev
             except:
                return 0, 0, 0

        # ê²°ê³¼ ë§¤í•‘
        ks_val, ks_chg, ks_prev = get_val_change_prev(ticker_map['kospi'])
        indices['kospi'] = {'value': round(ks_val, 2), 'change_pct': round(ks_chg, 2), 'prev_close': round(ks_prev, 2)}
        
        kq_val, kq_chg, kq_prev = get_val_change_prev(ticker_map['kosdaq'])
        indices['kosdaq'] = {'value': round(kq_val, 2), 'change_pct': round(kq_chg, 2), 'prev_close': round(kq_prev, 2)}
        
        g_val, g_chg, g_prev = get_val_change_prev(ticker_map['gold'])
        indices['kr_gold'] = {'value': round(g_val, 0), 'change_pct': round(g_chg, 2), 'prev_close': round(g_prev, 0)}
        
        s_val, s_chg, s_prev = get_val_change_prev(ticker_map['silver'])
        indices['kr_silver'] = {'value': round(s_val, 0), 'change_pct': round(s_chg, 2), 'prev_close': round(s_prev, 0)}
        
        ug_val, ug_chg, ug_prev = get_val_change_prev(ticker_map['us_gold'])
        indices['us_gold'] = {'value': round(ug_val, 2), 'change_pct': round(ug_chg, 2), 'prev_close': round(ug_prev, 2)}
        
        us_val, us_chg, us_prev = get_val_change_prev(ticker_map['us_silver'])
        indices['us_silver'] = {'value': round(us_val, 2), 'change_pct': round(us_chg, 2), 'prev_close': round(us_prev, 2)}
        
        sp_val, sp_chg, sp_prev = get_val_change_prev(ticker_map['sp500'])
        indices['sp500'] = {'value': round(sp_val, 2), 'change_pct': round(sp_chg, 2), 'prev_close': round(sp_prev, 2)}
        
        nd_val, nd_chg, nd_prev = get_val_change_prev(ticker_map['nasdaq'])
        indices['nasdaq'] = {'value': round(nd_val, 2), 'change_pct': round(nd_chg, 2), 'prev_close': round(nd_prev, 2)}
        
        b_val, b_chg, b_prev = get_val_change_prev(ticker_map['btc'])
        indices['btc'] = {'value': round(b_val, 2), 'change_pct': round(b_chg, 2), 'prev_close': round(b_prev, 2)}
        
        e_val, e_chg, e_prev = get_val_change_prev(ticker_map['eth'])
        indices['eth'] = {'value': round(e_val, 2), 'change_pct': round(e_chg, 2), 'prev_close': round(e_prev, 2)}
        
        x_val, x_chg, x_prev = get_val_change_prev(ticker_map['xrp'])
        indices['xrp'] = {'value': round(x_val, 4), 'change_pct': round(x_chg, 2), 'prev_close': round(x_prev, 4)}
        
        log(f"ì‹œì¥ ì§€ìˆ˜ ìˆ˜ì§‘ ì™„ë£Œ: KOSPI {ks_val}, Gold {g_val}", "SUCCESS")
            
    except Exception as e:
        log(f"ì‹œì¥ ì§€ìˆ˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {e} - ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©", "WARNING")
    
    return indices


def fetch_sector_indices():
    """pykrxë¥¼ ì‚¬ìš©í•˜ì—¬ KOSPI ì„¹í„° ì§€ìˆ˜ ìˆ˜ì§‘"""
    # ì„¹í„° ì½”ë“œ ë§¤í•‘ (KOSPI ì—…ì¢… ì§€ìˆ˜ - KRX ê³µì‹ ì½”ë“œ)
    sector_codes = {
        '1012': 'ì² ê°•',       # ì² ê°•Â·ê¸ˆì†
        '1027': '2ì°¨ì „ì§€',   # ì „ê¸°Â·ì „ì (2ì°¨ì „ì§€, ë°˜ë„ì²´ í¬í•¨)
        '1024': 'ë°˜ë„ì²´',     # ë°˜ë„ì²´
        '1016': 'ìë™ì°¨',     # ìš´ìˆ˜ì¥ë¹„
        '1020': 'ì¦ê¶Œ',       # ê¸ˆìœµì—…
        '1018': 'ITì„œë¹„ìŠ¤',   # ì„œë¹„ìŠ¤ì—… (IT)
        '1001': 'KOSPI200',   # KOSPI 200
        '1026': 'ì€í–‰',       # ì€í–‰
    }
    
    sectors = []
    
    try:
        from pykrx import stock
        # from datetime import datetime, timedelta
        
        today = datetime.now().strftime('%Y%m%d')
        yesterday = (datetime.now() - timedelta(days=3)).strftime('%Y%m%d')
        
        for code, name in sector_codes.items():
            try:
                df = stock.get_index_ohlcv_by_date(yesterday, today, code)
                if not df.empty and len(df) >= 2:
                    current = df['ì¢…ê°€'].iloc[-1]
                    prev = df['ì¢…ê°€'].iloc[-2]
                    change_pct = ((current - prev) / prev) * 100 if prev > 0 else 0
                    
                    # ê°•ì„¸/ì•½ì„¸ íŒë‹¨
                    if change_pct > 1.0:
                        signal = 'bullish'
                    elif change_pct < -1.0:
                        signal = 'bearish'
                    else:
                        signal = 'neutral'
                    
                    # ì ìˆ˜ ê³„ì‚° (ë“±ë½ë¥  ê¸°ë°˜)
                    score = min(max(50 + int(change_pct * 10), 0), 100)
                    
                    sectors.append({
                        'name': name,
                        'signal': signal,
                        'change_pct': round(change_pct, 2),
                        'score': score
                    })
            except Exception as e:
                pass
        
        if sectors:
            log(f"ì„¹í„° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(sectors)}ê°œ ì„¹í„°", "SUCCESS")
        
    except ImportError:
        log("pykrx ë¯¸ì„¤ì¹˜ - ìƒ˜í”Œ ì„¹í„° ë°ì´í„° ì‚¬ìš©", "WARNING")
    except Exception as e:
        log(f"pykrx ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e} - ìƒ˜í”Œ ë°ì´í„° ìƒì„±", "WARNING")
        
        
        return False
    
    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œ ë°˜í™˜
    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ìƒ˜í”Œ ê¸ˆì§€)
    if not sectors:
        return []
    
    return sectors


def fetch_stock_price(ticker):
    """ê°œë³„ ì¢…ëª© ì‹¤ì‹œê°„ ê°€ê²© ìˆ˜ì§‘"""
    import requests
    
    # 1. Try yfinance
    if YFINANCE_AVAILABLE:
        try:
            # í•œêµ­ ì¢…ëª©ì€ .KS (KOSPI) ë˜ëŠ” .KQ (KOSDAQ) ì ‘ë¯¸ì‚¬ í•„ìš”
            yahoo_ticker = f"{ticker}.KS"
            
            # yfinance ì—ëŸ¬ ë¡œê·¸ ì–µì œ ë° ì•ˆì „í•œ ë‹¤ìš´ë¡œë“œ
            import logging as _logging
            yf_logger = _logging.getLogger('yfinance')
            original_level = yf_logger.level
            yf_logger.setLevel(_logging.CRITICAL)
            
            hist = pd.DataFrame()
            try:
                 hist = yf.download(yahoo_ticker, period='5d', progress=False, threads=False)
            except: pass
            finally:
                 yf_logger.setLevel(original_level)

            # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ (Close ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€)
            is_valid = False
            if not hist.empty:
                 if isinstance(hist.columns, pd.MultiIndex):
                      if 'Close' in hist.columns.get_level_values(0): is_valid = True
                 elif 'Close' in hist.columns:
                      is_valid = True
            
            if not is_valid:
                # KOSDAQ ì‹œë„
                yahoo_ticker = f"{ticker}.KQ"
                yf_logger.setLevel(_logging.CRITICAL)
                try:
                    hist = yf.download(yahoo_ticker, period='5d', progress=False, threads=False)
                except: pass
                finally:
                    yf_logger.setLevel(original_level)

            if not hist.empty:
                # Extract Close series safely
                close_series = None
                if isinstance(hist.columns, pd.MultiIndex):
                    try:
                        close_series = hist['Close']
                        if isinstance(close_series, pd.DataFrame): 
                            close_series = close_series.iloc[:, 0]
                    except:
                        # ìµœì•…ì˜ ê²½ìš° ì²« ë²ˆì§¸ ì»¬ëŸ¼
                        close_series = hist.iloc[:, 0]
                elif 'Close' in hist.columns:
                    close_series = hist['Close']
                else:
                    close_series = hist.iloc[:, 0]
                
                # Ensure it is a Series
                if isinstance(close_series, pd.DataFrame):
                    close_series = close_series.iloc[:, 0]

                if not close_series.empty:
                    # ìŠ¤ì¹¼ë¼ ê°’ ë³€í™˜ (.item() ì‚¬ìš©)
                    def get_val(s, idx):
                        val = s.iloc[idx]
                        return val.item() if hasattr(val, 'item') else val

                    current = get_val(close_series, -1)
                    prev = get_val(close_series, -2) if len(close_series) > 1 else current
                    
                    change_pct = ((current - prev) / prev) * 100 if prev > 0 else 0
                    return {
                        'price': round(float(current), 0),
                        'change_pct': round(float(change_pct), 2),
                        'prev_close': round(float(prev), 0)
                    }
        except Exception as e:
            pass

    # 2. Try Toss Securities API (Fallback 1)
    try:
        toss_url = f"https://wts-info-api.tossinvest.com/api/v3/stock-prices/details?productCodes=A{str(ticker).zfill(6)}"
        res = requests.get(toss_url, timeout=3)
        if res.status_code == 200:
            result = res.json().get('result', [])
            if result:
                item = result[0]
                current = float(item.get('close', 0))
                prev = float(item.get('base', 0)) # base appears to be previous close
                
                if current > 0:
                    change_pct = ((current - prev) / prev) * 100 if prev > 0 else 0
                    return {
                        'price': round(current, 0),
                        'change_pct': round(change_pct, 2),
                        'prev_close': round(prev, 0)
                    }
    except Exception as e:
        # log(f"Toss API Fallback failed for {ticker}: {e}", "WARNING")
        pass

    # 3. Try Naver Securities API (Fallback 2)
    try:
        naver_url = f"https://m.stock.naver.com/api/stock/{str(ticker).zfill(6)}/basic"
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(naver_url, headers=headers, timeout=3)
        if res.status_code == 200:
            data = res.json()
            if 'closePrice' in data:
                current = float(data['closePrice'].replace(',', ''))
                change_pct = float(data.get('fluctuationsRatio', 0))
                prev = float(data.get('compareToPreviousClosePrice', '0').replace(',', ''))
                
                # Naver 'compareToPreviousClosePrice' is the diff, not the price itself usually? 
                # Actually closer inspection of Naver API:
                # compareToPreviousClosePrice is the diff value. 
                # prev_close = current - diff (if up) or current + diff (if down)
                # But safer to calculate from percentage if available.
                # Let's derive prev from current and change_pct to be safe
                
                prev_calc = current / (1 + (change_pct / 100)) if change_pct != -100 else 0
                
                return {
                    'price': round(current, 0),
                    'change_pct': round(change_pct, 2),
                    'prev_close': round(prev_calc, 0)
                }
    except Exception as e:
        # log(f"Naver API Fallback failed for {ticker}: {e}", "WARNING")
        pass

    return None


# ì „ì—­ ìºì‹œ (ì—¬ëŸ¬ í•¨ìˆ˜ì—ì„œ ê³µìœ )
_market_indices_cache = None
_sector_indices_cache = None

def get_market_indices():
    """ìºì‹œëœ ì‹œì¥ ì§€ìˆ˜ ë°˜í™˜"""
    global _market_indices_cache
    if _market_indices_cache is None:
        _market_indices_cache = fetch_market_indices()
    return _market_indices_cache

def get_sector_indices():
    """ìºì‹œëœ ì„¹í„° ì§€ìˆ˜ ë°˜í™˜"""
    global _sector_indices_cache
    if _sector_indices_cache is None:
        _sector_indices_cache = fetch_sector_indices()
    return _sector_indices_cache

def reset_cache():
    """ìºì‹œ ì´ˆê¸°í™” (Refresh ì‹œ í˜¸ì¶œ)"""
    global _market_indices_cache, _sector_indices_cache
    _market_indices_cache = None
    _sector_indices_cache = None
    log("ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ", "SUCCESS")



# ìƒ‰ìƒ ì½”ë“œ (í„°ë¯¸ë„)
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def log(message, level="INFO"):
    # File logging
    if level == "ERROR":
        logging.error(f"[init_data] {message}")
    elif level == "WARNING":
        logging.warning(f"[init_data] {message}")
    elif level == "SUCCESS":
        logging.info(f"[init_data] âœ… {message}")
    elif level == "DEBUG":
        logging.debug(f"[init_data] {message}")
    else:
        logging.info(f"[init_data] {message}")

    # Console logging
    if level == "SUCCESS":
        print(f"{Colors.OKGREEN}âœ… {message}{Colors.ENDC}", flush=True)
    elif level == "ERROR":
        print(f"{Colors.FAIL}âŒ {message}{Colors.ENDC}", flush=True)
    elif level == "WARNING":
        print(f"{Colors.WARNING}âš ï¸  {message}{Colors.ENDC}", flush=True)
    elif level == "HEADER":
        print(f"\n{Colors.HEADER}{Colors.BOLD}{'='*60}{Colors.ENDC}", flush=True)
        print(f"{Colors.HEADER}{message}{Colors.ENDC}", flush=True)
        print(f"{Colors.HEADER}{'='*60}{Colors.ENDC}", flush=True)
    elif level == "DEBUG":
        pass  # Skip console output for debug
    else:
        # INFO logs also skipped in console if not important, but keep default behavior for now or strict?
        # User wants "only errors when there is an error", but some info might be useful.
        # Let's keep INFO printing but move verbose logs to DEBUG.
        print(f"ğŸ“Œ {message}", flush=True)


def ensure_directory(dir_path):
    """ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤."""
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
        log(f"ë””ë ‰í† ë¦¬ ìƒì„±ë¨: {dir_path}", "SUCCESS")
    else:
        log(f"ë””ë ‰í† ë¦¬ í™•ì¸ë¨: {dir_path}")

def create_korean_stocks_list():
    """í•œêµ­ ì£¼ì‹ ëª©ë¡ ìƒì„± - pykrxë¡œ ì‹œê°€ì´ì•¡ ìƒìœ„ ì¢…ëª© ì¡°íšŒ"""
    log("í•œêµ­ ì£¼ì‹ ëª©ë¡ ìƒì„± ì¤‘ (pykrx ì‹œê°€ì´ì•¡ ìƒìœ„)...")
    try:
        from pykrx import stock
        # from datetime import datetime
        
        today = datetime.now().strftime('%Y%m%d')
        
        all_data = []
        
        def get_market_cap_safe(target_date, market):
            try:
                df = stock.get_market_cap(target_date, market=market)
                if not df.empty:
                    return df
            except:
                pass
            return pd.DataFrame()

        # KOSPI
        kospi_cap = get_market_cap_safe(today, "KOSPI")
        if kospi_cap.empty: # ì˜¤ëŠ˜ ë°ì´í„° ì—†ìœ¼ë©´ í•˜ë£¨ ì „ ì‹œë„
             from datetime import timedelta
             prev_date = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
             log(f"ì˜¤ëŠ˜({today}) KOSPI ë°ì´í„° ì—†ìŒ. ì „ì¼({prev_date}) ë°ì´í„° ì‹œë„...", "WARNING")
             kospi_cap = get_market_cap_safe(prev_date, "KOSPI")

        if not kospi_cap.empty:
            # ì‹œê°€ì´ì•¡ ìˆœ ì •ë ¬ í›„ ìƒìœ„ 1000ê°œ (VCP ë°œêµ´ í™•ë¥  í™•ëŒ€ë¥¼ ìœ„í•´ ëŒ€í­ ì¦ê°€)
            kospi_cap = kospi_cap.sort_values('ì‹œê°€ì´ì•¡', ascending=False).head(1000)
            for ticker in kospi_cap.index:
                try:
                    name = stock.get_market_ticker_name(ticker)
                    all_data.append({'ticker': ticker, 'name': name, 'market': 'KOSPI', 'sector': ''})
                except: pass
            log(f"KOSPI ì‹œê°€ì´ì•¡ ìƒìœ„ {len(kospi_cap)} ì¢…ëª© ìˆ˜ì§‘", "SUCCESS")
        else:
            log("KOSPI ì‹œê°€ì´ì•¡ ì¡°íšŒ ì‹¤íŒ¨", "WARNING")

        # KOSDAQ
        kosdaq_cap = get_market_cap_safe(today, "KOSDAQ")
        if kosdaq_cap.empty: 
             from datetime import timedelta
             prev_date = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
             log(f"ì˜¤ëŠ˜({today}) KOSDAQ ë°ì´í„° ì—†ìŒ. ì „ì¼({prev_date}) ë°ì´í„° ì‹œë„...", "WARNING")
             kosdaq_cap = get_market_cap_safe(prev_date, "KOSDAQ")

        if not kosdaq_cap.empty:
            # ì‹œê°€ì´ì•¡ ìˆœ ì •ë ¬ í›„ ìƒìœ„ 1000ê°œ (ì½”ìŠ¤ë‹¥ í¬í•¨ ìš”ì²­ ë°˜ì˜)
            kosdaq_cap = kosdaq_cap.sort_values('ì‹œê°€ì´ì•¡', ascending=False).head(1000)
            for ticker in kosdaq_cap.index:
                try:
                    name = stock.get_market_ticker_name(ticker)
                    all_data.append({'ticker': ticker, 'name': name, 'market': 'KOSDAQ', 'sector': ''})
                except: pass
            log(f"KOSDAQ ì‹œê°€ì´ì•¡ ìƒìœ„ {len(kosdaq_cap)} ì¢…ëª© ìˆ˜ì§‘", "SUCCESS")
        else:
            log("KOSDAQ ì‹œê°€ì´ì•¡ ì¡°íšŒ ì‹¤íŒ¨", "WARNING")
        
        if all_data:
            df = pd.DataFrame(all_data)
            file_path = os.path.join(BASE_DIR, 'data', 'korean_stocks_list.csv')
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            log(f"ì¢…ëª© ëª©ë¡ ìƒì„± ì™„ë£Œ: {file_path} ({len(df)} ì¢…ëª©)", "SUCCESS")
            return True
        else:
            raise Exception("ì‹œê°€ì´ì•¡ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        
    except Exception as e:
        log(f"pykrx ì¢…ëª© ì¡°íšŒ ì‹¤íŒ¨: {e} - ê¸°ë³¸ ì¢…ëª© ì‚¬ìš©", "WARNING")
        # í´ë°±: ì‹œê°€ì´ì•¡ ìƒìœ„ ì£¼ìš” ì¢…ëª© (KOSPI + KOSDAQ)
        data = {
            'ticker': [
                # KOSPI ìƒìœ„ 20ê°œ
                '005930', '000660', '005380', '373220', '207940', '000270', '035420', '068270', '105560', '055550',
                '035720', '003550', '015760', '028260', '017670', '032830', '009150', '251270', '012330', '034730',
                # KOSDAQ ìƒìœ„ 10ê°œ + ì¸ê¸°/ê¸‰ë“±ì£¼ (ì•Œí…Œì˜¤ì  , ë¦¬ë…¸ê³µì—… ë“±)
                '247540', '086520', '196170', '263750', '145020', '403870', '328130', '091990', '336370', '058470',
                '293490', '214150', '035900', '041510', '036930', '039030', '035760', '022100', '042700', '064350'
            ],
            'name': [
                # KOSPI
                'ì‚¼ì„±ì „ì', 'SKí•˜ì´ë‹‰ìŠ¤', 'í˜„ëŒ€ì°¨', 'LGì—ë„ˆì§€ì†”ë£¨ì…˜', 'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤', 'ê¸°ì•„', 'NAVER', 'ì…€íŠ¸ë¦¬ì˜¨', 'KBê¸ˆìœµ', 'ì‹ í•œì§€ì£¼',
                'ì¹´ì¹´ì˜¤', 'LG', 'í•œêµ­ì „ë ¥', 'ì‚¼ì„±ë¬¼ì‚°', 'SKí…”ë ˆì½¤', 'ì‚¼ì„±ìƒëª…', 'ì‚¼ì„±ì „ê¸°', 'ë„·ë§ˆë¸”', 'í˜„ëŒ€ëª¨ë¹„ìŠ¤', 'SK',
                # KOSDAQ
                'ì—ì½”í”„ë¡œë¹„ì— ', 'ì—ì½”í”„ë¡œ', 'ì•Œí…Œì˜¤ì  ', 'í„ì–´ë¹„ìŠ¤', 'íœ´ì ¤', 'í”¼ì—ì´ì¹˜ì—ì´', 'ë£¨ë‹›', 'ì…€íŠ¸ë¦¬ì˜¨ì œì•½', 'ì†”ë¸Œë ˆì¸', 'ë¦¬ë…¸ê³µì—…',
                'ì¹´ì¹´ì˜¤ê²Œì„ì¦ˆ', 'í´ë˜ì‹œìŠ¤', 'JYP Ent.', 'ì—ìŠ¤ì— ', 'ì£¼ì„±ì—”ì§€ë‹ˆì–´ë§', 'ì´ì˜¤í…Œí¬ë‹‰ìŠ¤', 'CJ ENM', 'í¬ìŠ¤ì½”DX', 'í•œë¯¸ë°˜ë„ì²´', 'í˜„ëŒ€ë¡œí…œ'
            ],
            'market': [
                'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI',
                'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI', 'KOSPI',
                'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ',
                'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ', 'KOSDAQ'
            ],
            'sector': [
                'ë°˜ë„ì²´', 'ë°˜ë„ì²´', 'ìë™ì°¨', '2ì°¨ì „ì§€', 'í—¬ìŠ¤ì¼€ì–´', 'ìë™ì°¨', 'ì¸í„°ë„·', 'í—¬ìŠ¤ì¼€ì–´', 'ê¸ˆìœµ', 'ê¸ˆìœµ',
                'ì¸í„°ë„·', 'ì§€ì£¼', 'ì—ë„ˆì§€', 'ê±´ì„¤', 'í†µì‹ ', 'ê¸ˆìœµ', 'ì „ê¸°ì „ì', 'ê²Œì„', 'ìë™ì°¨ë¶€í’ˆ', 'ì§€ì£¼',
                '2ì°¨ì „ì§€', '2ì°¨ì „ì§€', 'í—¬ìŠ¤ì¼€ì–´', 'ê²Œì„', 'í—¬ìŠ¤ì¼€ì–´', 'ìë™ì°¨ë¶€í’ˆ', 'AI/ì˜ë£Œ', 'í—¬ìŠ¤ì¼€ì–´', 'ë°˜ë„ì²´ì†Œì¬', 'ë°˜ë„ì²´ì¥ë¹„',
                'ê²Œì„', 'ë¯¸ìš©ê¸°ê¸°', 'ì—”í„°', 'ì—”í„°', 'ë°˜ë„ì²´ì¥ë¹„', 'ë°˜ë„ì²´ì¥ë¹„', 'ë¯¸ë””ì–´', 'ITì„œë¹„ìŠ¤', 'ë°˜ë„ì²´ì¥ë¹„', 'ë°©ì‚°'
            ],
        }
        df = pd.DataFrame(data)
        file_path = os.path.join(BASE_DIR, 'data', 'korean_stocks_list.csv')
        df.to_csv(file_path, index=False, encoding='utf-8-sig')
        log(f"ê¸°ë³¸ ì¢…ëª© ëª©ë¡ ìƒì„± ì™„ë£Œ: {file_path} ({len(df)} ì¢…ëª© - KOSPI 15ê°œ + KOSDAQ 10ê°œ)", "SUCCESS")
        return True



def fetch_prices_yfinance(start_date, end_date, existing_df, file_path):
    """yfinanceë¥¼ ì´ìš©í•œ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ í´ë°±"""
    try:
        if start_date.date() > end_date.date():
            log(f"yfinance ìˆ˜ì§‘: ì‹œì‘ì¼({start_date.strftime('%Y-%m-%d')})ì´ ì¢…ë£Œì¼({end_date.strftime('%Y-%m-%d')})ë³´ë‹¤ ë¯¸ë˜ì…ë‹ˆë‹¤. (ìµœì‹  ìƒíƒœ)", "SUCCESS")
            return True

        import yfinance as yf
        log("yfinance ë°±ì—… ìˆ˜ì§‘ ëª¨ë“œ ê°€ë™...", "DEBUG")
        
        # ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
        stocks_file = os.path.join(BASE_DIR, 'data', 'korean_stocks_list.csv')
        if not os.path.exists(stocks_file):
            log("ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ì–´ yfinance ìˆ˜ì§‘ ë¶ˆê°€", "ERROR")
            return False
            
        stocks_df = pd.read_csv(stocks_file, dtype={'ticker': str})
        tickers = stocks_df['ticker'].tolist()
        
        new_data_list = []
        
        total = len(tickers)
        for idx, ticker in enumerate(tickers):
            try:
                # ë§ˆì¼“ í™•ì¸
                market_info = stocks_df[stocks_df['ticker'] == ticker]['market'].values
                suffix = ".KS" if len(market_info) > 0 and market_info[0] == 'KOSPI' else ".KQ"
                yf_ticker = f"{ticker}{suffix}"
                
                # yfinance ì—ëŸ¬ ë¡œê·¸ ì–µì œ
                import logging as _logging
                yf_logger = _logging.getLogger('yfinance')
                original_level = yf_logger.level
                yf_logger.setLevel(_logging.CRITICAL)
                
                try:
                    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì§„í–‰ë¥  í‘œì‹œ ì—†ì´, ìŠ¤ë ˆë“œ ë¹„í™œì„±í™”)
                    df = yf.download(yf_ticker, start=start_date.strftime('%Y-%m-%d'), end=(end_date + timedelta(days=1)).strftime('%Y-%m-%d'), progress=False, threads=False)
                finally:
                    yf_logger.setLevel(original_level)
                
                if not df.empty:
                    # MultiIndex ì»¬ëŸ¼ ì²˜ë¦¬
                    if isinstance(df.columns, pd.MultiIndex):
                         # yfinance 0.2.x+ returns MultiIndex if configured or sometimes by default
                         # It usually is (Price, Ticker) or just Price.
                         # Dropping level if it exists
                        try:
                            df.columns = df.columns.droplevel(1)
                        except:
                            pass
                        
                    df = df.reset_index()
                    # ì»¬ëŸ¼ ì´ë¦„ì´ Date, Open, High ...
                    
                    # Rename columns to standard lowercase
                    df = df.rename(columns={
                        'Date': 'date', 'Open': 'open', 'High': 'high', 
                        'Low': 'low', 'Close': 'close', 'Volume': 'volume'
                    })
                    
                    # Ensure columns exist
                    required_cols = ['date', 'open', 'high', 'low', 'close', 'volume']
                    if not all(col in df.columns for col in required_cols):
                         continue

                    df['date'] = df['date'].dt.strftime('%Y-%m-%d')
                    df['ticker'] = ticker
                    
                    # Type conversion
                    df['open'] = df['open'].astype(int)
                    df['high'] = df['high'].astype(int)
                    df['low'] = df['low'].astype(int)
                    df['close'] = df['close'].astype(int)
                    df['volume'] = df['volume'].astype(int)
                    
                    subset = df[['date', 'ticker', 'open', 'high', 'low', 'close', 'volume']]
                    new_data_list.append(subset)
                    
            except Exception as e:
                continue
                
            if idx % 50 == 0:
                print(f"  -> yfinance ì§„í–‰: {idx}/{total}")

        if new_data_list:
            new_df = pd.concat(new_data_list)
            
            if not existing_df.empty:
                if 'date' in existing_df.columns and not pd.api.types.is_string_dtype(existing_df['date']):
                     existing_df['date'] = existing_df['date'].dt.strftime('%Y-%m-%d')
                     
                final_df = pd.concat([existing_df, new_df])
                final_df = final_df.drop_duplicates(subset=['ticker', 'date'], keep='last')
            else:
                final_df = new_df
                
            final_df.to_csv(file_path, index=False, encoding='utf-8-sig')
            log(f"yfinance ë°±ì—… ìˆ˜ì§‘ ì™„ë£Œ ({len(final_df)}í–‰)", "SUCCESS")
            return True
        else:
            log("yfinance ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ", "WARNING")
            return True
            
    except Exception as e:
        log(f"yfinance í´ë°± ì‹¤íŒ¨: {e}", "ERROR")
        return False


def create_daily_prices(target_date=None, force=False, lookback_days=5):
    """
    ì¼ë³„ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ - pykrx ë‚ ì§œë³„ ì¼ê´„ ì¡°íšŒ (ì†ë„ ìµœì í™”)
    Args:
        target_date: ê¸°ì¤€ ë‚ ì§œ (ê¸°ë³¸: ì˜¤ëŠ˜)
        force: ê°•ì œ ì—…ë°ì´íŠ¸ ì—¬ë¶€
        lookback_days: ê°•ì œ ì—…ë°ì´íŠ¸ ì‹œ ì¬ìˆ˜ì§‘í•  ê¸°ê°„ (ê¸°ë³¸: 5ì¼)
    """
    log("ì¼ë³„ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ ì¤‘ (Date-based Fast Mode)...", "DEBUG")
    try:
        from pykrx import stock
        import time
        from datetime import datetime, timedelta

        # ë‚ ì§œ ì„¤ì •
        if target_date:
            if isinstance(target_date, str):
                end_date_obj = datetime.strptime(target_date, '%Y-%m-%d')
            else:
                end_date_obj = target_date
        else:
            end_date_obj = datetime.now()

        # ë§ˆì§€ë§‰ ê°œì¥ì¼ í™•ì¸
        end_date_str, end_date_obj = get_last_trading_date(reference_date=end_date_obj)

        # [Safety] ë¯¸ë˜ ë‚ ì§œ ìš”ì²­ ë°©ì§€
        if end_date_obj > datetime.now():
            log(f"ìš”ì²­ ë‚ ì§œ({end_date_str})ê°€ ë¯¸ë˜ì´ë¯€ë¡œ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì¡°ì •í•©ë‹ˆë‹¤.", "WARNING")
            end_date_obj = datetime.now()
            end_date_str = end_date_obj.strftime('%Y%m%d')
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ë° ì‹œì‘ì¼ ê²°ì •
        file_path = os.path.join(BASE_DIR, 'data', 'daily_prices.csv')
        existing_df = pd.DataFrame()
        start_date_obj = end_date_obj - timedelta(days=90) # ê¸°ë³¸ 90ì¼

        if os.path.exists(file_path):
            try:
                existing_df = pd.read_csv(file_path, dtype={'ticker': str})
                if not existing_df.empty and 'date' in existing_df.columns:
                    max_date_str = existing_df['date'].max()
                    
                    # (ì¤‘ìš”) ì¢…ëª© ìˆ˜ ì²´í¬ - ìƒˆë¡œ ì¶”ê°€ëœ ì¢…ëª©ì´ ìˆì„ ìˆ˜ ìˆìŒ
                    # í˜„ì¬ ë“±ë¡ëœ ì¢…ëª© ìˆ˜(600ê°œ)ì™€ ë§ˆì§€ë§‰ ë‚ ì§œì˜ ë°ì´í„° ê°œìˆ˜ ë¹„ê²Œ
                    stocks_file = os.path.join(BASE_DIR, 'data', 'korean_stocks_list.csv')
                    total_stocks_count = 600
                    if os.path.exists(stocks_file):
                        try:
                            stocks_df = pd.read_csv(stocks_file)
                            total_stocks_count = len(stocks_df)
                        except:
                            pass
                    
                    last_date_count = len(existing_df[existing_df['date'] == max_date_str])
                    
                    if start_date_obj.date() > end_date_obj.date():
                        # Force check
                        if not force and last_date_count >= total_stocks_count * 0.9:
                            log("ì´ë¯¸ ìµœì‹  ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©° ì¶©ë¶„í•©ë‹ˆë‹¤.", "SUCCESS")
                            return True
                        elif force:
                             log(f"ìµœì‹  ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ë§Œ ê°•ì œ ì—…ë°ì´íŠ¸(force=True)ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. (ìµœê·¼ {lookback_days}ì¼)", "DEBUG")
                             start_date_obj = end_date_obj - timedelta(days=lookback_days)
                        else:
                            log(f"ë°ì´í„° ë‚ ì§œëŠ” ìµœì‹ ì´ë‚˜ ì¢…ëª© ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤({last_date_count}/{total_stocks_count}). ì¬ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.", "WARNING")
                            start_date_obj = end_date_obj - timedelta(days=lookback_days) # ë¶€ì¡±í•œ ê²½ìš°ì—ë„ lookback_days ì‚¬ìš©
                    else:
                        max_date_dt = datetime.strptime(max_date_str, '%Y-%m-%d')
                        # ë§ˆì§€ë§‰ ì €ì¥ì¼ ë‹¤ìŒë‚ ë¶€í„° ìˆ˜ì§‘
                        start_date_obj = max_date_dt + timedelta(days=1)
                        if force:
                             log(f"ê°•ì œ ì—…ë°ì´íŠ¸: ê¸°ì¡´ ë°ì´í„° ë¬´ì‹œí•˜ê³  ìµœê·¼ {lookback_days}ì¼ ì¬ìˆ˜ì§‘", "DEBUG")
                             start_date_obj = end_date_obj - timedelta(days=lookback_days)
                        else:
                             log(f"ê¸°ì¡´ ë°ì´í„° í™•ì¸: {max_date_str}ê¹Œì§€ ì¡´ì¬. ì´í›„ë¶€í„° ìˆ˜ì§‘.", "INFO")
                else:
                    log("ê¸°ì¡´ ë°ì´í„° ë¹„ì–´ìˆìŒ.", "INFO")
            except Exception as e:
                log(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}", "WARNING")
             
        req_start_date_str = start_date_obj.strftime('%Y%m%d')
        log(f"ìˆ˜ì§‘ êµ¬ê°„: {req_start_date_str} ~ {end_date_str}", "DEBUG")

        # ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        date_range = pd.date_range(start=start_date_obj, end=end_date_obj)
        total_days = len(date_range)
        
        new_data_list = []
        processed_days = 0
        
        for dt in date_range:
            if shared_state.STOP_REQUESTED:
                log("â›”ï¸ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì¤‘ë‹¨", "WARNING")
                break
                
            cur_date_str = dt.strftime('%Y%m%d')
            cur_date_fmt = dt.strftime('%Y-%m-%d')
            
            # ì£¼ë§ ì²´í¬ (í† /ì¼) - pykrxê°€ ì•Œì•„ì„œ ë¹ˆê°’ ì¤„ ìˆ˜ ìˆìœ¼ë‚˜ ë¯¸ë¦¬ ê±´ë„ˆë›°ë©´ ë¹ ë¦„
            if dt.weekday() >= 5: 
                processed_days += 1
                continue

            # [Optimization] ì´ë¯¸ ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” ê±´ë„ˆë›°ê¸° (ê³¼ê±° ë°ì´í„°ì¸ ê²½ìš°ë§Œ)
            # ì˜¤ëŠ˜ ë‚ ì§œëŠ” ì¥ì¤‘ ë³€ë™ ê°€ëŠ¥í•˜ë¯€ë¡œ í•­ìƒ ìˆ˜ì§‘
            if not existing_df.empty and 'date' in existing_df.columns:
                if cur_date_fmt in existing_df['date'].values:
                     # ì˜¤ëŠ˜ì´ ì•„ë‹ˆë©´ Skip
                    if dt.date() < datetime.now().date():
                         log(f"  -> {cur_date_fmt} ë°ì´í„° ì¡´ì¬ (Skip)", "DEBUG")
                         processed_days += 1
                         continue
                
            try:
                # í•´ë‹¹ ë‚ ì§œì˜ ì „ ì¢…ëª© ì‹œì„¸ ì¡°íšŒ (1íšŒ ìš”ì²­)
                df = stock.get_market_ohlcv(cur_date_str, market="ALL")
                
                if df is None or df.empty:
                    # íœ´ì¥ì¼ ê°€ëŠ¥ì„±
                    processed_days += 1
                    continue
                    
                # DataFrame ì •ë¦¬
                # indexëŠ” ticker, columns: ì‹œê°€, ê³ ê°€, ì €ê°€, ì¢…ê°€, ê±°ë˜ëŸ‰, ê±°ë˜ëŒ€ê¸ˆ, ë“±ë½ë¥ 
                df = df.reset_index() # tickerê°€ ì»¬ëŸ¼ìœ¼ë¡œ ë‚˜ì˜´ ('í‹°ì»¤')
                
                # ì»¬ëŸ¼ ë§¤í•‘
                # pykrx ë²„ì „ì— ë”°ë¼ ì»¬ëŸ¼ëª…ì´ 'í‹°ì»¤'ì¼ìˆ˜ë„, indexì¼ìˆ˜ë„ ìˆìŒ. 
                # get_market_ohlcv("YYYYMMDD") returns index=í‹°ì»¤.
                if 'í‹°ì»¤' in df.columns:
                    df = df.rename(columns={'í‹°ì»¤': 'ticker'})
                else: 
                    # reset_index() í–ˆì„ ë•Œ ê¸°ì¡´ index ì´ë¦„ì´ 'í‹°ì»¤'ì˜€ë‹¤ë©´ ê·¸ê²Œ ì»¬ëŸ¼ëª…ì´ ë¨
                    # ë§Œì•½ ì´ë¦„ì´ ì—†ì—ˆë‹¤ë©´ 'index'
                    if 'index' in df.columns:
                        df = df.rename(columns={'index': 'ticker'})
                
                # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸ (í•œê¸€/ì˜ë¬¸ ëŒ€ì‘)
                rename_map = {
                    'ì‹œê°€': 'open', 'ê³ ê°€': 'high', 'ì €ê°€': 'low', 
                    'ì¢…ê°€': 'close', 'ê±°ë˜ëŸ‰': 'volume', 'ê±°ë˜ëŒ€ê¸ˆ': 'trading_value',
                    'Open': 'open', 'High': 'high', 'Low': 'low', 
                    'Close': 'close', 'Volume': 'volume', 'Amount': 'trading_value'
                }
                
                # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ rename
                available_map = {k: v for k, v in rename_map.items() if k in df.columns}
                df = df.rename(columns=available_map)
                
                df['ticker'] = df['ticker'].astype(str).str.zfill(6)
                df['date'] = cur_date_fmt
                
                # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
                cols = ['date', 'ticker', 'open', 'high', 'low', 'close', 'volume', 'trading_value']
                # ê±°ë˜ëŒ€ê¸ˆ ì—†ì„ ê²½ìš° ì²˜ë¦¬
                if 'trading_value' not in df.columns:
                    df['trading_value'] = df['volume'] * df['close']
                    
                df_final = df[cols].copy()
                
                # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ë©”ëª¨ë¦¬ ê³ ë ¤: ë°”ë¡œë°”ë¡œ ëª¨ìŒ)
                # DataFrame to dict list is slow? append DF to list then concat.
                new_data_list.append(df_final)
                
                processed_days += 1
                progress = (processed_days / total_days) * 100
                log(f"[Daily Prices] {cur_date_fmt} ìˆ˜ì§‘ ì™„ë£Œ ({len(df_final)}ì¢…ëª©) - {progress:.1f}%", "DEBUG")
                
                # Rate Limit ë°©ì§€
                time.sleep(random.uniform(0.05, 0.1))
                
            except Exception as e:
                log(f"ë‚ ì§œë³„ ìˆ˜ì§‘ ì‹¤íŒ¨ ({cur_date_str}): {e}", "WARNING")
                processed_days += 1
                
        # ë³‘í•© ë° ì €ì¥
        if new_data_list:
            log("ë°ì´í„° ë³‘í•© ì¤‘...", "DEBUG")
            new_chunk_df = pd.concat(new_data_list, ignore_index=True)
            
            if not existing_df.empty:
                final_df = pd.concat([existing_df, new_chunk_df])
                final_df = final_df.drop_duplicates(subset=['date', 'ticker'], keep='last')
            else:
                final_df = new_chunk_df
                
            final_df = final_df.sort_values(['ticker', 'date'])
            final_df.to_csv(file_path, index=False, encoding='utf-8-sig')
            log(f"ì¼ë³„ ê°€ê²© ì €ì¥ ì™„ë£Œ: ì´ {len(final_df)}í–‰ (ì‹ ê·œ {len(new_chunk_df)}í–‰)", "DEBUG")
        else:
             if start_date_obj.date() > end_date_obj.date():
                 log("pykrx ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ (ì´ë¯¸ ìµœì‹ ).", "SUCCESS")
                 return True

             log("pykrx ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ. yfinance í´ë°± ì‹œë„...", "DEBUG")
             return fetch_prices_yfinance(start_date_obj, end_date_obj, existing_df, file_path)
                 
        return True

    except Exception as e:
        log(f"pykrx ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e} -> yfinance í´ë°± ì‹œë„", "WARNING")
        return fetch_prices_yfinance(start_date_obj, end_date_obj, existing_df, file_path)


def create_institutional_trend(target_date=None, force=False, lookback_days=7):
    """
    ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ - pykrx ê¸°ê´€/ì™¸êµ­ì¸ ìˆœë§¤ë§¤ (Optimized)
    Args:
        target_date: ê¸°ì¤€ ë‚ ì§œ
        force: ê°•ì œ ì—…ë°ì´íŠ¸ ì—¬ë¶€
        lookback_days: ê°•ì œ ì—…ë°ì´íŠ¸ ì‹œ ì¬ìˆ˜ì§‘í•  ê¸°ê°„ (ê¸°ë³¸: 7ì¼)
    """
    log("ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ (pykrx ì‹¤ì œ ë°ì´í„°)...", "DEBUG")
    try:
        from pykrx import stock
        
        # ì¢…ëª© ëª©ë¡ ë¡œë“œ
        stocks_file = os.path.join(BASE_DIR, 'data', 'korean_stocks_list.csv')
        tickers_set = set() # ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•´ set ì‚¬ìš©
        if os.path.exists(stocks_file):
            stocks_df = pd.read_csv(stocks_file)
            tickers_set = set(stocks_df['ticker'].astype(str).str.zfill(6).tolist())
            if '069500' not in tickers_set:
                tickers_set.add('069500')
        else:
            tickers_set = {'069500', '005930', '000660', '000270', '051910', '006400'}
        
        if target_date:
            if isinstance(target_date, str):
                target_date_obj = datetime.strptime(target_date, '%Y-%m-%d')
            else:
                target_date_obj = target_date
        else:
            target_date_obj = datetime.now()

        # ë§ˆì§€ë§‰ ê°œì¥ì¼ í™•ì¸ (ì£¼ë§/íœ´ì¼ ìë™ ì²˜ë¦¬)
        end_date, end_date_obj = get_last_trading_date(reference_date=target_date_obj)
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        file_path = os.path.join(BASE_DIR, 'data', 'all_institutional_trend_data.csv')
        existing_df = pd.DataFrame()
        start_date_obj = end_date_obj - timedelta(days=30) # ê¸°ë³¸ 30ì¼ ì „
        
        if os.path.exists(file_path):
            try:
                existing_df = pd.read_csv(file_path, dtype={'ticker': str, 'date': str})
                if not existing_df.empty and 'date' in existing_df.columns:
                    # ê°€ì¥ ìµœê·¼ ë°ì´í„° ë‚ ì§œ í™•ì¸
                    max_date_str = existing_df['date'].max()
                    
                    # (ì¤‘ìš”) ë‹¨ìˆœ ë‚ ì§œ ì²´í¬ë§Œ í•˜ì§€ ì•Šê³ , ì¢…ëª© ìˆ˜ê°€ ë¶€ì¡±í•œì§€ í™•ì¸
                    last_date_tickers = len(existing_df[existing_df['date'] == max_date_str])
                    
                    # ì‹ ê·œ ì¶”ê°€ëœ ì¢…ëª©ì´ ìˆëŠ”ì§€ í™•ì¸ (Backfill í•„ìš” ì—¬ë¶€)
                    existing_tickers = set(existing_df['ticker'].unique())
                    missing_tickers = tickers_set - existing_tickers
                    
                    if start_date_obj.date() > end_date_obj.date() and not missing_tickers:
                        if not force and last_date_tickers >= len(tickers_set) * 0.9: # 90% ì´ìƒ ì°¨ìˆìœ¼ë©´ ìµœì‹ ìœ¼ë¡œ ê°„ì£¼
                            log("ìˆ˜ê¸‰ ë°ì´í„°: ì´ë¯¸ ìµœì‹  ìƒíƒœì´ë©° ë°ì´í„°ê°€ ì¶©ë¶„í•©ë‹ˆë‹¤.", "SUCCESS")
                            return True
                        elif force:
                             log(f"ìˆ˜ê¸‰ ë°ì´í„°: ê°•ì œ ì—…ë°ì´íŠ¸ ì§„í–‰ (ìµœê·¼ {lookback_days}ì¼ ì¬ìˆ˜ì§‘)", "WARNING")
                             start_date_obj = end_date_obj - timedelta(days=lookback_days)

                    if missing_tickers and not force: # Forceì¼ë•ŒëŠ” ìœ„ì—ì„œ ì²˜ë¦¬ë¨
                        log(f"ìˆ˜ê¸‰ ë°ì´í„°: ì‹ ê·œ ì¢…ëª© {len(missing_tickers)}ê°œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. (ìµœì í™”: ìµœê·¼ {lookback_days}ì¼ë§Œ ì¬ìˆ˜ì§‘)", "WARNING")
                        # ì‹ ê·œ ì¢…ëª©ì´ ìˆì–´ë„ ê³¼ë„í•œ ì¬ìˆ˜ì§‘ ë°©ì§€ (30ì¼ -> lookback_days)
                        start_date_obj = end_date_obj - timedelta(days=lookback_days)
                    elif last_date_tickers < len(tickers_set) * 0.8:
                        log(f"ìˆ˜ê¸‰ ë°ì´í„°: ìµœì‹  ë‚ ì§œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤({last_date_tickers}/{len(tickers_set)}). ì¬ìˆ˜ì§‘í•©ë‹ˆë‹¤.", "WARNING")
                        start_date_obj = end_date_obj - timedelta(days=lookback_days)     
                    elif not force:
                        # ì •ìƒì ì¸ ê²½ìš° max_date ë‹¤ìŒë‚ ë¶€í„° (Forceê°€ ì•„ë‹ ë•Œë§Œ)
                        try:
                            max_date_dt = datetime.strptime(max_date_str, '%Y-%m-%d')
                            start_date_obj = max_date_dt + timedelta(days=1)
                        except: pass
            except Exception as e:
                log(f"ê¸°ì¡´ ìˆ˜ê¸‰ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ (ìƒˆë¡œ ì‹œì‘): {e}", "WARNING")

        start_date = start_date_obj.strftime('%Y%m%d')
        
        # ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ë¯¸ë˜ì¸ ê²½ìš° (ê·¸ë¦¬ê³  ë¯¸ì‹± í‹°ì»¤ ì—†ëŠ” ê²½ìš°) ì²˜ë¦¬
        if start_date > end_date:
             log("ìˆ˜ê¸‰ ë°ì´í„°: ì´ë¯¸ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.", "SUCCESS")
             return True

        log(f"ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ êµ¬ê°„(ê°œì„ ë¨): {start_date} ~ {end_date} (Date-based Bulk Fetch)", "DEBUG")
        
        # ë‚ ì§œ ë£¨í”„ ì‹œì‘
        date_range = pd.date_range(start=start_date_obj, end=end_date_obj)
        total_days = len(date_range)
        processed_days = 0
        
        new_data_list = []
        
        for dt in date_range:
            if shared_state.STOP_REQUESTED:
                log("â›”ï¸ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ë‹¨", "WARNING")
                break
                
            cur_date_str = dt.strftime('%Y%m%d')
            cur_date_fmt = dt.strftime('%Y-%m-%d')
            
            # ì£¼ë§ ì²´í¬
            if dt.weekday() >= 5:
                processed_days += 1
                continue
            
            # [Optimization] ì´ë¯¸ ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” ê±´ë„ˆë›°ê¸° (ê³¼ê±° ë°ì´í„°ì¸ ê²½ìš°ë§Œ)
            if not existing_df.empty and 'date' in existing_df.columns:
                if cur_date_fmt in existing_df['date'].values:
                    # ì˜¤ëŠ˜ì´ ì•„ë‹ˆë©´ Skip
                    if dt.date() < datetime.now().date():
                         log(f"  -> {cur_date_fmt} ìˆ˜ê¸‰ ë°ì´í„° ì¡´ì¬ (Skip)", "DEBUG")
                         processed_days += 1
                         continue
            
            try:
                # 1. ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ (ì „ ì¢…ëª©)
                df_foreign = stock.get_market_net_purchases_of_equities_by_ticker(cur_date_str, cur_date_str, "ALL", "ì™¸êµ­ì¸")
                time.sleep(0.2) # Rate limit
                
                # 2. ê¸°ê´€ ìˆœë§¤ìˆ˜ (ì „ ì¢…ëª©)
                df_inst = stock.get_market_net_purchases_of_equities_by_ticker(cur_date_str, cur_date_str, "ALL", "ê¸°ê´€í•©ê³„")
                time.sleep(0.2)
                
                # ë°ì´í„° ë³‘í•©
                # ì¸ë±ìŠ¤: í‹°ì»¤
                combined_rows = []
                
                # ì™¸êµ­ì¸ ë°ì´í„° ê¸°ì¤€ ë£¨í”„ (ë˜ëŠ” set of tickers)
                # target_tickersì— ìˆëŠ” ê²ƒë§Œ í•„í„°ë§
                
                # ì¸ë±ìŠ¤(í‹°ì»¤)ë¥¼ setìœ¼ë¡œ í™•ë³´
                available_tickers = set(df_foreign.index) | set(df_inst.index)
                target_intersect = available_tickers & tickers_set
                
                for ticker in target_intersect:
                    f_val = 0
                    i_val = 0
                    
                    if ticker in df_foreign.index:
                        # ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ ì»¬ëŸ¼ í™•ì¸
                        if 'ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ' in df_foreign.columns:
                            f_val = df_foreign.loc[ticker, 'ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ']
                    
                    if ticker in df_inst.index:
                        if 'ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ' in df_inst.columns:
                            i_val = df_inst.loc[ticker, 'ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ']
                            
                    combined_rows.append({
                        'date': cur_date_fmt,
                        'ticker': ticker,
                        'foreign_buy': int(f_val),
                        'inst_buy': int(i_val)
                    })
                
                if combined_rows:
                    new_data_list.extend(combined_rows)
                    log(f"[Supply Trend] {cur_date_fmt} ìˆ˜ì§‘ ì™„ë£Œ ({len(combined_rows)}ì¢…ëª©)", "DEBUG")
                else:
                    log(f"[Supply Trend] {cur_date_fmt} ë°ì´í„° ì—†ìŒ (íœ´ì¥ì¼?)", "DEBUG")
                
            except Exception as e:
                log(f"ìˆ˜ê¸‰ ë°ì´í„° ë‚ ì§œë³„ ìˆ˜ì§‘ ì‹¤íŒ¨ ({cur_date_str}): {e}", "WARNING")
            
            processed_days += 1
        
        # ê²°ê³¼ ì €ì¥
        if new_data_list:
            log("ìˆ˜ê¸‰ ë°ì´í„° ë³‘í•© ë° ì €ì¥ ì¤‘...", "DEBUG")
            new_df = pd.DataFrame(new_data_list)
            
            if not existing_df.empty:
                final_df = pd.concat([existing_df, new_df])
                final_df = final_df.drop_duplicates(subset=['date', 'ticker'], keep='last')
            else:
                final_df = new_df
            
            # ì •ë ¬
            final_df = final_df.sort_values(['ticker', 'date'])
            final_df.to_csv(file_path, index=False, encoding='utf-8-sig')
            log(f"ìˆ˜ê¸‰ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì´ {len(final_df)}í–‰ (ì‹ ê·œ {len(new_data_list)}í–‰)", "DEBUG")
            return True
        else:
            log("ìˆ˜ê¸‰ ë°ì´í„°: ì‹ ê·œ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", "SUCCESS")
            return True

    except Exception as e:
        log(f"ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", "ERROR")
        import traceback
        traceback.print_exc()
        return False


def create_signals_log(target_date=None, run_ai=True):
    """VCP ì‹œê·¸ë„ ë¡œê·¸ ìƒì„± - Using SmartMoneyScreener (engine.screener)"""
    log("VCP ì‹œê·¸ë„ ë¶„ì„ ì¤‘ (SmartMoneyScreener)...")
    try:
        from engine.screener import SmartMoneyScreener
        
        # ìŠ¤í¬ë¦¬ë„ˆ ì‹¤í–‰ (KOSPI+KOSDAQ ì „ì²´ ë¶„ì„)
        screener = SmartMoneyScreener(target_date=target_date)
        df_result = screener.run_screening(max_stocks=600)
        
        signals = []
        if not df_result.empty:
            for _, row in df_result.iterrows():
                signals.append({
                    'ticker': row['ticker'],
                    'name': row['name'],
                    'signal_date': target_date if target_date else datetime.now().strftime('%Y-%m-%d'),
                    'market': row['market'],
                    'status': 'OPEN',
                    'score': round(row['score'], 1),
                    'contraction_ratio': row.get('contraction_ratio', 0),
                    'entry_price': int(row['entry_price']),
                    'foreign_5d': int(row['foreign_net_5d']),
                    'inst_5d': int(row['inst_net_5d']),
                    'vcp_score': 0, # Screener doesn't expose sub-score directly, optional
                    'current_price': int(row.get('entry_price', 0)) # Approximation or need fetch
                })
        
        log(f"ì´ {len(signals)}ê°œ ì‹œê·¸ë„ ê°ì§€")
        
        # ì ìˆ˜ ë†’ì€ ìˆœ ì •ë ¬ (Top 20 ì œí•œ)
        signals = sorted(signals, key=lambda x: x['score'], reverse=True)[:20]
        
        # AI ë¶„ì„ ì‹¤í–‰ (ì˜µì…˜)
        if run_ai and signals:
            try:
                log(f"[AI Analysis] ê°ì§€ëœ {len(signals)}ê°œ ì‹œê·¸ë„ì— ëŒ€í•´ AI ì •ë°€ ë¶„ì„ ìˆ˜í–‰...", "INFO")
                from engine.vcp_ai_analyzer import get_vcp_analyzer
                analyzer = get_vcp_analyzer()
                
                # ë¹„ë™ê¸° ì‹¤í–‰ì„ ìœ„í•œ ë£¨í”„ ê°€ì ¸ì˜¤ê¸°
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                # ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰
                ai_results = loop.run_until_complete(analyzer.analyze_batch(signals))
                
                # ê²°ê³¼ ì €ì¥
                if ai_results:
                    date_str = signals[0]['signal_date'].replace('-', '')
                    
                    # 1. ai_analysis_results.jsonì— ì €ì¥ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                    ai_filename = f'ai_analysis_results_{date_str}.json'
                    ai_filepath = os.path.join(BASE_DIR, 'data', ai_filename)
                    
                    save_data = {
                        'generated_at': datetime.now().isoformat(),
                        'signal_date': signals[0]['signal_date'],
                        'signals': list(ai_results.values())
                    }
                    
                    with open(ai_filepath, 'w', encoding='utf-8') as f:
                        json.dump(save_data, f, ensure_ascii=False, indent=2)
                        
                    latest_path = os.path.join(BASE_DIR, 'data', 'ai_analysis_results.json')
                    with open(latest_path, 'w', encoding='utf-8') as f:
                         json.dump(save_data, f, ensure_ascii=False, indent=2)
                    
                    # 2. kr_ai_analysis.jsonì—ë„ ì €ì¥ (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ í˜•ì‹)
                    # VCP ì‹œê·¸ë„ ì •ë³´ + AI ë¶„ì„ ê²°ê³¼ + ë‰´ìŠ¤ í†µí•©
                    kr_ai_signals = []
                    
                    # ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
                    news_collector = None
                    try:
                        from engine.collectors import EnhancedNewsCollector
                        from engine.config import app_config
                        news_collector = EnhancedNewsCollector(app_config)
                        log("[AI Analysis] ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ", "INFO")
                    except Exception as news_init_err:
                        log(f"[AI Analysis] ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {news_init_err}", "WARNING")
                    
                    for signal in signals:
                        ticker = signal.get('ticker', '')
                        name = signal.get('name', '')
                        ai_data = ai_results.get(ticker, {})
                        
                        # ë‰´ìŠ¤ ìˆ˜ì§‘ (ìµœëŒ€ 5ê°œ)
                        news_items = []
                        if news_collector:
                            try:
                                news_list = asyncio.get_event_loop().run_until_complete(
                                    news_collector.get_stock_news(ticker, limit=5, name=name)
                                )
                                for news in news_list:
                                    news_items.append({
                                        'title': getattr(news, 'title', str(news)),
                                        'url': getattr(news, 'url', ''),
                                        'source': getattr(news, 'source', 'Naver'),
                                        'date': getattr(news, 'date', '')
                                    })
                            except Exception as news_err:
                                log(f"[AI Analysis] {name} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {news_err}", "WARNING")
                        
                        # í˜„ì¬ê°€ ë° ìˆ˜ìµë¥  ëª…ì‹œì  ê³„ì‚°
                        curr_p = int(signal.get('current_price', signal.get('entry_price', 0)))
                        entry_p = int(signal.get('entry_price', curr_p))
                        ret_p = round(((curr_p - entry_p) / entry_p * 100), 2) if entry_p > 0 else 0

                        kr_signal = {
                            'ticker': ticker,
                            'name': name,
                            'market': signal.get('market', 'KOSPI'),
                            'score': signal.get('score', 0),
                            'contraction_ratio': signal.get('contraction_ratio', 0),
                            'foreign_5d': signal.get('foreign_5d', 0),
                            'inst_5d': signal.get('inst_5d', 0),
                            'entry_price': entry_p,
                            'current_price': curr_p,
                            'return_pct': ret_p,
                            'vcp_score': signal.get('vcp_score', 0),
                            # AI ë¶„ì„ ê²°ê³¼ í†µí•©
                            'gemini_recommendation': ai_data.get('gemini_recommendation'),
                            'gpt_recommendation': ai_data.get('gpt_recommendation'),
                            'perplexity_recommendation': ai_data.get('perplexity_recommendation'),
                            # ë‰´ìŠ¤ ë°ì´í„° ì¶”ê°€
                            'news': news_items,
                        }
                        kr_ai_signals.append(kr_signal)
                    
                    # ì‹œì¥ ì§€ìˆ˜ ë°ì´í„° ìˆ˜ì§‘
                    market_indices = {}
                    try:
                        from pykrx import stock
                        today_str = datetime.now().strftime('%Y%m%d')
                        kospi = stock.get_index_ohlcv(today_str, today_str, "1001")  # KOSPI
                        kosdaq = stock.get_index_ohlcv(today_str, today_str, "2001")  # KOSDAQ
                        
                        if not kospi.empty:
                            market_indices['kospi'] = {
                                'value': float(kospi['ì¢…ê°€'].iloc[-1]) if len(kospi) > 0 else 0,
                                'change_pct': float(kospi['ë“±ë½ë¥ '].iloc[-1]) if len(kospi) > 0 and 'ë“±ë½ë¥ ' in kospi.columns else 0
                            }
                        if not kosdaq.empty:
                            market_indices['kosdaq'] = {
                                'value': float(kosdaq['ì¢…ê°€'].iloc[-1]) if len(kosdaq) > 0 else 0,
                                'change_pct': float(kosdaq['ë“±ë½ë¥ '].iloc[-1]) if len(kosdaq) > 0 and 'ë“±ë½ë¥ ' in kosdaq.columns else 0
                            }
                    except Exception as idx_e:
                        log(f"[AI Analysis] ì‹œì¥ ì§€ìˆ˜ ìˆ˜ì§‘ ì‹¤íŒ¨ (ë¬´ì‹œ): {idx_e}", "WARNING")
                    
                    kr_ai_data = {
                        'market_indices': market_indices,
                        'signals': kr_ai_signals,
                        'generated_at': datetime.now().isoformat(),
                        'signal_date': signals[0]['signal_date']
                    }
                    
                    kr_ai_path = os.path.join(BASE_DIR, 'data', 'kr_ai_analysis.json')
                    with open(kr_ai_path, 'w', encoding='utf-8') as f:
                        json.dump(kr_ai_data, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
                    
                    # ë‚ ì§œë³„ íˆìŠ¤í† ë¦¬ë„ ì €ì¥
                    kr_ai_history_path = os.path.join(BASE_DIR, 'data', f'kr_ai_analysis_{date_str}.json')
                    with open(kr_ai_history_path, 'w', encoding='utf-8') as f:
                        json.dump(kr_ai_data, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
                         
                    log(f"[AI Analysis] ë¶„ì„ ì™„ë£Œ ë° ì €ì¥: {ai_filename}, kr_ai_analysis.json", "SUCCESS")
                
            except Exception as e:
                log(f"[AI Analysis] ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", "ERROR")
                import traceback
                traceback.print_exc()
        


        # [FIX] AI ë¶„ì„ ê²°ê³¼ë¥¼ signals ë¦¬ìŠ¤íŠ¸ì— ë³‘í•© (CSV ì €ì¥ì„ ìœ„í•´)
        if run_ai and signals and 'ai_results' in locals() and ai_results:
            try:
                for signal in signals:
                    ticker = signal['ticker']
                    if ticker in ai_results:
                        ai_data = ai_results[ticker]
                        
                        # Gemini ê²°ê³¼ ìš°ì„ 
                        gemini = ai_data.get('gemini_recommendation')
                        if gemini:
                            signal['ai_action'] = gemini.get('action', 'HOLD')
                            signal['ai_confidence'] = gemini.get('confidence', 0)
                            signal['ai_reason'] = gemini.get('reason', '')
                        else:
                            # ë‹¤ë¥¸ AI ê²°ê³¼ í´ë°±? (ì¼ë‹¨ Gemini ê¸°ì¤€)
                            signal['ai_action'] = 'N/A'
                            signal['ai_confidence'] = 0
                            signal['ai_reason'] = 'ë¶„ì„ ì‹¤íŒ¨'
            except Exception as e:
                log(f"AI ê²°ê³¼ ë³‘í•© ì¤‘ ì˜¤ë¥˜: {e}", "WARNING")

        if signals:
            df_new = pd.DataFrame(signals)
            file_path = os.path.join(BASE_DIR, 'data', 'signals_log.csv')
            
            # ê¸°ì¡´ ë¡œê·¸ê°€ ìˆìœ¼ë©´ ë¡œë“œí•˜ì—¬ ë³‘í•© (Append & Deduplicate)
            if os.path.exists(file_path):
                try:
                    # íƒ€ì… ëª…ì‹œí•˜ì—¬ ë¡œë“œ (ì¤‘ë³µ ë°©ì§€ í•µì‹¬)
                    df_old = pd.read_csv(file_path, dtype={'ticker': str, 'signal_date': str})
                    df_old['ticker'] = df_old['ticker'].str.zfill(6)
                    
                    # ìƒˆ ë°ì´í„° í¬ë§· í†µì¼
                    df_new['ticker'] = df_new['ticker'].astype(str).str.zfill(6)
                    df_new['signal_date'] = df_new['signal_date'].astype(str)

                    # [ìˆ˜ì •] í•´ë‹¹ ë‚ ì§œì˜ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì¬ì‹¤í–‰ ì‹œ ì¤‘ë³µ ë°©ì§€)
                    current_date = str(df_new['signal_date'].iloc[0])
                    df_old = df_old[df_old['signal_date'] != current_date]

                    # ë³‘í•©
                    if df_old.empty and df_new.empty:
                         df_combined = pd.DataFrame()
                    elif df_old.empty:
                         df_combined = df_new
                    elif df_new.empty:
                         df_combined = df_old
                    else:
                         df_combined = pd.concat([df_old, df_new])
                         
                    # ì¤‘ë³µ ì œê±° (ì•ˆì „ì¥ì¹˜)
                    if not df_combined.empty:
                        df_combined = df_combined.drop_duplicates(subset=['signal_date', 'ticker'], keep='last')
                        # ì •ë ¬ (ìµœì‹  ë‚ ì§œ ìš°ì„ , ì ìˆ˜ ë†’ì€ ìˆœ)
                        df_combined = df_combined.sort_values(by=['signal_date', 'score'], ascending=[False, False])
                    
                    df_combined.to_csv(file_path, index=False, encoding='utf-8-sig')
                    # í•´ë‹¹ ë‚ ì§œ ë°ì´í„° ë°˜í™˜ (common.py ì—°ë™ìš©) -> init_data.pyì—ì„œëŠ” True ë°˜í™˜í•´ì•¼ í•¨
                    return True
                except Exception as e:
                    log(f"ê¸°ì¡´ ë¡œê·¸ ë³‘í•© ì‹¤íŒ¨: {e}, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤(ë®ì–´ì“°ê¸°).", "WARNING")
                    df_new.to_csv(file_path, index=False, encoding='utf-8-sig')
                    return True
            else:
                df_new.to_csv(file_path, index=False, encoding='utf-8-sig')
                return True
                
            log(f"VCP ì‹œê·¸ë„ ë¶„ì„ ì™„ë£Œ: {len(signals)} ì¢…ëª© ê°ì§€ (ëˆ„ì  ì €ì¥)", "SUCCESS")
            return True
        else:
            log("VCP ì¡°ê±´ ì¶©ì¡± ì¢…ëª© ì—†ìŒ", "WARNING")
            # ë¹ˆ ê²°ê³¼ íŒŒì¼ ìƒì„± (ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì•ˆí•¨)
            df = pd.DataFrame(columns=['ticker', 'name', 'signal_date', 'market', 'status', 'score', 'contraction_ratio', 'entry_price', 'foreign_5d', 'inst_5d'])
            file_path = os.path.join(BASE_DIR, 'data', 'signals_log.csv')
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            log("VCP ì¡°ê±´ ì¶©ì¡± ì¢…ëª© ì—†ìŒ - ë¹ˆ ê²°ê³¼ ì €ì¥", "INFO")
            return True
            
    except Exception as e:
        log(f"VCP ë¶„ì„ ì‹¤íŒ¨: {e}", "WARNING")
        # ë¹ˆ ê²°ê³¼ íŒŒì¼ ìƒì„± (ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì•ˆí•¨)
        df = pd.DataFrame(columns=['ticker', 'name', 'signal_date', 'market', 'status', 'score', 'contraction_ratio', 'entry_price', 'foreign_5d', 'inst_5d'])
        file_path = os.path.join(BASE_DIR, 'data', 'signals_log.csv')
        df.to_csv(file_path, index=False, encoding='utf-8-sig')
        log("VCP ë¶„ì„ ì˜¤ë¥˜ - ë¹ˆ ê²°ê³¼ ì €ì¥", "INFO")
        return True



def create_jongga_v2_latest():
    """ì¢…ê°€ë² íŒ… V2 ìµœì‹  ê²°ê³¼ ìƒì„± - Using Central SignalGenerator"""
    log("ì¢…ê°€ë² íŒ… V2 ë¶„ì„ ì¤‘ (SignalGenerator)...")
    try:
        from engine.generator import run_screener
        import asyncio

        # Run analysis (Sync wrapper for Async)
        # run_screener returns ScreenerResult object
        result = asyncio.run(run_screener())
        
        # Convert to JSON serializable structure
        if result:
            signals_json = [s.to_dict() for s in result.signals]
            
            output_data = {
                'date': result.date.strftime('%Y-%m-%d'),
                'total_candidates': result.total_candidates,
                'filtered_count': result.filtered_count,
                'scanned_count': getattr(result, 'scanned_count', 0),
                'signals': signals_json,
                'by_grade': result.by_grade,
                'by_market': result.by_market,
                'processing_time_ms': result.processing_time_ms,
                'market_status': result.market_status,
                'market_summary': result.market_summary,
                'trending_themes': result.trending_themes,
                'updated_at': datetime.now().isoformat()
            }

            # Save to JSON
            file_path = os.path.join(BASE_DIR, 'data', 'jongga_v2_latest.json')
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
                
            log(f"ì¢…ê°€ë² íŒ… V2 ë¶„ì„ ì™„ë£Œ: {len(signals_json)} ì¢…ëª© (SignalGenerator)", "SUCCESS")
            return True
        else:
            log("ì¢…ê°€ë² íŒ… ë¶„ì„ ê²°ê³¼ ì—†ìŒ (None returned)", "WARNING")
            return False

    except Exception as e:
        log(f"ì¢…ê°€ë² íŒ… ë¶„ì„ ì‹¤íŒ¨: {e}", "ERROR")
        import traceback
        traceback.print_exc()
        return False


def create_market_gate(target_date=None):
    """Market Gate ë°ì´í„° ìƒì„± (8ê°œ ì„¹í„°, KOSPI/KOSDAQ ì§€ìˆ˜ í¬í•¨) - ì‹¤ì‹œê°„ ë°ì´í„°"""
    log("Market Gate ë°ì´í„° ìƒì„± ì¤‘...")
    try:
        # ì‹¤ì‹œê°„ ì‹œì¥ ì§€ìˆ˜ ìˆ˜ì§‘
        indices = get_market_indices()
        kospi = indices['kospi']
        kosdaq = indices['kosdaq']
        
        # Market Gate ì ìˆ˜ ê³„ì‚° (KOSPI ë“±ë½ë¥  ê¸°ë°˜ ì„¸ë¶„í™”)
        change = kospi['change_pct']
        
        if change >= 2.0:
            gate_status = 'GREEN'
            gate_label = 'VERY BULLISH'
            gate_score = 90
        elif change >= 1.0:
            gate_status = 'GREEN'
            gate_label = 'BULLISH'
            gate_score = 75
        elif change >= 0.5:
            gate_status = 'YELLOW'
            gate_label = 'SLIGHTLY BULLISH'
            gate_score = 60
        elif change >= 0:
            gate_status = 'YELLOW'
            gate_label = 'NEUTRAL'
            gate_score = 50
        elif change >= -0.5:
            gate_status = 'YELLOW'
            gate_label = 'SLIGHTLY BEARISH'
            gate_score = 40
        elif change >= -1.0:
            gate_status = 'RED'
            gate_label = 'BEARISH'
            gate_score = 25
        else:
            gate_status = 'RED'
            gate_label = 'VERY BEARISH'
            gate_score = 10
        
        gate_data = {
            'status': gate_status,
            'score': gate_score,
            'label': gate_label,
            'reasons': [
                f"KOSPI {kospi['change_pct']:+.2f}% ë³€ë™",
                'ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ ì§€ì†',
                'ë°˜ë„ì²´ ì„¹í„° ê°•ì„¸ ì§€ì†'
            ],
            'sectors': get_sector_indices(),  # ì‹¤ì œ ì„¹í„° ë°ì´í„° ì‚¬ìš©
            'indices': {
                'kospi': {'value': kospi['value'], 'change_pct': kospi['change_pct']},
                'kosdaq': {'value': kosdaq['value'], 'change_pct': kosdaq['change_pct']}
            },
            'commodities': {
                'gold': indices.get('kr_gold', {'value': 0, 'change_pct': 0}),
                'silver': indices.get('kr_silver', {'value': 0, 'change_pct': 0}),
                'us_gold': indices.get('us_gold', {'value': 0, 'change_pct': 0}),
                'us_silver': indices.get('us_silver', {'value': 0, 'change_pct': 0})
            },
            'global_indices': {
                'sp500': indices.get('sp500', {'value': 0, 'change_pct': 0}),
                'nasdaq': indices.get('nasdaq', {'value': 0, 'change_pct': 0})
            },
            'crypto': {
                'btc': indices.get('btc', {'value': 0, 'change_pct': 0}),
                'eth': indices.get('eth', {'value': 0, 'change_pct': 0}),
                'xrp': indices.get('xrp', {'value': 0, 'change_pct': 0})
            },
            'metrics': {
                'kospi': kospi['value'],
                'kospi_ma20': kospi['value'] * 0.98,  # ê·¼ì‚¬ê°’
                'kospi_ma60': kospi['value'] * 0.96,  # ê·¼ì‚¬ê°’
                'kosdaq': kosdaq['value'],
                'kosdaq_ma20': kosdaq['value'] * 0.98,
                'usd_krw': 1345.5,
                'foreign_net_total': 1200000000000,
                'rsi': 62.5
            },
            'updated_at': datetime.now().isoformat()
        }

        file_path = os.path.join(BASE_DIR, 'data', 'market_gate.json')
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(gate_data, f, indent=2, ensure_ascii=False)
            
        # ë‚ ì§œë³„ ì•„ì¹´ì´ë¸Œ ì €ì¥
        if target_date:
             date_str = target_date.replace('-', '') if isinstance(target_date, str) else target_date.strftime('%Y%m%d')
        else:
             date_str = datetime.now().strftime('%Y%m%d')
        
        archive_path = os.path.join(BASE_DIR, 'data', f'market_gate_{date_str}.json')
        with open(archive_path, 'w', encoding='utf-8') as f:
             json.dump(gate_data, f, indent=2, ensure_ascii=False)
             
        log(f"Market Gate ë°ì´í„° ìƒì„± ì™„ë£Œ: {file_path}", "SUCCESS")
        return True

    except Exception as e:
        log(f"Market Gate ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}", "ERROR")
        return False

def create_kr_ai_analysis(target_date=None):
    """AI ë¶„ì„ ê²°ê³¼ ìƒì„± (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)"""
    log("AI ë¶„ì„ ì‹œì‘ (Real Mode)...")
    try:
        import sys
        # Root path ì¶”ê°€
        root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        if root_dir not in sys.path:
            sys.path.append(root_dir)
            
        from engine.kr_ai_analyzer import KrAiAnalyzer
        import pandas as pd
        import json
        
        # ë‚ ì§œ ì„¤ì •
        if not target_date:
            target_date = datetime.now().strftime('%Y-%m-%d')
            
        data_dir = os.path.join(BASE_DIR, 'data')
        signals_path = os.path.join(data_dir, 'signals_log.csv')
        
        if not os.path.exists(signals_path):
            log("VCP ì‹œê·¸ë„ íŒŒì¼ì´ ì—†ì–´ AI ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.", "WARNING")
            return
            
        # VCP ê²°ê³¼ ë¡œë“œ
        df = pd.read_csv(signals_path, dtype={'ticker': str, 'signal_date': str})
        if df.empty:
            log("VCP ì‹œê·¸ë„ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.", "WARNING")
            return

        # í•´ë‹¹ ë‚ ì§œ ë°ì´í„° í•„í„°ë§
        target_df = df[df['signal_date'] == str(target_date)].copy()
        
        if target_df.empty:
            # ë‚ ì§œ í¬ë§· ë¶ˆì¼ì¹˜ ê°€ëŠ¥ì„± ì²´í¬ (YYYY-MM-DD vs YYYYMMDD)
            alt_date = target_date.replace('-', '')
            target_df = df[df['signal_date'] == alt_date].copy()
            
        if target_df.empty:
            log(f"í•´ë‹¹ ë‚ ì§œ({target_date})ì˜ VCP ì‹œê·¸ë„ì´ ì—†ìŠµë‹ˆë‹¤.", "WARNING")
            return

        # [í•„ìˆ˜] ê¸°ì¡´ ë¶„ì„ íŒŒì¼ ì‚­ì œ (ì´ˆê¸°í™”)
        date_str_clean = str(target_date).replace('-', '')
        filename = f'ai_analysis_results_{date_str_clean}.json'
        filepath = os.path.join(data_dir, filename)
        
        if os.path.exists(filepath):
            try:
                os.remove(filepath)
                log(f"ê¸°ì¡´ AI ë¶„ì„ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {filename}", "INFO")
            except Exception as e:
                log(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}", "WARNING")

        # ë¶„ì„ ëŒ€ìƒ ì„ ì • (Score ìƒìœ„ 20ê°œ)
        if 'score' in target_df.columns:
            target_df['score'] = pd.to_numeric(target_df['score'], errors='coerce').fillna(0)
            target_df = target_df.sort_values('score', ascending=False)
            
        target_df = target_df.head(20)
        tickers = target_df['ticker'].tolist()
        
        log(f"AI ë¶„ì„ ëŒ€ìƒ: {len(tickers)} ì¢…ëª©")
        
        # ë¶„ì„ ì‹¤í–‰
        analyzer = KrAiAnalyzer()
        results = analyzer.analyze_multiple_stocks(tickers)
        
        # [Fix] CSVì˜ supply ë°ì´í„°ë¥¼ AI ê²°ê³¼ì— ë³‘í•©
        try:
            csv_data = {row['ticker']: row for _, row in target_df.iterrows()}
            for signal in results.get('signals', []):
                ticker = signal.get('ticker')
                if ticker in csv_data:
                    csv_row = csv_data[ticker]
                    
                    # ë°ì´í„° ë³‘í•© (íƒ€ì… ì•ˆì „ ì²˜ë¦¬)
                    try:
                        signal['foreign_5d'] = int(float(csv_row.get('foreign_5d', 0)))
                    except: signal['foreign_5d'] = 0
                        
                    try:
                        signal['inst_5d'] = int(float(csv_row.get('inst_5d', 0)))
                    except: signal['inst_5d'] = 0
                        
                    try:
                        signal['score'] = float(csv_row.get('score', 0))
                    except: signal['score'] = 0.0
                        
                    try:
                        signal['contraction_ratio'] = float(csv_row.get('contraction_ratio', 0))
                    except: signal['contraction_ratio'] = 0.0
                        
                    try:
                        signal['entry_price'] = int(float(csv_row.get('entry_price', 0)))
                    except: signal['entry_price'] = 0

                    try:
                        current_p = int(float(csv_row.get('current_price', 0)))
                        if current_p == 0:
                            current_p = signal['entry_price']
                        signal['current_price'] = current_p
                    except: signal['current_price'] = signal.get('entry_price', 0)
                        
                    try:
                        signal['vcp_score'] = float(csv_row.get('vcp_score', 0))
                    except: signal['vcp_score'] = 0.0
                        
                    signal['market'] = csv_row.get('market', signal.get('market', 'KOSPI'))
            
            log("AI ê²°ê³¼ì— Supply ë°ì´í„° ë³‘í•© ì™„ë£Œ", "INFO")
        except Exception as merge_e:
            log(f"ë°ì´í„° ë³‘í•© ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {merge_e}", "WARNING")
        
        # ë©”íƒ€ë°ì´í„°
        results['generated_at'] = datetime.now().isoformat()
        results['signal_date'] = target_date
        
        # ì‹œì¥ ì§€ìˆ˜ ë°ì´í„° ìˆ˜ì§‘ (frontend í˜¸í™˜ìš©)
        market_indices = {}
        try:
            from pykrx import stock
            today_str = datetime.now().strftime('%Y%m%d')
            kospi = stock.get_index_ohlcv(today_str, today_str, "1001")
            kosdaq = stock.get_index_ohlcv(today_str, today_str, "2001")
            
            if not kospi.empty:
                market_indices['kospi'] = {
                    'value': float(kospi['ì¢…ê°€'].iloc[-1]),
                    'change_pct': float(kospi['ë“±ë½ë¥ '].iloc[-1]) if 'ë“±ë½ë¥ ' in kospi.columns else 0
                }
            if not kosdaq.empty:
                market_indices['kosdaq'] = {
                    'value': float(kosdaq['ì¢…ê°€'].iloc[-1]),
                    'change_pct': float(kosdaq['ë“±ë½ë¥ '].iloc[-1]) if 'ë“±ë½ë¥ ' in kosdaq.columns else 0
                }
        except: pass
        
        results['market_indices'] = market_indices

        # ì €ì¥ (ai_analysis_results.json)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
            
        log(f"AI ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}", "SUCCESS")
        
        # ìµœì‹  íŒŒì¼ (ai_analysis_results.json)
        if target_date == datetime.now().strftime('%Y-%m-%d'):
            main_path = os.path.join(data_dir, 'ai_analysis_results.json')
            with open(main_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            # [Fix] kr_ai_analysis.json ìƒì„± (Frontendìš©)
            kr_ai_path = os.path.join(data_dir, 'kr_ai_analysis.json')
            with open(kr_ai_path, 'w', encoding='utf-8') as f:
                 json.dump(results, f, ensure_ascii=False, indent=2)
            log(f"Frontend ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ: {kr_ai_path}", "SUCCESS")
                
        return True

    except Exception as e:
        log(f"AI ë¶„ì„ ì‹¤íŒ¨: {e}", "ERROR")
        import traceback
        traceback.print_exc()
        return False

def create_kr_ai_analysis_with_key(target_dates=None, api_key=None):
    """
    [ì‚¬ìš©ì ìš”ì²­] API Keyë¥¼ ì£¼ì…í•˜ì—¬ AI ë¶„ì„ ì‹¤í–‰ (create_kr_ai_analysis ë³€í˜•)
    - ê³µìš© ë°°ì¹˜ ì‘ì—…ì´ ì•„ë‹ˆë¼, íŠ¹ì • ì‚¬ìš©ìì˜ ìš”ì²­ì— ì˜í•´ íŠ¸ë¦¬ê±°ë¨.
    - target_dates: ['YYYY-MM-DD', ...] or None
    - api_key: ì‚¬ìš©ìì˜ Google Gemini API Key (ì—†ìœ¼ë©´ ê³µìš© í‚¤ ì‚¬ìš© - ì •ì±…ì— ë”°ë¦„)
    """
    log(f"AI ì¬ë¶„ì„ ìš”ì²­ (Key Present: {bool(api_key)})", "INFO")
    
    try:
        import sys
        # Root path ì¶”ê°€
        root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        if root_dir not in sys.path:
            sys.path.append(root_dir)
            
        from kr_ai_analyzer import KrAiAnalyzer
        import pandas as pd
        import json
        
        # Analyzer ì´ˆê¸°í™”ì‹œ í‚¤ ì£¼ì…
        analyzer = KrAiAnalyzer(api_key=api_key)
        
        data_dir = os.path.join(BASE_DIR, 'data')
        signals_path = os.path.join(data_dir, 'signals_log.csv')
        
        if not os.path.exists(signals_path):
            log("VCP ì‹œê·¸ë„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", "WARNING")
            return {'count': 0}

        df = pd.read_csv(signals_path, dtype={'ticker': str, 'signal_date': str})
        if df.empty:
            return {'count': 0}

        # ë‚ ì§œ í•„í„°ë§
        if not target_dates:
            # ë‚ ì§œ ì—†ìœ¼ë©´ ìµœì‹  ë‚ ì§œ í•˜ë‚˜ë§Œ
            latest_date = df['signal_date'].max()
            target_dates = [latest_date]
            
        all_results = {}
        total_analyzed = 0
        
        for t_date in target_dates:
            log(f"Deep Analysis for date: {t_date}")
            
            # ë‚ ì§œ í¬ë§· ë§¤ì¹­
            target_df = df[df['signal_date'] == str(t_date)].copy()
            if target_df.empty:
                 alt_date = str(t_date).replace('-', '')
                 target_df = df[df['signal_date'] == alt_date].copy()
            
            if target_df.empty:
                continue
                
            # Score ìƒìœ„ ì¢…ëª© ì„ ì •
            if 'score' in target_df.columns:
                target_df['score'] = pd.to_numeric(target_df['score'], errors='coerce').fillna(0)
                target_df = target_df.sort_values('score', ascending=False)
            
            # ìµœëŒ€ 20ê°œ (Rate Limit ë° ì‹œê°„ ê³ ë ¤)
            target_df = target_df.head(20)
            tickers = target_df['ticker'].tolist()
            
            # ë¶„ì„ ì‹¤í–‰
            results = analyzer.analyze_multiple_stocks(tickers) # api_key ì‚¬ìš©ë¨
            
            if results and 'signals' in results:
                count = len(results['signals'])
                total_analyzed += count
                
                # ì €ì¥ (ë®ì–´ì“°ê¸°)
                date_str_clean = str(t_date).replace('-', '')
                filename = f'ai_analysis_results_{date_str_clean}.json'
                filepath = os.path.join(data_dir, filename)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                
                # ì˜¤ëŠ˜ ë‚ ì§œë©´ ë©”ì¸ íŒŒì¼ë„ ì—…ë°ì´íŠ¸
                if t_date == datetime.now().strftime('%Y-%m-%d'):
                    main_path = os.path.join(data_dir, 'ai_analysis_results.json')
                    with open(main_path, 'w', encoding='utf-8') as f:
                        json.dump(results, f, ensure_ascii=False, indent=2)
                        
        return {'count': total_analyzed}

    except Exception as e:
        log(f"AI ì¬ë¶„ì„ ì‹¤íŒ¨: {e}", "ERROR")
        import traceback
        traceback.print_exc()
        return {'error': str(e)}

def send_jongga_notification():
    """ì¢…ê°€ë² íŒ… V2 ê²°ê³¼ ì•Œë¦¼ ë°œì†¡"""
    try:
        import json
        import os
        from engine.messenger import Messenger
        from engine.models import ScreenerResult, Signal, ScoreDetail, ChecklistDetail, SignalStatus, Grade
        from datetime import datetime
        
        data_file = os.path.join(BASE_DIR, 'data', 'jongga_v2_latest.json')
        
        if os.path.exists(data_file):
            with open(data_file, 'r', encoding='utf-8') as f:
                file_data = json.load(f)
            
            if file_data and file_data.get('signals'):
                # ê°ì²´ ë³µì› (Messenger í˜¸í™˜ì„±)
                signals = []
                for i, s in enumerate(file_data.get('signals', [])):
                    # ScoreDetail ë³µì› (total í¬í•¨)
                    sc = s.get('score', {})
                    score_obj = ScoreDetail(**sc)
                    
                    # ChecklistDetail ë³µì›
                    cl = s.get('checklist', {})
                    checklist_obj = ChecklistDetail(**cl)
                    
                    # ë‚ ì§œ/ì‹œê°„
                    try:
                        sig_date = datetime.strptime(s.get('signal_date', datetime.now().strftime('%Y-%m-%d')), '%Y-%m-%d').date()
                    except:
                        sig_date = datetime.now().date()
                        
                    try:
                        created_at = datetime.fromisoformat(s.get('created_at', datetime.now().isoformat()))
                    except:
                        created_at = datetime.now()
                    
                    # Enum ì²˜ë¦¬
                    grade_val = s.get('grade')
                    if isinstance(grade_val, str):
                        try:
                            grade = Grade(grade_val)
                        except:
                            grade = grade_val
                    
                    status_val = s.get('status', 'waiting')
                    if isinstance(status_val, str):
                        try:
                            status = SignalStatus(status_val)
                        except:
                            status = SignalStatus.PENDING
                    
                    target_price = s.get('target_price', 0)
                    if target_price == 0:
                        target_price = s.get('target_price_1', 0)

                    signal_obj = Signal(
                        stock_code=s['stock_code'],
                        stock_name=s['stock_name'],
                        market=s.get('market', ''),
                        sector=s.get('sector', ''),
                        signal_date=sig_date,
                        signal_time=datetime.now(),
                        grade=grade,
                        score=score_obj,
                        checklist=checklist_obj,
                        news_items=s.get('news_items', []),
                        current_price=s.get('current_price', 0.0),
                        entry_price=s.get('entry_price', 0),
                        stop_price=s.get('stop_price', 0),
                        target_price=target_price,
                        r_value=s.get('r_value', 0.0),
                        position_size=s.get('position_size', 0.0),
                        quantity=s.get('quantity', 0),
                        r_multiplier=s.get('r_multiplier', 0.0),
                        trading_value=s.get('trading_value', 0),
                        change_pct=s.get('change_pct', 0.0),
                        status=status,
                        created_at=created_at,
                        volume_ratio=s.get('volume_ratio', 0.0),
                        themes=s.get('themes', []),
                        score_details=s.get('score_details', {})
                    )
                    signals.append(signal_obj)
                    
                # ScreenerResult ìƒì„±
                res_date = datetime.now().date()
                try:
                    date_val = file_data.get('date')
                    if date_val:
                        res_date = datetime.strptime(date_val, '%Y-%m-%d').date()
                except:
                    pass
                
                # Calculate statistics if missing
                by_grade = file_data.get('by_grade', {})
                if not by_grade:
                    from collections import Counter
                    grades = [str(s.grade.value if hasattr(s.grade, 'value') else s.grade) for s in signals]
                    by_grade = dict(Counter(grades))
                    
                by_market = file_data.get('by_market', {})
                if not by_market:
                    from collections import Counter
                    markets = [s.market for s in signals]
                    by_market = dict(Counter(markets))
                    
                result = ScreenerResult(
                    date=res_date,
                    total_candidates=file_data.get('total_candidates', 0),
                    filtered_count=len(signals),
                    scanned_count=file_data.get('scanned_count', 0),
                    signals=signals,
                    by_grade=by_grade,
                    by_market=by_market,
                    processing_time_ms=file_data.get('processing_time_ms', 0.0),
                    market_status=file_data.get('market_status'),
                    market_summary=file_data.get('market_summary', ""),
                    trending_themes=file_data.get('trending_themes', [])
                )
                
                messenger = Messenger()
                messenger.send_screener_result(result)
                log(f"ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ: {len(signals)}ê°œ ì‹ í˜¸", "SUCCESS")
            else:
                log("ë°œì†¡í•  ì‹ í˜¸ ì—†ìŒ (0ê°œ)", "INFO")
                
    except Exception as notify_error:
        log(f"ì•Œë¦¼ ë°œì†¡ ì¤‘ ì˜¤ë¥˜: {notify_error}", "ERROR")
        import traceback
        traceback.print_exc()

def main():
    log("ë°ì´í„° ì´ˆê¸°í™” ì‹œì‘...", "HEADER")
    data_dir = os.path.join(BASE_DIR, 'data')
    ensure_directory(data_dir)
    
    tasks = [
        create_korean_stocks_list,
        create_daily_prices,
        create_institutional_trend,
        create_signals_log,
        create_jongga_v2_latest,

        create_kr_ai_analysis  # AI ë¶„ì„ ì¶”ê°€
    ]

    
    success_count = 0
    total_tasks = len(tasks)
    
    for task in tasks:
        if task():
            success_count += 1
            
    print()
    log("ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ", "HEADER")
    print(f"ì™„ë£Œëœ ì‘ì—…: {success_count}/{total_tasks}")
    
    if success_count == total_tasks:
        log("ğŸ‰ ëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!", "SUCCESS")
        log("ë‹¤ìŒ ë‹¨ê³„: [python3 flask_app.py] ì‹¤í–‰ í›„ í”„ë¡ íŠ¸ì—”ë“œ í™•ì¸")
    else:
        log(f"âš ï¸ ì¼ë¶€ ë°ì´í„° ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ ({total_tasks - success_count}/{total_tasks}).", "WARNING")
        log("ìƒì„¸ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.", "WARNING")


def update_vcp_signals_recent_price():
    """VCP ì‹œê·¸ë„ ë¡œê·¸(signals_log.csv)ì˜ ìµœì‹  ê°€ê²© ì—…ë°ì´íŠ¸"""
    log("VCP ì‹œê·¸ë„ ìµœì‹  ê°€ê²© ì—…ë°ì´íŠ¸ ì‹œì‘...")
    try:
        file_path = os.path.join(BASE_DIR, 'data', 'signals_log.csv')
        if not os.path.exists(file_path):
            log("VCP ì‹œê·¸ë„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", "WARNING")
            return

        df = pd.read_csv(file_path, dtype={'ticker': str})
        
        # ì˜¤ëŠ˜ ë‚ ì§œ
        today_str = datetime.now().strftime('%Y%m%d')
        
        # ìµœì‹  ê°€ê²© ë°ì´í„° ë¡œë“œ (pykrx ì‚¬ìš©)
        from pykrx import stock
        import time

        updated_count = 0
        
        # ìœ ë‹ˆí¬ í‹°ì»¤ ëª©ë¡
        tickers = df['ticker'].unique()
        
        current_prices = {}
        log(f"ì´ {len(tickers)}ê°œ ì¢…ëª©ì˜ í˜„ì¬ê°€ ì¡°íšŒ ì¤‘...")
        
        for ticker in tickers:
            try:
                price_found = False
                current_price = 0
                
                # 1. pykrx ì‹œë„
                try:
                    df_price = stock.get_market_ohlcv(today_str, today_str, ticker)
                    if not df_price.empty:
                        current_price = int(df_price['ì¢…ê°€'].iloc[-1])
                        if current_price > 0:
                            current_prices[ticker] = current_price
                            price_found = True
                except:
                    pass
                
                # 2. yfinance í´ë°± (fetch_stock_price ì‚¬ìš©)
                if not price_found:
                    data = fetch_stock_price(ticker)
                    if data and 'price' in data:
                        current_price = int(data['price'])
                        if current_price > 0:
                            current_prices[ticker] = current_price
                            price_found = True
                            # log(f"  -> {ticker} yfinance í´ë°± ì„±ê³µ: {current_price}", "INFO")

                time.sleep(0.01) # Rate limiting
            except Exception as e:
                # log(f"{ticker} ê°€ê²© ì¡°íšŒ ì‹¤íŒ¨: {e}", "WARNING")
                pass
        
        log(f"{len(current_prices)}ê°œ ì¢…ëª© í˜„ì¬ê°€ í™•ë³´ ì™„ë£Œ. ì—…ë°ì´íŠ¸ ì ìš© ì¤‘...")
        
        # ë°ì´í„°í”„ë ˆì„ ì—…ë°ì´íŠ¸
        for idx, row in df.iterrows():
            ticker = row['ticker']
            if ticker in current_prices:
                current_p = current_prices[ticker]
                entry_p = row['entry_price']
                
                df.at[idx, 'current_price'] = current_p
                if entry_p > 0:
                    ret = ((current_p - entry_p) / entry_p) * 100
                    df.at[idx, 'return_pct'] = round(ret, 2)
                
                updated_count += 1
        
        # ì €ì¥
        df.to_csv(file_path, index=False, encoding='utf-8-sig')
        log(f"VCP ì‹œê·¸ë„ ê°€ê²© ì—…ë°ì´íŠ¸ ì™„ë£Œ: {updated_count}ê±´ ê°±ì‹ ", "SUCCESS")
        
        # kr_ai_analysis.jsonë„ ë™ê¸°í™” (ì„ íƒ ì‚¬í•­)
        update_kr_ai_analysis_prices(current_prices)
        
    except Exception as e:
        log(f"ê°€ê²© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}", "ERROR")

def update_kr_ai_analysis_prices(price_map):
    """kr_ai_analysis.json íŒŒì¼ì˜ ê°€ê²© ì •ë³´ë„ ì—…ë°ì´íŠ¸"""
    try:
        kr_ai_path = os.path.join(BASE_DIR, 'data', 'kr_ai_analysis.json')
        if not os.path.exists(kr_ai_path):
            return
            
        with open(kr_ai_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        updated = False
        if 'signals' in data:
            for signal in data['signals']:
                ticker = signal.get('ticker')
                if ticker in price_map:
                    current_p = price_map[ticker]
                    entry_p = signal.get('entry_price', current_p)
                    
                    signal['current_price'] = current_p
                    if entry_p > 0:
                        signal['return_pct'] = round(((current_p - entry_p) / entry_p) * 100, 2)
                    updated = True
        
        if updated:
            with open(kr_ai_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
            log("kr_ai_analysis.json ê°€ê²© ë™ê¸°í™” ì™„ë£Œ", "INFO")
            
    except Exception as e:
        log(f"AI ë¶„ì„ íŒŒì¼ ê°€ê²© ë™ê¸°í™” ì‹¤íŒ¨: {e}", "WARNING")

if __name__ == '__main__':
    if len(sys.argv) > 1:
        cmd = sys.argv[1]
        if cmd == "init-prices":
            create_daily_prices()
        elif cmd == "init-inst":
            create_institutional_trend()
        elif cmd == "init-stocks":
            create_korean_stocks_list()
        elif cmd == "vcp-signal":
             # íŠ¹ì • ë‚ ì§œ ì§€ì • ê°€ëŠ¥ (YYYY-MM-DD)
            target_date = sys.argv[2] if len(sys.argv) > 2 else None
            create_signals_log(target_date)
        elif cmd == "ai-analysis":
            create_kr_ai_analysis()
        elif cmd == "update-prices":
            update_vcp_signals_recent_price()
        elif cmd == "all":
            log("ì „ì²´ ë°ì´í„° ì´ˆê¸°í™” ì‹œì‘...")
            create_korean_stocks_list()
            create_daily_prices()
            create_institutional_trend()
            create_signals_log() # VCP ë¶„ì„
            create_kr_ai_analysis() # AI ë¶„ì„
            log("ì „ì²´ ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ!", "SUCCESS")
    else:
        main()

