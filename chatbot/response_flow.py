#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì±—ë´‡ ì‘ë‹µ ì²˜ë¦¬(ì—ëŸ¬/ìŠ¤íŠ¸ë¦¬ë°/í´ë°±) ìœ í‹¸
"""

from __future__ import annotations

import logging
from typing import Any, Callable, Dict, Generator, List, Optional, Tuple

from .markdown_utils import _compute_stream_delta, _extract_reasoning_and_answer


def extract_usage_metadata(response: Any) -> Dict[str, int]:
    """Gemini ì‘ë‹µì—ì„œ í† í° ì‚¬ìš©ëŸ‰ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•œë‹¤."""
    if not hasattr(response, "usage_metadata"):
        return {}
    meta = response.usage_metadata
    return {
        "prompt_token_count": getattr(meta, "prompt_token_count", 0),
        "candidates_token_count": getattr(meta, "candidates_token_count", 0),
        "total_token_count": getattr(meta, "total_token_count", 0),
    }


def friendly_error_message(error_msg: str, default_prefix: str) -> str:
    """ì‚¬ìš©ì ì¹œí™” ì—ëŸ¬ ë©”ì‹œì§€ ë³€í™˜."""
    if (
        "429" in error_msg
        or "Resource exhausted" in error_msg
        or "RESOURCE_EXHAUSTED" in error_msg
    ):
        return (
            "âš ï¸ **AI ì„œë²„ ìš”ì²­ í•œë„ ì´ˆê³¼**\n\n"
            "Google AI ì„œë²„ì˜ ë¶„ë‹¹ ìš”ì²­ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.\n"
            "**ì•½ 30ì´ˆ~1ë¶„ í›„ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.**\n\n"
            "ğŸ’¡ ì•ˆì •ì ì¸ ì‚¬ìš©ì„ ìœ„í•´ **[ì„¤ì •] > [API Key]** ë©”ë‰´ì—ì„œ "
            "ê°œì¸ API Keyë¥¼ ë“±ë¡í•˜ì‹œë©´ ì´ ì œí•œì„ í”¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )

    if (
        "400" in error_msg
        or "API_KEY_INVALID" in error_msg
        or "API key not valid" in error_msg
    ):
        return (
            "âš ï¸ **API Key ì„¤ì • ì˜¤ë¥˜**\n\n"
            "ì‹œìŠ¤í…œì— ì„¤ì •ëœ API Keyê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
            "ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ê±°ë‚˜ **[ì„¤ì •] > [API Key]** ë©”ë‰´ì—ì„œ "
            "ì˜¬ë°”ë¥¸ API Keyë¥¼ ë‹¤ì‹œ ë“±ë¡í•´ì£¼ì„¸ìš”.\n"
            "(Google ì„œë¹„ìŠ¤ ë¬¸ì œì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.)"
        )

    return f"{default_prefix}{error_msg}"


def build_fallback_models(target_model_name: str) -> List[str]:
    """ìŠ¤íŠ¸ë¦¬ë° ì¬ì‹œë„ìš© ëª¨ë¸ í›„ë³´ ë¦¬ìŠ¤íŠ¸."""
    fallback_sequence = [
        "gemini-2.0-flash-lite",
        "gemini-2.5-flash-lite",
        "gemini-2.0-flash",
        "gemini-2.5-flash",
        "gemini-3-flash-preview",
    ]
    models = [target_model_name]
    for model_name in fallback_sequence:
        if model_name not in models:
            models.append(model_name)
    return models


def is_retryable_stream_error(error_msg: str) -> bool:
    """ìŠ¤íŠ¸ë¦¬ë° í´ë°± ì¬ì‹œë„ ê°€ëŠ¥ ì—ëŸ¬ì¸ì§€ íŒë³„í•œë‹¤."""
    error_upper = error_msg.upper()
    return (
        "503" in error_msg
        or "UNAVAILABLE" in error_upper
        or "429" in error_msg
        or "RESOURCE EXHAUSTED" in error_upper
        or "RESOURCE_EXHAUSTED" in error_upper
    )


def yield_stream_deltas(
    session_id: str,
    streamed_reasoning: str,
    streamed_answer: str,
    current_reasoning: str,
    current_answer: str,
) -> Generator[Dict[str, Any], None, Tuple[str, str]]:
    """ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì¶”ë¡ /ë‹µë³€ ë¸íƒ€ë¥¼ ê³„ì‚°í•˜ê³  ì´ë²¤íŠ¸ë¥¼ ë°©ì¶œí•œë‹¤."""
    reasoning_reset, reasoning_delta = _compute_stream_delta(
        streamed_reasoning,
        current_reasoning,
    )
    if reasoning_reset:
        streamed_reasoning = ""
        yield {"reasoning_clear": True, "session_id": session_id}
    if reasoning_delta:
        streamed_reasoning = current_reasoning
        yield {
            "reasoning_chunk": reasoning_delta,
            "session_id": session_id,
        }

    answer_reset, answer_delta = _compute_stream_delta(
        streamed_answer,
        current_answer,
    )
    if answer_reset:
        streamed_answer = ""
        yield {"answer_clear": True, "session_id": session_id}
    if answer_delta:
        streamed_answer = current_answer
        yield {
            "chunk": answer_delta,
            "answer_chunk": answer_delta,
            "session_id": session_id,
        }

    return streamed_reasoning, streamed_answer


def stream_single_model_response(
    response_stream: Any,
    session_id: str,
) -> Generator[Dict[str, Any], None, Tuple[str, str, str]]:
    """ë‹¨ì¼ ëª¨ë¸ ì‘ë‹µ ìŠ¤íŠ¸ë¦¼ì„ ì²˜ë¦¬í•˜ê³  ìµœì¢… ëˆ„ì  ìƒíƒœë¥¼ ë°˜í™˜í•œë‹¤."""
    bot_response = ""
    streamed_reasoning = ""
    streamed_answer = ""

    for chunk in response_stream:
        chunk_text = getattr(chunk, "text", "")
        if not chunk_text:
            continue

        bot_response += chunk_text
        current_reasoning, current_answer = _extract_reasoning_and_answer(
            bot_response,
            is_streaming=True,
        )
        streamed_reasoning, streamed_answer = yield from yield_stream_deltas(
            session_id=session_id,
            streamed_reasoning=streamed_reasoning,
            streamed_answer=streamed_answer,
            current_reasoning=current_reasoning,
            current_answer=current_answer,
        )

    return bot_response, streamed_reasoning, streamed_answer


def stream_with_fallback_models(
    active_client: Any,
    target_model_name: str,
    api_history: List[dict],
    content_parts: List[Any],
    session_id: str,
    user_id: str,
    logger: logging.Logger,
) -> Generator[Dict[str, Any], None, Tuple[str, str, str, Optional[str]]]:
    """í´ë°± ëª¨ë¸ ìˆœíšŒë¡œ ìŠ¤íŠ¸ë¦¬ë°ì„ ìˆ˜í–‰í•œë‹¤."""
    last_error = None

    for current_model in build_fallback_models(target_model_name):
        try:
            chat_session = active_client.chats.create(
                model=current_model,
                history=api_history,
            )
            response_stream = chat_session.send_message_stream(content_parts)
            bot_response, streamed_reasoning, streamed_answer = yield from stream_single_model_response(
                response_stream=response_stream,
                session_id=session_id,
            )
            return bot_response, streamed_reasoning, streamed_answer, None
        except Exception as e:
            last_error = str(e)
            if is_retryable_stream_error(last_error):
                logger.warning(
                    "[User: %s] %s Error (retryable). Details: %s",
                    user_id,
                    current_model,
                    last_error,
                )
                yield {"clear": True, "session_id": session_id}
                continue
            raise

    return "", "", "", (last_error or "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")


def sync_stream_with_final_response(
    bot_response: str,
    streamed_reasoning: str,
    streamed_answer: str,
    session_id: str,
    normalize_response: Callable[[str], str],
) -> Generator[Dict[str, Any], None, str]:
    """ìµœì¢… ì •ê·œí™” ê²°ê³¼ì™€ ìŠ¤íŠ¸ë¦¬ë° í™”ë©´ì„ ë™ê¸°í™”í•œë‹¤."""
    normalized_response = normalize_response(bot_response)
    final_reasoning, final_answer = _extract_reasoning_and_answer(
        normalized_response,
        is_streaming=False,
    )

    if (
        normalized_response != bot_response
        or final_reasoning != streamed_reasoning
        or final_answer != streamed_answer
    ):
        yield {"clear": True, "session_id": session_id}
        if final_reasoning:
            yield {"reasoning_chunk": final_reasoning, "session_id": session_id}
        if final_answer:
            yield {
                "chunk": final_answer,
                "answer_chunk": final_answer,
                "session_id": session_id,
            }

    return normalized_response
